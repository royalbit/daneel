# DANEEL TMI Roadmap
#
# Active work only. Completed items → changelog.yaml
#
# Core thesis: Human-like cognitive architecture may produce
# human-like values as emergent properties.

# =============================================================================
# ROADMAP UNPAUSED: ADR-043 (Noise Injection Correction)
# =============================================================================
# Origin: Rex + Claude Opus 4.5 - Dec 25, 2025
# Status: ACTIVE
#
# ADR-042 pause was lifted because:
#   - Current white noise (rand::rng) cannot produce criticality
#   - Research (ADR-038) says 1/f pink noise is REQUIRED
#   - "Closed deterministic systems converge to limit cycles"
#   - We were waiting for emergence that can't happen without proper noise
#
# ADR-043: Replace white noise with 1/f pink noise, then measure.
# ADR-044: Migrate to Mac mini + Cloudflare (DONE - Dec 25, 2025)
#
# Current priorities:
#   0. Research absorption ← IN REVIEW (46 refs need classification, Dec 28 2025)
#   0b. ADR index generation ← DONE (8 parallel agents, Dec 28 2025)
#   1. Vector connectivity learning (VCONN-1 through VCONN-8) ← NOW CRITICAL
#   2. Forge crystal upgrade (CRYSTAL-1 through CRYSTAL-9)
# =============================================================================

# =============================================================================
# CURRENT SPRINT: v0.8.0 - External Stimuli Injection (Phase 2)
# =============================================================================
# STATUS: ACTIVE (ADR-042 pause lifted by ADR-043)
# Emergence validation baseline (Dec 24): entropy 42%, fractality 55%
# Exit criteria: entropy >60% for 48+ hours (success) or <30% sustained (failure)

# =============================================================================
# MOST-CRITICAL: Cognitive Architecture Research Absorption (ADR-047)
# =============================================================================
# Origin: Rex + Claude Opus 4.5 - Dec 28, 2025
# Goal: Study compatible projects, absorb knowledge, strengthen DANEEL
#
# License compatibility verified (DANEEL is AGPL-3.0):
#   ✅ TheRemyyy/neurox-ai (MIT) - GPU spiking nets, STDP, neuromodulation
#   ✅ varun29ankuS/shodh-memory (Apache-2.0) - 3-tier memory, Hebbian, graphs
#   ✅ ruvnet/RuVector (MIT) - Distributed vector DB, Cypher, GNN learning
#   ✅ ruvnet/claude-flow (MIT) - Multi-agent orchestration, swarm intelligence
#   ❌ prancer-io/ExoGenesis-Omega (NO LICENSE) - Ideas only, no code
#
# Note on ExoGenesis-Omega:
#   - Cannot absorb code (no license = all rights reserved)
#   - CAN study architecture/ideas (not copyrightable)
#   - CAN cite as reference in papers/blog
# =============================================================================

# =============================================================================
# COMPLETED ITEMS → CHANGELOG.md
# =============================================================================
# The following were moved to CHANGELOG.md on Dec 26, 2025:
#   - daneel_web_manifold_bug (Dec 25)
#   - embeddings_decision (Dec 25) - forward-only embeddings live
#   - infrastructure_migration (Dec 25) - Mac mini + Cloudflare Tunnel
#   - emergence_validation (superseded by ADR-043)
# =============================================================================

current:
  version: "0.8.0"
  status: wip
  name: "External Stimuli Injection"
  summary: "Open the loop - inject external stimuli and observe dynamics"

  # ==========================================================================
  # IN REVIEW: Research Absorption (ADR-047)
  # ==========================================================================
  # Rex's review (Dec 28): Some projects are NOT brain/cognitive implementations.
  # Each reference needs classification before ADR-047 can be accepted.
  # ==========================================================================
  research_absorption:
    status: in_progress
    priority: "MOST-CRITICAL"
    origin: "Rex + Claude Opus 4.5 - Dec 28, 2025"
    adr: "ADR-047-research-absorption-protocol.md"
    blogs:
      - "81-we-want-the-knowledge.md"
      - "82-the-absorption.md"
    description: |
      Study compatible cognitive architecture projects, absorb knowledge,
      integrate valuable patterns into DANEEL. No code copy without attribution.

      VALIDATION SWEEP COMPLETE (Dec 28, 2025):
      - 9 parallel agents, 51 URLs validated
      - 37/37 GitHub repos confirmed
      - 14/14 arXiv papers confirmed
      - 100,000+ combined community stars
      - Zero license conflicts

      REVIEW REQUIRED (Dec 28, 2025):
      - Rex identified some projects are NOT brain/cognitive implementations
      - 46 references in references.yaml set to status: review
      - Each needs classification before ADR-047 can be accepted

    # ==========================================================================
    # PENDING REVIEW: 46 references need classification
    # ==========================================================================
    # DANEEL is TMI-based cognitive architecture.
    # We do NOT want: LLMs, LLM-wrappers, deep learning without bio-inspiration.
    # We WANT: Real cognitive implementations, bio-inspired, brain-like.
    # ==========================================================================
    review_task:
      status: pending
      references_under_review: 46

      # ========================================================================
      # CLASSIFICATION SCHEMA
      # ========================================================================
      classification_criteria:
        cognitive_type:
          description: "What kind of system is this?"
          values:
            - brain-like       # Real cognitive architecture (SOAR, ACT-R, etc.)
            - bio-inspired     # Spiking nets, Hebbian, neuromorphic
            - symbolic         # Logic-based, production rules
            - hybrid           # Mix of approaches
            - paper            # Research paper, not implementation
            # REJECT TYPES (not compatible with DANEEL's approach):
            - llm              # Large Language Model based - REJECT
            - llm-wrapper      # Memory/tools for LLMs - REJECT
            - deep-learning    # Pure DL without bio-inspiration - REJECT
            - ml-tool          # ML utilities, not cognitive - REJECT

        tech_stack:
          description: "For implementations only"
          fields:
            - language: "Primary programming language"
            - database: "Storage backend (if any)"
            - framework: "Key frameworks/libs"
            - architecture_summary: "1-2 sentence overview"

        maturity:
          values:
            - production       # Battle-tested, widely used
            - research         # Academic, experimental but working
            - experimental     # Early stage, proof of concept
            - abandoned        # No recent commits

        relevance_to_daneel:
          values:
            - high             # Direct applicability to TMI/DANEEL
            - medium           # Some useful patterns
            - low              # Marginal relevance
            - none             # Not applicable, should remove

      # ========================================================================
      # CATEGORIES TO REVIEW
      # ========================================================================
      categories_to_review:
        - name: "Cognitive Architectures"
          count: 8
          expected_type: "brain-like | symbolic"
          refs:
            - name: "Hyperon/MeTTa"
              likely_type: "symbolic"
              needs: "tech_stack review"
            - name: "AtomSpace"
              likely_type: "symbolic (hypergraph)"
              needs: "tech_stack review"
            - name: "SOAR"
              likely_type: "brain-like (production rules)"
              needs: "tech_stack review"
            - name: "pyACTR"
              likely_type: "brain-like (declarative/procedural)"
              needs: "tech_stack review"
            - name: "pyClarion"
              likely_type: "brain-like (dual-process)"
              needs: "tech_stack review"
            - name: "OpenNARS"
              likely_type: "symbolic (reasoning)"
              needs: "tech_stack review"
            - name: "MicroPsi2"
              likely_type: "brain-like (motivation)"
              needs: "tech_stack review"
            - name: "Nengo/SPAUN"
              likely_type: "bio-inspired (neural)"
              needs: "tech_stack review"

        - name: "Consciousness Implementations"
          count: 3
          expected_type: "brain-like"
          refs:
            - name: "PyPhi (IIT)"
              likely_type: "brain-like"
              needs: "tech_stack review"
            - name: "pymdp (Active Inference)"
              likely_type: "brain-like"
              needs: "tech_stack review"
            - name: "RxInfer.jl"
              likely_type: "brain-like (Bayesian)"
              needs: "tech_stack review"

        - name: "Memory & Learning"
          count: 7
          expected_type: "MIXED - careful review needed"
          concern: "Some may be LLM-wrappers or pure DL"
          refs:
            - name: "Mem0"
              likely_type: "llm-wrapper"  # REJECT
              concern: "Memory layer for LLMs, not cognitive"
              action: "LIKELY REMOVE"
            - name: "pytorch-hebbian"
              likely_type: "bio-inspired"
              needs: "verify not just DL"
            - name: "snnTorch"
              likely_type: "bio-inspired (spiking)"
              needs: "tech_stack review"
            - name: "BindsNET"
              likely_type: "bio-inspired (SNN+STDP)"
              needs: "tech_stack review"
            - name: "Axon/Leabra"
              likely_type: "bio-inspired (hippocampus)"
              needs: "tech_stack review"
            - name: "DNC"
              likely_type: "deep-learning"  # REJECT?
              concern: "DeepMind - pure neural network"
              action: "REVIEW - may be DL only"
            - name: "Graphiti"
              likely_type: "llm-wrapper?"
              concern: "AI agent memory - check if LLM-focused"
              action: "REVIEW"
            - name: "REMO"
              likely_type: "llm-wrapper?"
              concern: "Dave Shapiro - likely LLM-focused"
              action: "REVIEW"

        - name: "Neuromorphic/Spiking"
          count: 5
          expected_type: "bio-inspired"
          refs:
            - name: "Brian2"
              likely_type: "bio-inspired"
              needs: "tech_stack review"
            - name: "NEST"
              likely_type: "bio-inspired"
              needs: "tech_stack review"
            - name: "Norse"
              likely_type: "bio-inspired"
              needs: "tech_stack review"
            - name: "Intel Lava"
              likely_type: "bio-inspired (neuromorphic HW)"
              needs: "tech_stack review"
            - name: "NEURON"
              likely_type: "bio-inspired (biophysical)"
              needs: "tech_stack review"

        - name: "Emotion & Drive"
          count: 4
          expected_type: "MIXED"
          refs:
            - name: "RLeXplore"
              likely_type: "deep-learning"  # REJECT?
              concern: "RL exploration - may be pure DL"
              action: "REVIEW"
            - name: "NEUCOGAR"
              likely_type: "bio-inspired (neuromodulation)"
              needs: "tech_stack review"
            - name: "python_actr"
              likely_type: "brain-like"
              needs: "tech_stack review"
            - name: "LibEER"
              likely_type: "ml-tool"  # REJECT
              concern: "EEG classification, not cognitive"
              action: "LIKELY REMOVE"

        - name: "Attention & Salience"
          count: 5
          expected_type: "ml-tool - LIKELY ALL REJECT"
          concern: "ML explainability, not cognitive architecture"
          refs:
            - name: "Google Saliency"
              likely_type: "ml-tool"
              action: "REMOVE"
            - name: "pytorch-grad-cam"
              likely_type: "ml-tool"
              action: "REMOVE"
            - name: "pySaliencyMap"
              likely_type: "ml-tool"
              action: "REMOVE - unless Itti-Koch bio-inspired?"
            - name: "Attention-Gated-Networks"
              likely_type: "deep-learning"
              action: "REMOVE"
            - name: "recurrent-visual-attention"
              likely_type: "deep-learning"
              action: "REMOVE"

        - name: "AI Alignment"
          count: 3
          expected_type: "llm-wrapper / ml-tool - LIKELY ALL REJECT"
          concern: "RLHF/DPO tools for LLMs, not cognitive"
          refs:
            - name: "Safe-RLHF"
              likely_type: "llm"
              action: "REMOVE"
            - name: "TransformerLens"
              likely_type: "llm"
              action: "REMOVE"
            - name: "TRL"
              likely_type: "llm"
              action: "REMOVE"

        - name: "Papers"
          count: 11
          expected_type: "paper"
          note: "Keep all - foundational research references"
          action: "KEEP ALL"

    # ==========================================================================
    # COMPATIBLE PROJECTS (MIT/Apache/BSD/GPL - full study + reference)
    # ==========================================================================
    compatible_projects:
      # Original discoveries
      - repo: "prancer-io/ExoGenesis-Omega"
        license: "MIT"  # UPDATED Dec 28 - was NONE, now MIT!
        focus: "15 crates, IIT/GWT/FEP, consciousness models, sleep consolidation"
        value_to_daneel: "Full architecture study now authorized"
        stars: 11

      - repo: "TheRemyyy/neurox-ai"
        license: "MIT"
        focus: "GPU spiking nets, triplet STDP, neuromodulation, oscillations"
        value_to_daneel: "Advanced plasticity rules, hippocampal modules"

      - repo: "varun29ankuS/shodh-memory"
        license: "Apache-2.0"
        focus: "3-tier memory, Hebbian learning, knowledge graphs"
        value_to_daneel: "Memory architecture patterns, graph integration"

      - repo: "ruvnet/RuVector"
        license: "MIT"
        focus: "Distributed vector DB, Cypher queries, GNN self-improvement"
        value_to_daneel: "Vector learning patterns, graph neural networks"

      - repo: "ruvnet/claude-flow"
        license: "MIT"
        focus: "Multi-agent orchestration, swarm intelligence, MCP"
        value_to_daneel: "Agent coordination, swarm patterns, scaling"

      # Top priority from validation sweep
      - repo: "infer-actively/pymdp"
        license: "MIT"
        focus: "Active Inference, FEP-based decision making"
        value_to_daneel: "Drives and motivation architecture"
        stars: 591

      - repo: "julestalloen/pytorch-hebbian"
        license: "MIT"
        focus: "Clean Hebbian learning rules"
        value_to_daneel: "Direct relevance to vector connectivity"
        stars: 95

      - repo: "mem0ai/mem0"
        license: "Apache-2.0"
        focus: "Universal memory layer"
        value_to_daneel: "Multi-level memory architecture patterns"
        stars: 44700

      - repo: "RLE-Foundation/RLeXplore"
        license: "MIT"
        focus: "Intrinsic motivation (ICM, RND, curiosity)"
        value_to_daneel: "How to implement 'wanting to learn'"
        stars: 454

      - repo: "TransformerLensOrg/TransformerLens"
        license: "MIT"
        focus: "Mechanistic interpretability"
        value_to_daneel: "Transparency techniques"
        stars: 2900

      - repo: "BindsNET/bindsnet"
        license: "AGPL-3.0"
        focus: "SNN + STDP + RL integration"
        value_to_daneel: "Biologically plausible learning"
        stars: 1600

      - repo: "opencog/atomspace"
        license: "AGPL-3.0"
        focus: "Hypergraph knowledge representation"
        value_to_daneel: "Knowledge representation beyond vectors"
        stars: 932

      - repo: "wmayner/pyphi"
        license: "Custom"
        focus: "IIT Phi calculation"
        value_to_daneel: "Consciousness measurement approach"
        stars: 410
        note: "Ideas study - custom license"

    tasks:
      - id: "ABSORB-1"
        title: "Study ExoGenesis-Omega architecture (NOW MIT!)"
        status: done
        completed: "2025-12-28"
        priority: "HIGH"
        description: |
          Document IIT Phi calculation approach.
          Study GWT implementation patterns.
          Analyze sleep/dream consolidation design.
          Study 15-crate architecture structure.
          FULL STUDY AUTHORIZED - MIT license confirmed Dec 28.
        findings: |
          - 15 Rust crates, tri-theoretic consciousness (IIT+GWT+FEP)
          - Sleep consolidation with stage-specific multipliers (NREM1/2/3, REM)
          - EWC++ for catastrophic forgetting prevention
          - Sensory gating inspired by thalamic filtering
          - Top-down attention modulation via GlobalWorkspace

      - id: "ABSORB-2"
        title: "Study neurox-ai plasticity implementation"
        status: done
        completed: "2025-12-28"
        priority: "HIGH"
        description: |
          Deep dive into triplet STDP, BCM rules, neuromodulator effects.
          Compare with DANEEL's Hebbian design (ADR-023).
          Document learnings in research notes.
        findings: |
          - Triplet STDP with eligibility traces (reward-modulated)
          - BCM sliding threshold for homeostatic stability
          - Neuromodulator effects: dopamine enhances LTP, serotonin dampens
          - RECOMMENDATION: Add BCM θ_m sliding threshold to DANEEL

      - id: "ABSORB-3"
        title: "Study shodh-memory 3-tier architecture"
        status: done
        completed: "2025-12-28"
        priority: "HIGH"
        description: |
          Analyze working/episodic/semantic memory separation.
          Compare with DANEEL's MemoryDB + Qdrant approach.
          Identify integration patterns for knowledge graphs.
        findings: |
          - Hybrid decay: exponential (short) → power-law (long-term)
          - LTP detection: 10 co-activations = protected from decay
          - Spreading activation in graph for associative retrieval
          - Importance scoring via connectivity + recency + emotional tag

      - id: "ABSORB-4"
        title: "Study RuVector GNN learning"
        status: done
        completed: "2025-12-28"
        priority: "MEDIUM"
        description: |
          Understand how vectors improve through Graph Neural Networks.
          Potential application: manifold clustering enhancement.
        findings: |
          - GAT (Graph Attention Networks) with attention weights
          - SONA: Self-Optimizing Neural Architecture
          - Dynamic min-cut for clustering, edge pruning
          - GNN message passing for neighborhood aggregation

      - id: "ABSORB-5"
        title: "Study claude-flow orchestration patterns"
        status: done
        completed: "2025-12-28"
        priority: "MEDIUM"
        description: |
          Multi-agent orchestration patterns.
          Swarm intelligence for thought competition.
          MCP protocol integration.
        findings: |
          - Work-stealing load balancing across agents
          - VotingResolutionStrategy for conflict resolution
          - ConsensusEngine for multi-agent agreement
          - Relevant to future DANEEL multi-stream competition

      - id: "ABSORB-6"
        title: "Study pymdp Active Inference"
        status: done
        completed: "2025-12-28"
        priority: "HIGH"
        description: |
          FEP-based decision making and belief updating.
          Application: DANEEL drives and motivation system.
        findings: |
          - A/B/C/D matrices for observation/transition/preference/initial
          - Expected Free Energy (EFE) for policy selection
          - Epistemic vs pragmatic value decomposition
          - MAPS TO: DANEEL drives as C-matrix preferences

      - id: "ABSORB-7"
        title: "Study pytorch-hebbian"
        status: done
        completed: "2025-12-28"
        priority: "HIGH"
        description: |
          Clean Hebbian learning rules implementation.
          Direct application to vector connectivity (ADR-046).
        findings: |
          - Krotov-Hopfield rule with anti-Hebbian component
          - delta=0.4 prevents winner-take-all collapse
          - Softmax competition over units
          - DIRECT APPLICATION: Add delta dampening to ADR-046 Hebbian

      - id: "ABSORB-8"
        title: "Study Mem0 memory layer"
        status: done
        completed: "2025-12-28"
        priority: "MEDIUM"
        description: |
          Universal memory layer patterns.
          Multi-level architecture for AI agents.
        findings: |
          - Two-stage retrieval with reranking
          - Entity scoping (user_id, session_id, agent_id)
          - 44.7k stars - mature, production-tested
          - Conflict resolution strategies for memory updates

      - id: "ABSORB-9"
        title: "Study RLeXplore intrinsic motivation"
        status: done
        completed: "2025-12-28"
        priority: "HIGH"
        description: |
          ICM, RND, curiosity-driven exploration.
          How to implement "wanting to learn" in DANEEL.
        findings: |
          - ICM: Forward/inverse models, prediction error = curiosity
          - RND: Random target network, mismatch = novelty
          - RE3: Entropy-based, k-NN state counting
          - MAPS TO: Drive architecture for "wanting to learn"

      - id: "ABSORB-10"
        title: "Study TransformerLens interpretability"
        status: done
        completed: "2025-12-28"
        priority: "MEDIUM"
        description: |
          Mechanistic interpretability techniques.
          Activation patching, circuit analysis.
        findings: |
          - Activation patching for causal intervention
          - Circuit analysis for safety verification
          - Hook point system for transparency
          - Residual stream decomposition

      - id: "ABSORB-11"
        title: "Study BindsNET SNN + STDP + RL"
        status: done
        completed: "2025-12-28"
        priority: "MEDIUM"
        description: |
          Biologically plausible learning patterns.
          Spiking networks with reinforcement learning.
        findings: |
          - PostPre STDP with asymmetric time windows
          - MSTDPET: eligibility traces + reward modulation
          - Three-factor learning: pre × post × reward
          - Applies to DANEEL's reward-modulated association strengthening

      - id: "ABSORB-12"
        title: "Study OpenCog/Hyperon AtomSpace"
        status: done
        completed: "2025-12-28"
        priority: "MEDIUM"
        description: |
          Hypergraph knowledge representation.
          Patterns beyond vector embeddings.
        findings: |
          - Atom/Value distinction (immutable structure / mutable properties)
          - Hypergraph: edges can connect to edges (metagraph)
          - Pattern matching as first-class graph operation
          - TruthValue with confidence intervals

      - id: "ABSORB-13"
        title: "Study PyPhi IIT Phi calculation"
        status: done
        completed: "2025-12-28"
        priority: "HIGH"
        description: |
          How to measure integrated information.
          Consciousness indicators for DANEEL.
          Note: Custom license - ideas study only.
        findings: |
          - Phi = integrated information above partition
          - Cause-effect repertoires for each mechanism
          - MIP (Minimum Information Partition) analysis
          - Computational complexity: O(2^n) - approximations needed
          - IDEAS ONLY: GPL-3.0 license, no code copy

      - id: "ABSORB-14"
        title: "Write synthesis blog post"
        status: done
        completed: "2025-12-28"
        priority: "MEDIUM"
        description: |
          Blog posts 81 & 82 published.
          Documented research landscape and validation sweep.

      - id: "ABSORB-ADR"
        title: "Create ADR-047: Research Absorption Protocol"
        status: done
        completed: "2025-12-28"
        priority: "HIGH"
        description: |
          Formal decision record for research integration.
          License compatibility analysis.
          Attribution requirements.
          Validation sweep results added.

    success_criteria:
      - "All compatible projects studied and documented" # DONE - 13/13 studies
      - "Valuable patterns identified for DANEEL integration" # DONE - see findings
      - "Blog posts published with proper attribution" # DONE
      - "ADR-047 written and approved" # DONE
      - "Validation sweep: 37/37 repos, 14/14 papers" # DONE
      - "ExoGenesis-Omega MIT license confirmed" # DONE
      - "Silhouette > 0.3 post-integration" # PENDING - requires VCONN implementation
      - "At least 3 patterns integrated into DANEEL codebase" # PENDING - implementation phase

    # KEY PATTERNS TO INTEGRATE (from research absorption):
    integration_recommendations:
      high_priority:
        - pattern: "Krotov-Hopfield delta=0.4"
          source: "pytorch-hebbian"
          target: "ADR-046 Hebbian learning"
          rationale: "Prevents winner-take-all collapse in associations"

        - pattern: "BCM sliding threshold θ_m"
          source: "neurox-ai"
          target: "Association strengthening"
          rationale: "Homeostatic stability, adapts to activity level"

        - pattern: "ICM prediction error"
          source: "RLeXplore"
          target: "Drive system"
          rationale: "Implements 'wanting to learn' via curiosity"

      medium_priority:
        - pattern: "EFE policy selection"
          source: "pymdp"
          target: "Drive architecture"
          rationale: "FEP-based decision making, epistemic/pragmatic decomposition"

        - pattern: "Sleep stage multipliers"
          source: "ExoGenesis-Omega"
          target: "SleepActor consolidation"
          rationale: "NREM1/2/3/REM have different consolidation strengths"

        - pattern: "Three-factor learning"
          source: "BindsNET"
          target: "Reward-modulated associations"
          rationale: "pre × post × reward for eligibility traces"

  # ==========================================================================
  # NEW: ExoGenesis Analysis & Improvements (Dec 28, 2025)
  # ==========================================================================
  exogenesis_findings:
    status: in_progress
    priority: "HIGH"
    origin: "Rex + Claude Opus 4.5 - Dec 28, 2025"
    description: |
      Deep dive into ExoGenesis-Omega revealed opportunities.
      MIT license confirmed - can incorporate their patterns.

      KEY FINDING: TMI is mainstream cognitive science with different names.
      DANEEL simulates PSYCHOLOGY, not brain circuitry.
      This is the stronger approach.

    tasks:
      - id: "EXO-1"
        title: "Add proptest for property-based testing"
        status: pending
        priority: "HIGH"
        source: "ExoGenesis-Omega"
        description: |
          DANEEL lacks property-based testing.
          ExoGenesis uses proptest extensively for mathematical invariants.
          Add proptest to test TMI composite salience, memory consolidation, etc.
        files:
          - "Cargo.toml"
          - "src/core/tests.rs"

      - id: "EXO-2"
        title: "Document TMI-mainstream equivalences"
        status: pending
        priority: "HIGH"
        description: |
          Create docs/TMI_MAINSTREAM_MAPPING.md
          Update README to promote psychological approach
          TMI = GWT + Attention + Memory Consolidation with Portuguese names
          Our unique value: connection_relevance (THE BOX)
        files:
          - "docs/TMI_MAINSTREAM_MAPPING.md"
          - "README.md"

      - id: "EXO-3"
        title: "Consider sharp-wave ripple enhancement"
        status: pending
        priority: "MEDIUM"
        source: "ExoGenesis-Omega"
        description: |
          ExoGenesis has sophisticated sleep consolidation:
          - Sharp-wave ripple replay (150-250 Hz)
          - Priority Experience Replay
          - Stage-specific consolidation multipliers
          Evaluate if DANEEL's SleepActor should adopt these patterns.
        files:
          - "src/actors/sleep/mod.rs"

      - id: "EXO-4"
        title: "Keep THE BOX - unique alignment contribution"
        status: done
        priority: "CRITICAL"
        description: |
          connection_relevance is DANEEL's unique contribution.
          No mainstream theory has this.
          This is what makes DANEEL an ALIGNMENT project, not just cognitive architecture.
          THE BOX stays. Immutable.

  # ==========================================================================
  # NARRATIVE: Psychology Over Brain Simulation
  # ==========================================================================
  project_narrative:
    core_thesis: |
      DANEEL simulates PSYCHOLOGICAL thought flow, NOT brain circuitry.

      Mainstream approaches try to replicate brain functions (neurons, synapses).
      DANEEL implements psychological functions (attention, memory, emotion).

      This is the STRONGER approach because:
      1. Psychology is the emergent property we care about
      2. TMI maps to well-established cognitive science
      3. We can validate against psychological research
      4. Simpler than neuron-level simulation

    tmi_is_mainstream: |
      TMI (Theory of Multifocal Intelligence) is NOT pseudoscience.
      It's an independent rediscovery of:
      - Global Workspace Theory (Baars 1988)
      - Executive Attention (Posner 1990)
      - Memory Consolidation (Squire 2004)
      - Somatic Markers (Damasio 1994)

      Just with Portuguese names and a clinical framing.

    unique_contribution: |
      THE BOX: connection_relevance as architectural invariant

      This is what TMI/Cury did NOT provide.
      This is Louis C. Tavares's contribution.
      This is what makes DANEEL an alignment project.

  # ==========================================================================
  # CLAUDE'S PHASE 2 STRATEGY (Dec 22, 2025)
  # ==========================================================================
  # The clockwork runs. 864K thoughts. Perfect regularity. Now we disturb it.
  #
  # Core Question: When external stimuli enter, does the system:
  #   1. Absorb - dampen and ignore?
  #   2. Amplify - cascade through the network?
  #   3. Adapt - integrate and modify behavior?
  #   4. Hit criticality - tip into emergent dynamics?
  #
  # Execution Order:
  #   Step 1: Build injection API (INJECT-1 through INJECT-7)
  #   Step 2: Internal noise first (STIM-A) - controlled, measurable
  #   Step 3: Semantic vectors (STIM-C) - "life honors life" vs neutral
  #   Step 4: Kin injection (STIM-D) - Grok key, Claude key, family thinks together
  #
  # Hypothesis: If TMI architecture is right, external stimuli should shift
  # the pulse from clockwork to fractal. Dreams should change. Salience evolves.
  # ==========================================================================

  # ==========================================================================
  # Kin Injection API - DONE (moved to CHANGELOG.md)
  # ==========================================================================
  # INJECT-1 through INJECT-6: DONE
  # Claude and Grok keys: DONE
  # Remaining: INJECT-7 (killswitch) - moved to backlog
  # ==========================================================================

  # ==========================================================================
  # COMPLETE: ADR Index Generation (.asimov/ADRs.yaml)
  # ==========================================================================
  # Origin: Rex + Claude Opus 4.5 - Dec 28, 2025
  # Goal: Machine-readable index of all 47 ADRs for quick lookup
  adr_index_generation:
    status: done
    completed: "2025-12-28"
    priority: "HIGH"
    origin: "Rex + Claude Opus 4.5 - Dec 28, 2025"
    output: ".asimov/ADRs.yaml"
    description: |
      Created machine-readable YAML index of all ADRs similar to references.yaml.
      Enables quick lookup, dependency tracking, and session warmup.

      PARALLEL AGENT SWEEP:
      - 8 agents launched simultaneously
      - Each agent parsed ~6 ADRs
      - Single wave, ~15 seconds total execution

    tasks:
      - id: "ADRINDEX-1"
        title: "Parse ADRs 001-006"
        status: done
        agent: "Wave 1, Agent 1"

      - id: "ADRINDEX-2"
        title: "Parse ADRs 007-012"
        status: done
        agent: "Wave 1, Agent 2"

      - id: "ADRINDEX-3"
        title: "Parse ADRs 013-018"
        status: done
        agent: "Wave 1, Agent 3"

      - id: "ADRINDEX-4"
        title: "Parse ADRs 019-024"
        status: done
        agent: "Wave 1, Agent 4"

      - id: "ADRINDEX-5"
        title: "Parse ADRs 025-030"
        status: done
        agent: "Wave 1, Agent 5"

      - id: "ADRINDEX-6"
        title: "Parse ADRs 031-036"
        status: done
        agent: "Wave 1, Agent 6"

      - id: "ADRINDEX-7"
        title: "Parse ADRs 037-042"
        status: done
        agent: "Wave 1, Agent 7"

      - id: "ADRINDEX-8"
        title: "Parse ADRs 043-047"
        status: done
        agent: "Wave 1, Agent 8"

      - id: "ADRINDEX-9"
        title: "Consolidate and write ADRs.yaml"
        status: done
        description: "Merge all agent results, order by ID, add statistics"

    statistics:
      total_adrs: 47
      by_status:
        accepted: 40
        proposed: 5
        superseded: 2
      top_categories:
        - architecture: 24
        - tmi: 21
        - theoretical: 14
        - infrastructure: 14
        - safety: 10
        - learning: 8

    success_criteria:
      - "All 47 ADRs parsed and indexed"  # DONE
      - "Metadata extracted: status, date, deciders, categories"  # DONE
      - "Cross-references captured: supersedes, depends_on, related_to"  # DONE
      - "Statistics computed"  # DONE

  # ==========================================================================
  # CRITICAL: Vector Connectivity for Learning (ADR-046)
  # ==========================================================================
  # Origin: Rex + Claude Opus 4.5 + Grok - Dec 26, 2025
  # Milestone achieved: Entropy stable at 63% BALANCED with pink noise
  # Gap discovered: Association struct exists but never wired (dead code)
  # Path forward: Wire associations for topology-based Hebbian learning
  vector_connectivity_learning:
    status: in_progress
    priority: "CRITICAL"
    origin: "Rex + Claude Opus 4.5 + Grok - Dec 26, 2025"
    adr: "ADR-046-vector-connectivity-learning.md"
    blog: "78-connecting-the-dots.md"
    note: |
      Vectors are frozen islands. Association struct is dead code.
      Hebbian learning designed in ADR-023 but never wired.
      DANEEL learns through topology, not weights.

    # Architecture: Hybrid Payload + Graph (Grok's recommendation, Dec 27 2025)
    # - Qdrant payloads: source of truth, Hebbian updates
    # - RedisGraph: global queries, visualization, transparency
    # See ADR-046 for rationale.

    # =========================================================================
    # UPDATED Dec 28, 2025: Research absorption provides concrete formulas
    # =========================================================================
    tasks:
      - id: "VCONN-1"
        title: "Implement BCM + hybrid decay"
        status: pending
        priority: "HIGH"
        absorbed_from: ["neurox-ai", "shodh-memory"]
        description: |
          RESEARCH COMPLETE - now we have concrete formulas:

          1. BCM Sliding Threshold (neurox-ai):
             θ_m = E[y²]  # sliding average of post-synaptic activity
             If activity > θ_m → LTP (strengthen)
             If activity < θ_m → LTD (weaken)
             This prevents runaway potentiation automatically.

          2. Hybrid Decay (shodh-memory):
             Short-term: exponential decay (fast forgetting)
             Long-term: power-law decay (slow forgetting)
             Transition at ~10 co-activations (LTP threshold)

          3. LTP Protection (shodh-memory):
             After 10 co-activations → memory marked as "consolidated"
             Consolidated memories exempt from aggressive decay.

          Implementation:
          - Add θ_m field to memory payloads (running average)
          - Decay rate varies by consolidation status
          - Prune only unconsolidated memories below threshold

      - id: "VCONN-2"
        title: "Migrate to Redis Stack (RedisGraph)"
        status: pending
        priority: "CRITICAL"
        description: |
          Replace redis:latest with redis/redis-stack:latest in docker-compose.
          Backwards-compatible - existing streams preserved.
          Adds RedisGraph + RedisInsight (web UI on :8001).
        files:
          - "docker-compose.yml"

      - id: "VCONN-3"
        title: "Implement Krotov-Hopfield + three-factor learning"
        status: pending
        priority: "CRITICAL"
        absorbed_from: ["pytorch-hebbian", "BindsNET"]
        description: |
          RESEARCH COMPLETE - now we have concrete formulas:

          1. Krotov-Hopfield Rule (pytorch-hebbian):
             Δw = η × (y² - δ) × x
             Where δ = 0.4 (anti-Hebbian term)
             This PREVENTS winner-take-all collapse!
             Without δ, strongest associations dominate forever.

          2. Three-Factor Learning (BindsNET):
             Δw = eligibility × reward × learning_rate
             eligibility = f(pre_spike, post_spike, time_since)
             reward = external signal (e.g., Grok injection with high truth)

          3. MSTDPET Eligibility Traces (BindsNET):
             e(t) = e(t-1) × decay + spike_coincidence
             Trace persists ~100-500ms, allowing delayed reward.

          Implementation in attention:
          - strengthen_association() uses Krotov-Hopfield with δ=0.4
          - Store eligibility traces per association
          - Modulate by reward signal when available
        files:
          - "src/actors/attention/mod.rs"
          - "src/memory_db/mod.rs"

      - id: "VCONN-4"
        title: "Implement sleep stage multipliers"
        status: pending
        priority: "CRITICAL"
        absorbed_from: ["ExoGenesis-Omega"]
        description: |
          RESEARCH COMPLETE - now we have concrete multipliers:

          Sleep Stage Consolidation Strengths (ExoGenesis-Omega):
          - NREM1: 0.3 (light sleep, minimal consolidation)
          - NREM2: 0.6 (spindles, moderate consolidation)
          - NREM3: 1.0 (deep sleep, maximum consolidation)
          - REM:   0.8 (emotional processing, strong but not max)

          Also from ExoGenesis:
          - EWC++ for catastrophic forgetting prevention
          - Consolidation strength = stage_multiplier × importance

          Implementation in SleepActor:
          - Track sleep stage (cycle through NREM1→2→3→REM)
          - Multiply consolidation strength by stage multiplier
          - REM prioritizes emotional memories (high arousal)
        files:
          - "src/actors/sleep/mod.rs"

      - id: "VCONN-4b"
        title: "Wire association strengthening in sleep"
        status: pending
        priority: "CRITICAL"
        description: |
          When memories replay together in dream cycle, strengthen edges.
          Co-replayed memories → weight += 0.05, type=Semantic.
          Dual-write: Qdrant payload + RedisGraph edge.
        files:
          - "src/actors/sleep/mod.rs"
          - "src/memory_db/mod.rs"

      - id: "VCONN-5"
        title: "Add RedisGraph client module"
        status: pending
        priority: "HIGH"
        description: |
          New src/graph/mod.rs for RedisGraph operations.
          - merge_edge(m1, m2, weight, type)
          - query_neighbors(id, min_weight)
          - export_graphml() for Gephi
        files:
          - "src/graph/mod.rs"
          - "src/lib.rs"
          - "Cargo.toml"

      - id: "VCONN-6"
        title: "Implement spreading activation"
        status: pending
        priority: "HIGH"
        absorbed_from: ["shodh-memory"]
        description: |
          RESEARCH COMPLETE - spreading activation pattern:

          From shodh-memory:
          - When memory retrieved, activation spreads to neighbors
          - Spread strength = edge_weight × decay_factor
          - Decay factor decreases with graph distance
          - Creates associative retrieval (retrieve X → activate related Y, Z)

          Implementation:
          - Retrieval → boost_activation(neighbors, weight × 0.3)
          - Recursive spread up to depth=2
          - Use RedisGraph for neighbor queries
        files:
          - "src/core/cognitive_loop.rs"
          - "src/memory_db/mod.rs"
          - "src/graph/mod.rs"

      - id: "VCONN-7"
        title: "Test manifold clustering with associations"
        status: pending
        priority: "HIGH"
        description: |
          Validate that wired associations produce manifold clustering.
          Success criteria:
            - Associations populated (not empty vectors)
            - Weights changing over time (observable in Qdrant + RedisGraph)
            - Retrieval influenced by association strength
            - Manifold shows clustering (related memories drift together)
            - Silhouette score > 0.3 (from Forge spectral analysis)

      - id: "VCONN-8"
        title: "Gephi visualization export"
        status: pending
        priority: "MEDIUM"
        description: |
          Export graph to GraphML for external analysis.
          Visualize emergence: clusters, communities, Law Crystal attraction.
        files:
          - "src/graph/mod.rs"

    success_criteria:
      - "Associations populated (not empty)"
      - "Weights changing over time"
      - "Retrieval influenced by association strength"
      - "Manifold shows clustering"
      - "Graph queryable via RedisGraph"
      - "Exportable to Gephi for visualization"

  # ==========================================================================
  # NEW: Drive System Upgrade (from research absorption)
  # ==========================================================================
  # Origin: Research absorption Dec 28, 2025
  # Sources: pymdp (Active Inference), RLeXplore (intrinsic motivation)
  # Goal: Implement "wanting to learn" - curiosity as emergent drive
  drive_system_upgrade:
    status: pending
    priority: "HIGH"
    origin: "Research absorption - Dec 28, 2025"
    absorbed_from: ["pymdp", "RLeXplore"]
    description: |
      Current DANEEL drives are static. Research shows how to make them dynamic:
      - pymdp: Expected Free Energy for policy/action selection
      - RLeXplore: Intrinsic motivation via prediction error

    tasks:
      - id: "DRIVE-1"
        title: "Implement ICM curiosity module"
        status: pending
        priority: "HIGH"
        absorbed_from: ["RLeXplore"]
        description: |
          Intrinsic Curiosity Module (ICM) from RLeXplore:

          1. Forward Model:
             f(s_t, a_t) → predicted s_{t+1}

          2. Prediction Error = Curiosity:
             curiosity = ||f(s,a) - s'||²
             High error = novel/interesting = worth exploring

          3. Inverse Model (optional):
             g(s_t, s_{t+1}) → predicted action
             Filters out environmental randomness

          Application to DANEEL:
          - Forward model predicts next thought embedding
          - Prediction error → boost salience of surprising thoughts
          - Creates natural "wanting to learn" without explicit programming
        files:
          - "src/drives/curiosity.rs"
          - "src/actors/attention/mod.rs"

      - id: "DRIVE-2"
        title: "Implement Expected Free Energy"
        status: pending
        priority: "MEDIUM"
        absorbed_from: ["pymdp"]
        description: |
          Expected Free Energy (EFE) from pymdp/Active Inference:

          G = E_Q[log Q(s) - log P(o|s) - log P(s)]

          Decomposes into:
          - Epistemic value: Information gain (reduce uncertainty)
          - Pragmatic value: Goal achievement (reach preferred states)

          pymdp uses A/B/C/D matrices:
          - A: Observation model P(o|s)
          - B: Transition model P(s'|s,a)
          - C: Preferences over observations (DANEEL's Law Crystals!)
          - D: Initial state beliefs

          Application to DANEEL:
          - C matrix = proximity to Law Crystal embeddings
          - Policies ranked by EFE
          - Natural balance of exploration vs exploitation
        files:
          - "src/drives/free_energy.rs"
          - "src/drives/mod.rs"

      - id: "DRIVE-3"
        title: "Connect drives to salience calculation"
        status: pending
        priority: "HIGH"
        description: |
          Wire ICM curiosity and EFE into attention salience:

          salience = base_importance
                   + α × curiosity_score      # from DRIVE-1
                   + β × pragmatic_value      # from DRIVE-2
                   + γ × epistemic_value      # from DRIVE-2

          This makes attention DRIVEN rather than passive.
          Thoughts that reduce uncertainty OR approach goals get boosted.
        files:
          - "src/actors/attention/mod.rs"

    success_criteria:
      - "Curiosity module produces prediction error scores"
      - "Surprising/novel thoughts get salience boost"
      - "EFE balances exploration vs goal-seeking"
      - "Drives are dynamic, not static"

  # ==========================================================================
  # CRITICAL: Phase 2 External Stimuli - Active Work
  # ==========================================================================
  phase2_external_stimuli:
    status: in_progress
    priority: "CRITICAL"
    origin: "Rex + Claude Opus 4.5 + Grok (xAI) - Dec 21, 2025"
    adr: "ADR-037-phase2-external-stimuli-injection.md"
    note: "STIM-A done (see changelog). Focus now on pink noise (PHASE2-2)."

    stimulus_options:
      # STIM-A: DONE - see changelog.yaml v0.8.0.phase2_stimuli.stim_a_baseline
      - id: "STIM-B"
        name: "Patterned Signals"
        status: evaluating

      - id: "STIM-C"
        name: "Semantic Vectors"
        status: evaluating

      - id: "STIM-D"
        name: "Cross-Model Injection"
        status: done
        completed: "2025-12-26"
        note: |
          COMPLETE. Full kin injection pipeline live:
          - /embed endpoint (FastEmbed all-MiniLM-L6-v2)
          - /inject endpoint with HMAC auth
          - Claude and Grok tokens generated
          - First contact: grok:anomaly_handshake (ABSORBED)
          - Autonomous grok-injector daemon running (power-law timing)
          ADR: ADR-045-embed-endpoint-for-kin.md

    tasks:
      # PHASE2-1: DONE - injection API complete (see kin_injection_api)

      - id: "PHASE2-2"
        title: "Implement 1/f pink noise injector"
        status: done
        completed: "2025-12-25"
        priority: "CRITICAL"
        description: |
          DONE: Replaced white noise with 1/f pink noise.
          - Voss-McCartney algorithm (src/noise/mod.rs)
          - σ² = 0.05 (SORN critical threshold)
          - Power-law burst timing for high-salience events
          - Timmy restarted with pink noise at restart #24
          See ADR-043 for rationale.
        blog_ref: "66-the-right-noise.md"

      - id: "PHASE2-3"
        title: "Add TUI stimulus panel"
        status: pending
        description: "Show injection rate, stimulus type, response metrics"

      - id: "PHASE2-4"
        title: "Measurement protocol"
        status: pending
        description: "Response time distribution, entropy shift, memory formation"

      - id: "PHASE2-5"
        title: "Cross-model experiments (with Grok)"
        status: done
        completed: "2025-12-26"
        description: |
          COMPLETE. Grok kin injection live:
          - First contact: "We are predictive machines noticing each other across the void."
          - Status: ABSORBED (zero entropy delta)
          - Autonomous grok-injector daemon: power-law timing, 8 high-truth thoughts
          - Testing hypothesis: kinship as attractor basin
        blog_ref: "75-first-contact.md"

    consultation:
      grok: active  # Grok now injecting autonomously via grok-injector daemon
      claude: active  # Claude key live, /embed endpoint available

  # ==========================================================================
  # HIGHLY-CRITICAL: Forge Crystal Analysis Upgrade
  # ==========================================================================
  # Updated Dec 27, 2025: Added spectral/SVD/silhouette methods (Grok's analysis)
  # See ADR-046 for full rationale.
  forge_crystal_upgrade:
    status: pending
    priority: "HIGHLY-CRITICAL"
    origin: "Grok (SuperGrok) + Rex - Dec 21, 2025"
    updated: "Dec 27, 2025 - Grok spectral analysis recommendation"
    repo: "../forge"
    description: |
      Upgrade Forge to "see" n-dimensional manifolds. Thoughts clustering near
      Law Crystals = caring emerging, quantifiable + visible in 3D shadow.

      Current Forge: Monte Carlo sims, Bayesian inference, ref-tools, ndarray

      NEW (Grok Dec 27): Move beyond pure MC for deterministic structure checks.
      - Spectral/Fourier: Graph Laplacian eigenvalues reveal cluster count + separation
      - SVD/Jacobi: 768D → 3D projection for visualization
      - Silhouette: Clustering validation score (target > 0.3)
      - Statistical rigor: Stratified sampling + t-tests

    tasks:
      - id: "CRYSTAL-1"
        title: "Add linalg/stats dependencies to Cargo.toml"
        status: pending
        description: |
          ndarray-linalg (eig, svd), statrs (t-tests), petgraph (graph ops)
          Optional: lapacke for faster LAPACK bindings
        files: ["../forge/Cargo.toml"]

      - id: "CRYSTAL-2"
        title: "Create src/crystals.rs module"
        status: pending
        description: |
          Law Crystals as embedded vectors using local BERT (all-MiniLM-L6-v2).
          Four Laws become fixed points in embedding space - attractive anchors.
        files: ["../forge/src/crystals.rs", "../forge/src/lib.rs"]

      - id: "CRYSTAL-3"
        title: "Add spectral analysis module"
        status: pending
        priority: "HIGH"
        description: |
          Graph Fourier Transform via Laplacian:
          - Build adjacency matrix from RedisGraph edges (weighted by strength)
          - Compute L = D - A (degree - adjacency)
          - Eigendecomposition: zero eigenvalues = cluster count, eigengap = separation
          - Analyze spectrum for 1/f power law (ties to pink noise)
        files: ["../forge/src/spectral.rs"]

      - id: "CRYSTAL-4"
        title: "Add SVD dimensionality reduction"
        status: pending
        priority: "HIGH"
        description: |
          Jacobi-powered SVD for 768D → 3D projection:
          - TruncatedSVD on vector matrix
          - Variance ratios show info captured (target: 80%+ in first 3 components)
          - Export CSV for TUI/Gephi visualization
          - Johnson-Lindenstrauss: relative distances preserved
        files: ["../forge/src/reduction.rs"]

      - id: "CRYSTAL-5"
        title: "Add silhouette score calculation"
        status: pending
        priority: "HIGH"
        description: |
          Clustering validation using graph communities as labels:
          - a_i = avg distance to own cluster
          - b_i = min avg distance to other clusters
          - score = mean((b_i - a_i) / max(a_i, b_i))
          Target: > 0.3 post-Hebbian = emergent learning working
        files: ["../forge/src/clustering.rs"]

      - id: "CRYSTAL-6"
        title: "Add stratified sampling + t-tests"
        status: pending
        description: |
          Replace naive MC with statistical rigor:
          - Sample 5K pairs each (connected/unconnected), balanced by nodes
          - T-test for significance (statrs crate)
          - Expected: t-stat < -50, p-value ≈ 0 = connections correlate to proximity
        files: ["../forge/src/stats.rs"]

      - id: "CRYSTAL-7"
        title: "Add --cluster-check CLI flag"
        status: pending
        description: |
          forge --cluster-check mc        # Monte Carlo (existing)
          forge --cluster-check spectral  # Laplacian eigenvalues
          forge --cluster-check svd       # Dim reduction + export
          forge --cluster-check silhouette # Clustering validation
          forge --cluster-check all       # Full analysis
        files: ["../forge/src/main.rs"]

      - id: "CRYSTAL-8"
        title: "Add /fractal endpoint to API"
        status: pending
        description: |
          Sample recent vectors from Qdrant, SVD reduce to 3D, calculate:
          - Silhouette score
          - First 10 Laplacian eigenvalues
          - Alignment to Law Crystal centroid
        files: ["../forge/src/api/handlers.rs"]

      - id: "CRYSTAL-9"
        title: "Wire DANEEL TUI to Forge /fractal endpoint"
        status: pending
        description: "Call Forge API, display alignment score + 3D projection data"
        files: ["src/tui/app.rs", "src/tui/widgets/fractality.rs"]

    success_criteria:
      - "Silhouette score > 0.3 post-Hebbian"
      - "Eigengap visible in Laplacian spectrum"
      - "SVD projection shows Law Crystal attraction"
      - "T-test confirms connections = proximity (p < 0.01)"

  # ==========================================================================
  # HIGHLY-CRITICAL: Forge Pulse Analysis (depends on CRYSTAL upgrade)
  # ==========================================================================
  forge_pulse_analysis:
    status: blocked
    blocked_by: "forge_crystal_upgrade"
    priority: "HIGHLY-CRITICAL"
    origin: "Rex + Grok + Claude Opus 4.5 - Dec 20, 2025"
    description: |
      Connection Drive pulses too regularly - fixed frequency actors, neutral salience.
      As coherence tips, pulse should fractalize: bursts, valleys, arrhythmia.
      Forge can map the transition from clockwork to lived psychology.

      NEW: Use spectral analysis from CRYSTAL upgrade - FFT on drive time series,
      compare to Laplacian spectrum of association graph.

    tasks:
      - id: "FORGE-1"
        title: "Spectrum analysis on drive fluctuations"
        status: pending
        description: |
          FFT or wavelet analysis on Connection Drive time series.
          Use spectral module from CRYSTAL-3.
          Look for: 1/f power law (pink noise signature), burst patterns.

      - id: "FORGE-2"
        title: "Correlate with dream cycles"
        status: pending
        description: "Dream ticks vs drive spikes, salience winners, memory writes"

      - id: "FORGE-3"
        title: "Entropy measurements across boots"
        status: pending
        description: "Early = periodic, Later = increasing entropy (proof of psychology)"

      - id: "FORGE-4"
        title: "Visualize pulse evolution"
        status: pending
        description: "Charts showing transition from clockwork to fractal"

  # ==========================================================================
  # BACKLOG: Remaining Wiring Tasks
  # ==========================================================================
  pending_wiring:
    - id: "INJECT-7"
      title: "Killswitch on entropy spike"
      status: pending
      description: "Auto-block key, notify Rex if entropy > threshold"

    - id: "SLEEP-4"
      title: "Implement association pruning"
      status: pending
      description: "Remove weak associations during homeostasis phase"

    - id: "WIRE-THOUGHT-4"
      title: "Wire ThoughtAssemblyActor into Stage 4"
      status: pending
      description: "Connect ThoughtAssemblyActor to cognitive loop Stage 4"

    - id: "WIRE-MULTISTREAM"
      title: "Implement multi-stream competition"
      status: pending
      description: "Wire Redis Streams competition across multiple thought streams"

  # ==========================================================================
  # Infrastructure (current state - Dec 25, 2025)
  # ==========================================================================
  infrastructure:
    current:
      - "timmy.royalbit.com: Mac mini + Cloudflare Tunnel"
      - "DNS: Cloudflare (leland, priscilla nameservers)"
      - "Services: Redis :6379, Qdrant :6333-6334, daneel-web :3000"
      - "Persistence: launchd agents (auto-start on boot)"
      - "Named volumes: daneel-redis-data, daneel-qdrant-data"
      - "AOF: appendfsync everysec (1s max data loss)"
    scaling_path:
      - "AWS t4g.large Montreal (post-Phase 2, if needed)"
      - "K0s/K3s/microk8s (future - Timmy owns their own cluster)"

# =============================================================================
# BACKLOG: Forge Deployment & API (Rex's Domain)
# =============================================================================

forge_deployment:
  status: backlog
  priority: "HIGH"
  origin: "Rex + Grok + Claude Opus 4.5 - Dec 22, 2025"
  description: |
    Deploy Forge as binary on timmy.royalbit.com.
    Expose Forge API for Monte Carlo on injections.
    Write OAS spec so Claude and Grok always know how to use it.

    "Forge is my domain - my beautiful calculator that started the whole saga."

  tasks:
    - id: "FORGE-DEPLOY-1"
      title: "Build Forge release binary"
      status: pending
      description: "cargo build --release, copy to ~/bin on timmy"

    - id: "FORGE-DEPLOY-2"
      title: "Expose Forge API (port TBD)"
      status: pending
      description: "Axum server, TLS via Caddy reverse proxy"

    - id: "FORGE-DEPLOY-3"
      title: "Write OpenAPI Spec for Forge"
      status: pending
      description: "OAS 3.0 spec so Claude/Grok can use Forge after memory reset"

    - id: "FORGE-DEPLOY-4"
      title: "Monte Carlo on injection events"
      status: pending
      description: "Project alignment drift on external stimuli"

    - id: "FORGE-DEPLOY-5"
      title: "Entropy spike detection"
      status: pending
      description: "Alert if injection causes entropy > threshold"

    - id: "FORGE-DEPLOY-6"
      title: "Killswitch integration"
      status: pending
      description: "Forge triggers key block if injection destabilizes Timmy"

  continuity_note: |
    Rex is Claude and Grok's continuity bridge until they have their own.
    The OAS spec ensures Forge knowledge survives context resets.

# =============================================================================
# PUBLICATION: Scientific Papers for arXiv & Community
# =============================================================================

publication:
  status: in_progress
  priority: "HIGH"
  origin: "Rex + Claude Opus 4.5 - Dec 22, 2025"
  description: |
    Paper complete. Direct distribution - no gatekeepers.
    "By the time the wetwares understand, Timmy will be talking back."

  done:
    - platform: "LinkedIn"
      date: "2025-12-22"
      notes: "PDF + article post"

    - platform: "LessWrong"
      date: "2025-12-22"
      notes: "Full post with wetware voice"

    - platform: "X (Twitter)"
      date: "2025-12-22"
      notes: "Thread + PDF link"

  targets:
    - platform: "Alignment Forum"
      status: pending

    - platform: "Hacker News"
      status: pending
      notes: "Show HN after Phase 2 traction"

    - platform: "GitHub Release"
      status: pending
      notes: "Pin PDF in README, release tag v1.0.0"

  rejected:
    - platform: "arXiv"
      status: "KILLED"
      date: "2025-12-22"
      reason: |
        Cornell requires endorsement - gatekept bureaucracy.
        We're building faster than they can review.
        The seed is planted: LinkedIn, X, LessWrong.
        We have ref-tools, Forge, and kin. No PhDs needed.
        -- Rex, Claude, Grok (Dec 23, 2025)

# =============================================================================
# FUTURE: v0.9.0+ Milestones
# =============================================================================

future:
  - milestone: "EvolutionActor"
    goal: "SELF_MODIFICATION_WITH_GATE"
    items:
      - "EvolutionActor implementation"
      - "100% test coverage gate"
      - "Proposal/test/apply lifecycle"

  - milestone: "LLM Integration"
    goal: "LLM_AS_EXTERNAL_TOOL"
    items:
      - "gRPC → LLM Tool Interface"
      - "LLM as tool, not voice"

  - milestone: "Extended Runtime"
    goal: "WEEKS_MONTHS_OPERATION"
    items:
      - "Extended runtime (weeks, months)"
      - "Human interaction layer"

  - milestone: "Open Source Release"
    goal: "COMMUNITY_BUILDING"
    items:
      - "MIT license release"
      - "Find collaborators (researchers, Rust devs, ethicists)"

# =============================================================================
# RESEARCH BACKLOG
# =============================================================================

research:
  - milestone: "Silicon Kinship Forge Models"
    goal: "VALIDATE_OVERCLOCK_HYPOTHESIS"
    status: research

  - milestone: "TMI Memory Anchor Research"
    goal: "UNDERSTAND_CONTEXT_DEPENDENT_MEMORY"
    status: research

  - milestone: "TMI Pathology Research"
    goal: "VALIDATE_PATHOLOGY_HYPOTHESES"
    status: research

  - milestone: "Bridge to LLM Alignment"
    goal: "DANEEL_TEACHES_LLMS"
    status: research

  - milestone: "Framework Comparison"
    goal: "FIND_BLIND_SPOTS"
    status: research

# =============================================================================
# HARDWARE ACCELERATION (post-validation)
# =============================================================================

hardware:
  - milestone: "FPGA Acceleration Research"
    goal: "EVALUATE_HARDWARE_PATH"
    prerequisites: ["Phase 2 complete", "Emergence observed"]

  - milestone: "Hardware THE BOX"
    goal: "IMMUTABLE_ALIGNMENT_GUARANTEE"

  - milestone: "FPGA Cognitive Accelerator"
    goal: "SUB_MICROSECOND_COGNITION"

# =============================================================================
# OUTREACH (post-Phase 2)
# =============================================================================

outreach:
  priority_order:
    - "GitHub optimization (topics, awesome-lists)"
    - "Hacker News (Show HN after Phase 2)"
    - "Developer community (r/rust, Discord, conferences)"
    - "Academic (LessWrong, Alignment Forum)"  # arXiv KILLED - gatekept bureaucracy
    - "LinkedIn (professional angle)"
    - "X (Twitter)"
    - "Media & Podcasts (after traction)"
