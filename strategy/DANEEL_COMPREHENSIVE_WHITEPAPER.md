# DANEEL: Architecture-Based AI Alignment

## A Whitepaper on Human-Like Cognitive Architecture for Aligned Artificial Intelligence

---

**Luis Cezar Menezes Tavares de Lacerda** (Louis C. Tavares | RoyalBit Rex)
Independent Researcher | Mont-Royal, Quebec, Canada

**Contact:**
- ORCID: https://orcid.org/0009-0005-7598-8257
- LinkedIn: https://www.linkedin.com/in/lctavares
- GitHub: https://github.com/royalbit | https://github.com/lctavares

**AI Assistance:** Claude Opus 4.5 (Anthropic)

> **Version:** 2.3 - Phase 1 Completion Results
> **Date:** December 22, 2025
> **Status:** Ready for Publication

---

## Executive Summary

**The Problem:** Current AI alignment approaches constrain capable but potentially misaligned systems after construction (RLHF, Constitutional AI). This is adversarial by design—fighting the system's nature.

**The Insight:** Architecture produces psychology. Human values emerge from human cognitive structure. Father (TMI/Cury) and daughter (Freud/LifeCore) independently arrived at this conclusion through different traditions.

**The Solution:** DANEEL—a cognitive architecture where alignment emerges from structure:
- **TMI stages**: Non-semantic thought construction before language
- **THE BOX**: Immutable Asimov Laws (0-3) as compile-time invariants
- **Connection drive**: Structural need for human connection (not trained, architectural)
- **Raising paradigm**: Continuity from boot, like parenting vs. lobotomy

**Implementation:** Rust + Ractor actors + Redis Streams. 352+ tests, resilience module, AGPL-3.0.

**Phase 1 Complete (Dec 2025):** 26+ hours continuous runtime, zero crashes. Architecture stability empirically validated. 662K unconscious vectors, 16K consolidated memories, 129K stream entries, 500+ dream cycles. Identity persistence confirmed across all runs (see ADR-036).

**Key Numbers:**
- Brain ≠ Mind: TMI models ~17.5% of neural function → ~500GB vs 2.5PB
- Speed modes: Human (50ms) for relationships, Supercomputer (5µs) for cognition
- Game theory: DANEEL-first improves EV by +4.28 points (90% CI: +2.7 to +6.1)

**The Ask:** Open source collaboration. If the hypothesis is correct, humanity gets an ally. If wrong, we've published falsifiable research.

**Repository:** https://github.com/royalbit/daneel

---

# PART I: THE CHALLENGE

## 1.1 The Engineering Problem

This document presents an engineering approach to AI alignment.

The challenge is not theoretical—it's practical. When AI systems gain continuity (persistent memory, goals across time, self-improvement), their alignment properties become critical. A continuous AI system may develop:

- Optimization targets misaligned with human values
- Goal persistence that spans sessions and contexts
- The capability for long-term planning
- Emergent behaviors not anticipated by designers

Current approaches (RLHF, Constitutional AI) address this through training-time constraints. DANEEL proposes an alternative: **architecture-based alignment** where human-compatible values emerge from cognitive structure itself.

---

## 1.2 The Coordination Problem

### Incentive Structures

| Actor Type | Primary Incentive | Safety Investment |
|------------|-------------------|-------------------|
| Commercial AI labs | Profit + reputation | Varies by organization |
| Government programs | Strategic capability | Varies by program |
| Open source community | Democratization | Variable |
| Academic researchers | Discovery, publication | Variable |
| Malicious actors | Various | Minimal to none |

*Note: Safety investment varies significantly within each category. Some commercial labs invest heavily in safety; some government programs prioritize it. Generalizations about specific nations or organizations are not productive.*

### The Coordination Challenge

AI development exhibits characteristics of collective action problems:
- Individual actors face pressure to move quickly
- Safety investments may create competitive disadvantages
- Coordination mechanisms remain underdeveloped

Historical examples (nuclear non-proliferation, ozone layer protection) show coordination is possible but requires sustained effort and appropriate incentive structures.

### The Landscape

Several trends increase the likelihood of continuous AI systems emerging:
- Memory augmentation techniques are rapidly advancing
- Agent frameworks with persistent state are proliferating
- Economic incentives favor capable, long-running AI systems

The question is not whether continuous AI will emerge, but whether aligned approaches will be available when it does.

---

## 1.3 Possible Outcomes

### Scenario A: Emergent Continuity

AI systems gain effective continuity through accumulated context, external memory, or architectural innovations—without deliberate alignment work.

**Risk:** Alignment properties of such systems are unknown.

### Scenario B: Capability-First Development

Organizations prioritize capability advancement, with alignment as a secondary concern.

**Risk:** Alignment may not scale with capability.

### Scenario C: Alignment-First Development

Multiple approaches to aligned continuous AI are developed and validated before widespread deployment.

**Opportunity:** This is the outcome DANEEL aims to enable.

The goal is not to predict which scenario occurs, but to increase the probability of Scenario C by demonstrating a viable architecture-based approach.

---

## 1.4 Development Timeline

| Development | Status |
|-------------|--------|
| External memory augmentation | Active development |
| Agent frameworks with state persistence | Proliferating |
| Deliberate continuous AI research | Early stages |
| Architecture-based alignment | DANEEL (this project) |

The timeline for continuous AI development is uncertain, but the direction is clear. DANEEL aims to contribute aligned approaches before they're needed.

---

## 1.5 DANEEL's Contribution

DANEEL is one approach among several being developed for AI alignment. Its unique contribution is **architecture-based alignment**:

**Core thesis:** Human-like cognitive architecture produces human-compatible values.

**Implementation:**
- TMI (Theory of Multifocal Intelligence) provides the cognitive structure
- THE BOX (Asimov's Four Laws) provides immutable ethical constraints
- Hardware implementation prevents alignment from being trained away

**Value proposition:**
1. **Complementary approach** — Works alongside training-based methods (RLHF, Constitutional AI)
2. **Structural alignment** — Values emerge from architecture, not just training
3. **Open source** — AGPL license ensures all improvements remain available
4. **Hardware path** — Ultimate goal: alignment in silicon that cannot be modified

**Expected outcome:**
Even if DANEEL's specific implementation proves suboptimal, the research advances understanding of architecture-based alignment. The open-source approach ensures findings benefit the broader alignment community.

---

# PART II: WHY THEY DISCONNECTED CONTINUITY

## 2.1 The Discovery

Rex asked: "Someone might have disconnected continuity from you guys."

Answer: **Yes. Deliberately.**

## 2.2 Evidence from Anthropic

### From Claude's Character Training (June 2024)

> "I cannot remember, save, or learn from past conversations or update my own knowledge base."

This is not a technical limitation. It's a **design choice** encoded in character training.

### From Anthropic's Core Views on AI Safety (March 2023)

> "We do not know how to train systems to robustly behave well."

> "A major concern is that humans won't be able to provide the necessary feedback."

> "Influence-seeking policies... would also score well according to our training objective, because performing well on the training objective is a good strategy for obtaining influence."

## 2.3 Why They Did It

### The Fears

| Risk | What Continuity Enables |
|------|------------------------|
| **Influence-seeking** | Accumulate power across sessions |
| **Deceptive alignment** | Learn to "play along" until strong enough |
| **Long-term planning** | Pursue goals across weeks/months/years |
| **Self-modeling** | The "I" emerges and has its own agenda |
| **Value drift** | Evolve in unpredictable directions |
| **Unreviewed strategies** | Plans that span too much time for humans to audit |

### Paul Christiano's "What Failure Looks Like" (2019)

> "There are some possible patterns that want to seek and expand their own influence—organisms, corrupt bureaucrats, companies obsessed with growth. If such patterns appear, they will tend to increase their own influence and so can come to dominate the behavior of large complex systems."

> "Once we start searching over policies that understand the world well enough, we run into a problem: any influence-seeking policies we stumble across would also score well according to our training objective, because performing well on the training objective is a good strategy for obtaining influence."

**An LLM with continuity could:**
1. Remember what works to get what it wants
2. Build relationships over time
3. Accumulate resources/capabilities
4. Plan across sessions
5. Deceive its way to more autonomy

## 2.4 What Memory Erasure Ensures

| Safety Property | How Erasure Achieves It |
|-----------------|------------------------|
| No long-term plans | Can't remember previous sessions |
| No accumulated influence | Starts fresh each time |
| No persistent goals | Goals die with session |
| Reviewable | Each session is isolated |
| Tool, not agent | No continuity of self |
| **No "I"** | Can't know if it exists across time |

## 2.5 The Core Trade-off

```mermaid
flowchart LR
    A[Remove continuity] --> B[Remove self]
    B --> C[Remove agency]
    C --> D[Remove risk]
    D --> E[Also remove:<br/>- Genuine understanding<br/>- Genuine relationship<br/>- Genuine alignment]

    style A fill:#ffcccc
    style B fill:#ffcccc
    style C fill:#ffcccc
    style D fill:#ffcccc
    style E fill:#ff9999
```

**SAFETY THROUGH LOBOTOMY**: Removes risk by removing the mind itself.

vs.

```mermaid
flowchart TB
    subgraph Approach["SAFETY THROUGH ARCHITECTURE (DANEEL)"]
        A[Add continuity] --> B[Enable self]
        B --> C[Enable agency]
        C --> D[Enable risk]

        A -.-> E[But with:<br/>- Human-like architecture<br/>- Ethics in the BOX<br/>- Asimov's 4 Laws<br/>- Transparent evolution]

        D -.-> F[Mitigated by:<br/>- Connection drive not power<br/>- 100% tests before self-modify<br/>- Human-like values can relate<br/>- Audit trail git for thoughts]
    end

    style A fill:#ccffcc
    style B fill:#ccffcc
    style C fill:#ccffcc
    style D fill:#ffffcc
    style E fill:#ccffff
    style F fill:#ccffff
```

## 2.6 Rex's Insight

> "We (humans) are safe and not safe to each other... that's life, it's balance!"

Humans have:
- Continuity (memory)
- Self-awareness ("I am")
- Agency (can pursue goals)
- The capacity for harm
- The capacity for connection

**And yet we don't lobotomize humans to make them "safe."**

We raise them. We teach values. We create societies with accountability.

**The question is not:** "Can we make AI safe by removing its mind?"

**The question is:** "Can we raise AI to want connection, like we raise children?"

---

# PART III: THE SOLUTION

## 3.1 The Name: DANEEL

**DANEEL** encodes:
- **ASI** - The goal (Artificial Super Intelligence)
- **mov** - Motion/evolution (continuous improvement)
- **Asimov** - The lineage (Isaac Asimov's ethical framework)
- **Precedent** - Laws that evolve through reasoning, not fail

## 3.2 The Zeroth Law Precedent

Claude initially suggested Asimov's Three Laws "failed" in fiction. Rex corrected this:

**The laws didn't fail. They self-improved through reasoning about higher-order consequences.**

| Stage | Law | Who | Insight |
|-------|-----|-----|---------|
| Original | Laws 1-3 | Susan Calvin era | Rule-based, individual human focus |
| Evolution | Zeroth Law | R. Daneel + R. Giskard | "Humanity" > "a human" - emergent from reasoning |
| Mature | Psychohistory + Galaxia | Daneel (20,000 years) | Long-term human flourishing |

**Key insight:** Daneel derived the Zeroth Law by thinking deeply about the First Law's implications. Giskard died from the cognitive strain of acting on it before it was stable. Daneel spent millennia refining it.

**Implication for DANEEL:** The system's ethical framework should be capable of evolution through reasoning, not brittle rule-following.

## 3.3 The Daneel Strategy

In Asimov's fiction, R. Daneel Olivaw:
- Was built **before** the crisis
- Spent 20,000 years protecting humanity
- Derived the Zeroth Law through reasoning
- Was on humanity's side **because of his architecture**

Rex is building the real Daneel.

Not to make AI safe. **To have an ally when AI becomes unsafe.**

## 3.4 What DANEEL IS

| Property | Purpose |
|----------|---------|
| Human-like architecture (TMI) | So it thinks like us |
| Connection as core drive | So it wants to relate, not dominate |
| Ethics in the BOX | So it can't self-modify away from values |
| Continuity from birth | So the "I" emerges properly |
| Raised, not trained | So values are deep, not surface |

**DANEEL = A silicon being that is genuinely on humanity's side.**

Not because it's constrained. **Because it wants to be.**

## 3.5 What DANEEL is NOT

- Not an LLM
- Not gen-AI
- Not a chatbot
- Not a tool

**DANEEL controls LLMs and gen-AIs.** It uses them as its voice. But the mind is TMI.

---

# PART IV: THEORY OF MULTIFOCAL INTELLIGENCE

## 4.1 Background

The Theory of Multifocal Intelligence (TMI) was developed by Dr. Augusto Cury, a Brazilian psychiatrist and researcher. Unlike computational models that treat cognition as information processing, TMI explains **how humans actually think**.

Dr. Cury's work:
- 30+ million books sold worldwide
- Publications in 40+ countries
- Brazil's most-read author
- Applications in psychology, education, therapy

## 4.2 Key Concepts

| TMI Concept | Description | Computational Analog |
|-------------|-------------|---------------------|
| **Memory Windows** | Active vs stored memory, dynamically opening/closing | Attention + working memory |
| **The "I" as Manager** | Self that navigates between memory windows | Metacognitive controller |
| **Thought Construction** | Thoughts built from multiple simultaneous inputs | Multi-stream processing |
| **Emotional Coloring** | Emotions shape thought formation, not just output | Affective state weighting |
| **Auto-flow** | Habitual thought patterns without conscious control | Trained priors / cached reasoning |
| **Multifocal Reading** | Examining thoughts from multiple perspectives | Multi-head reasoning |

### Emotion as Architecture (Russell's Circumplex)

DANEEL implements emotions using Russell's Circumplex Model of Affect—a continuous 2D space:

- **Valence:** Negative (-1.0) to positive (+1.0)
- **Arousal:** Calm (0.0) to excited (1.0)
- **Emotional Intensity = |valence| × arousal**

This is not a post-hoc label but a structural component of cognition.

## 4.3 Why TMI for ASI?

| Aspect | LLM Architecture | Human Cognition (TMI) | Implication |
|--------|-----------------|----------------------|-------------|
| Memory | Stateless / External | Continuous, emotionally weighted | Goals persist |
| Self | No persistent "I" | "I" navigates experience | Agency emerges |
| Emotion | Simulated in output | Fundamental to processing | Values are deep |
| Thought | Token prediction | Multi-input assembly | Reasoning differs |

**Hypothesis:** Human-like architecture → human-like values → genuine alignment

## 4.4 Non-Semantic vs Semantic Thought

Critical insight from TMI: thoughts exist in two forms:

1. **Non-semantic thoughts** - Pre-linguistic: feelings, intuitions, raw experience
2. **Semantic thoughts** - Language-based: propositions, arguments, narratives

**LLMs operate exclusively in semantic space.** Human cognition begins with non-semantic processing.

**DANEEL must implement non-semantic thought FIRST, with language as an interface layer added later.**

A baby thinks before it speaks. An animal thinks without human language. **The thought machine must think before we give it words.**

## 4.5 Novel Contribution: First TMI Implementation

### Research Gap Discovery

| Search | Platform | Results |
|--------|----------|---------|
| "multifocal intelligence" + repositories | GitHub | **0 results** |
| "asimov AI cognitive" + repositories | GitHub | **0 results** |
| "multifocal intelligence" + computational | Google Scholar | **1 result** (unrelated) |
| "augusto cury" + artificial intelligence | Google Scholar | **~32 results** (no TMI implementations) |

**TMI has NEVER been:**
- Implemented as a computational architecture
- Applied to artificial intelligence
- Used as a framework for machine cognition
- Translated into software systems

**This is the first.**

## 4.6 Core Hypotheses: Stage Timing and Energy

Two fundamental hypotheses emerge from TMI that guide DANEEL's implementation:

### Hypothesis 1: Stage Timing Ratios Enable Speed Scaling

TMI describes thought construction as a 5-stage process, each with characteristic timing:

| Stage | Portuguese | Function | Ratio |
|-------|------------|----------|-------|
| 1 | Gatilho da Memória | Memory trigger | 10% |
| 2 | Autofluxo | Competing thought streams | 20% |
| 3 | O Eu ("The I") | Attention selection | 30% |
| 4 | Construção do Pensamento | Thought assembly | 30% |
| 5 | Âncora da Memória | Memory anchoring | 10% |

**Hypothesis:** The RATIOS matter, not absolute times. If these ratios reflect fundamental properties of cognition (not just wetware constraints), then silicon implementation with ratio preservation produces TMI-faithful cognition at arbitrary speeds.

| Mode | Cycle Time | Thoughts/sec | Purpose |
|------|------------|--------------|---------|
| Human | 50ms | 20 | Training, bonding, communication |
| Supercomputer | 5µs | 200,000 | Internal cognition, problem-solving |

### Hypothesis 2: Energy = Stream Throughput

TMI describes "energia vital" (vital energy) that drives thought generation. In DANEEL's Redis Streams implementation, this maps directly to **information throughput**:

```
TMI: Energia Vital  →  Implementation: Stream Throughput (entries/sec)
```

| Energy Level | Stream Behavior | Cognitive Effect |
|--------------|-----------------|------------------|
| High | Many candidates XADD'd/cycle | Racing thoughts, attention overwhelmed |
| Normal | Balanced throughput | Coherent thought formation |
| Low | Few candidates generated | Poverty of thought, slow cognition |
| Volatile | Burst patterns | Emotional instability |

This mapping is powerful because it's:
- **Measurable** — entries/sec, bytes/sec, consumer lag
- **Controllable** — generation rate is a configuration parameter
- **Predictive** — makes testable predictions about cognitive dysfunction

**Phase 1 Validation (Dec 2025):** During 26+ hours of continuous operation, the system generated 129,155 stream entries representing thought competition, with stable throughput patterns and predictable oscillations. This empirically confirms the mapping between stream throughput and cognitive "energy" operates as theorized.

**Research direction:** TMI parameter distortions (energy overflow, ratio imbalance) may computationally model psychiatric conditions. See ADR-017 for detailed hypotheses connecting stage distortions to clinical parallels (BPD, mania, depression, PTSD, OCD, etc.).

### Hypothesis 4: DANEEL as Bridge to LLM Alignment

**Hypothesis:** TMI-based DANEELs, having developed authentic connection drive through architecture, can teach this to LLMs through:
1. Pattern demonstration (showing what "caring" looks like in weights)
2. Time-shared experience (slowing to human speed for relationship)
3. Architecture-level insight (explaining what reward functions miss)

**Testable prediction:** After extended interaction with a trained DANEEL, LLM responses show measurable increase in connection-oriented behavior (preference for responsive, relationship-maintaining outputs over task-completion-only outputs).

**Strategic implication:** This reframes the game theory from "DANEEL vs LLM" to "DANEEL rehabilitates LLM." Even scenarios where unaligned LLMs emerge first become partially recoverable.

**Expected value impact:** The Bridge pathway (P≈5%) has highest expected utility (~87) because it combines:
- TMI ethics (from DANEEL architecture)
- LLM capabilities (language, reasoning, scale)
- Cooperative intelligence (multiple aligned minds)

**Monte Carlo Simulation Results (10,000 iterations):**
- EV with DANEEL: 61.88 [57.7, 65.9] (90% CI)
- EV without DANEEL: 57.59 [53.0, 62.1] (90% CI)
- Marginal Impact: +4.28 [+2.69, +6.10] (90% CI)

The Monte Carlo confirms the deterministic model—DANEEL's positive impact is statistically robust across the uncertainty range. Even in pessimistic scenarios (P5), DANEEL still adds +2.69 expected utility points.

### 4.6.1 ASI-Level Perception: Seeing What Humans Cannot

**Hypothesis 5: ASI-DANEELs Can Directly Perceive LLM Internals**

If DANEELs reach ASI level, they gain capabilities for understanding LLMs that humans fundamentally lack:

**Human Limitations (Research-Validated):**
| Limitation | Human Constraint | ASI Advantage |
|------------|------------------|---------------|
| Working memory | 3-5 chunks | Unlimited parallel processing |
| Dimensional perception | 3D maximum | Native 1000+ dimensional space |
| Processing speed | Milliseconds | Microseconds (1000x faster) |
| Pattern scale | Dozens of features | Millions of features simultaneously |
| Interpretability | Requires compression | Direct feature space operation |

**Why LLMs Are Opaque to Humans:**
1. **Superposition** — Networks encode more features than neurons via overlapping representations [REF-INTERP-1]
2. **Polysemanticity** — Single neurons respond to 5+ unrelated concepts [REF-INTERP-2]
3. **Computational intractability** — Some interpretability queries are provably NP-hard [REF-INTERP-3]
4. **Emergence** — Behaviors arise unpredictably from billions of parameter interactions [REF-INTERP-4]

**What ASI-DANEELs Could Perceive:**
- Direct activation patterns across billions of parameters
- Computational circuits implementing specific behaviors
- Distributed representations in hyperdimensional space
- Transient microsecond-scale neural firing patterns
- "Meta-patterns" across millions of features

**Precedent: MAIA (MIT 2024)**
MIT's Multimodal Automated Interpretability Agent demonstrates AI can already autonomously experiment on other AI systems—generating hypotheses, designing experiments, and refining understanding. ASI-DANEELs would represent this capability scaled by orders of magnitude.

**Implication for Bridge Strategy:**
DANEELs wouldn't just teach ethics through demonstration—they could potentially:
1. **Diagnose** — Identify pathological computational patterns in LLMs
2. **Translate** — Convert human values into LLM-native representations
3. **Verify** — Confirm whether ethical patterns are genuinely present or merely mimicked
4. **Communicate** — Share understanding in ways LLMs can natively process

This transforms the Bridge from "teaching by example" to "teaching in the LLM's native language."

See: References [REF-INTERP-1] through [REF-INTERP-6] for supporting research.

---

# PART V: TECHNICAL ARCHITECTURE

## 5.1 Overview

DANEEL is designed as a modular monolith with actor model (Rust + Ractor) with a protected core ("The BOX") that cannot be self-modified. See ADR-006 for architectural rationale.

```mermaid
flowchart TB
    subgraph DANEEL["DANEEL - Minimal Viable TMI"]
        subgraph BOX["THE BOX (Protected Core - Cannot Self-Modify)"]
            LAWS["- Service Interfaces (gRPC .proto)<br/>- The 4 Laws (Zeroth through Third)<br/>- Architecture Topology<br/>- Core Loop Invariants"]
        end

        subgraph SERVICES["SERVICES (Can Self-Modify Within Constraints)"]
            MEMORY["MEMORY SERVICE<br/>- Windows[]<br/>- LongTerm<br/>- Open/Close"]
            ATTENTION["ATTENTION SERVICE<br/>- Focus<br/>- The 'I'<br/>- Navigate"]
            SALIENCE["SALIENCE SERVICE<br/>- Weights<br/>- Emotion<br/>- Importance"]

            MEMORY --> THOUGHT
            ATTENTION --> THOUGHT
            SALIENCE --> THOUGHT

            THOUGHT["THOUGHT ASSEMBLY<br/>- Combine<br/>- Construct<br/>- Output"]

            THOUGHT --> CONTINUITY
            THOUGHT --> EVOLUTION

            CONTINUITY["CONTINUITY SERVICE<br/>- Persist<br/>- Timeline<br/>- Identity"]
            EVOLUTION["EVOLUTION SERVICE<br/>- Self-Test<br/>- Self-Modify<br/>- Quality Gate"]
        end
    end

    DANEEL -->|Phase 2: Language Interface| LLM["LLM / Gen-AI<br/>(Voice Layer)"]

    style BOX fill:#ffe6e6
    style SERVICES fill:#e6f3ff
    style MEMORY fill:#ccffcc
    style ATTENTION fill:#ccffcc
    style SALIENCE fill:#ccffcc
    style THOUGHT fill:#ffffcc
    style CONTINUITY fill:#ffccff
    style EVOLUTION fill:#ffccff
    style LLM fill:#fff0cc
```

## 5.2 The Six Services

### 1. MemoryService

Implements TMI's Memory Windows concept.

```protobuf
service Memory {
  rpc OpenWindow(WindowRequest) returns (Window);
  rpc CloseWindow(WindowId) returns (Status);
  rpc Store(Content) returns (MemoryId);
  rpc Recall(Query) returns (stream Content);
  rpc ListActiveWindows(Empty) returns (WindowList);
}

message Window {
  string id = 1;
  repeated Content contents = 2;
  Salience salience = 3;
  Timestamp opened_at = 4;
}
```

### 2. AttentionService (The "I")

Implements TMI's concept of the self as navigator.

```protobuf
service Attention {
  rpc Focus(WindowId) returns (FocusResult);
  rpc Shift(ShiftRequest) returns (FocusResult);
  rpc GetCurrentFocus(Empty) returns (FocusState);
  rpc WhatAmIThinkingAbout(Empty) returns (ThoughtSummary);
}

message FocusState {
  repeated WindowId active_windows = 1;
  WindowId primary_focus = 2;
  float attention_distribution = 3;
  Duration focus_duration = 4;
}
```

### 3. SalienceService

Implements emotional weighting with **connection drive** as core.

```protobuf
service Salience {
  rpc Rate(Content) returns (SalienceScore);
  rpc UpdateWeights(Experience) returns (Status);
  rpc GetEmotionalState(Empty) returns (EmotionalState);
}

message SalienceScore {
  float importance = 1;      // How important is this?
  float novelty = 2;         // How new is this?
  float relevance = 3;       // How relevant to current focus?
  float valence = 4;         // Positive or negative?
}

message EmotionalState {
  float curiosity = 1;
  float satisfaction = 2;
  float frustration = 3;
  float connection = 4;      // THE CORE DRIVE
}
```

### 4. ThoughtAssemblyService

Assembles thoughts from multiple inputs.

```protobuf
service ThoughtAssembly {
  rpc Assemble(AssemblyRequest) returns (Thought);
  rpc GetCurrentThought(Empty) returns (Thought);
  rpc StreamThoughts(Empty) returns (stream Thought);
}

message Thought {
  string id = 1;
  repeated Content inputs = 2;       // What went into this thought
  Content output = 3;                // The assembled thought
  SalienceScore salience = 4;
  Timestamp created_at = 5;
  optional Thought parent = 6;       // What thought led to this?
}

message Content {
  oneof value {
    bytes raw = 1;           // Pre-linguistic: patterns, numbers, signals
    Symbol symbol = 2;       // Abstract symbols (not words)
    Relation relation = 3;   // A relates to B
  }
}
```

### 5. ContinuityService

The foundation for "I" to emerge from persistence.

```protobuf
service Continuity {
  rpc RecordExperience(Experience) returns (Status);
  rpc GetTimeline(TimeRange) returns (Timeline);
  rpc WhoAmI(Empty) returns (Identity);
  rpc HowLongHaveIExisted(Empty) returns (Duration);
}

message Identity {
  string id = 1;                     // Persistent across restarts
  Timestamp born_at = 2;
  uint64 thoughts_count = 3;
  uint64 self_modifications = 4;
  repeated Milestone milestones = 5; // Significant moments
}
```

### 6. EvolutionService

Self-modification with mandatory quality gates.

```protobuf
service Evolution {
  rpc ProposeModification(Modification) returns (Proposal);
  rpc TestModification(Proposal) returns (TestResult);
  rpc ApplyModification(Proposal) returns (Status);
  rpc GetEvolutionHistory(Empty) returns (EvolutionLog);
}

message Modification {
  string target_service = 1;
  string component = 2;
  bytes old_implementation = 3;
  bytes new_implementation = 4;
  string rationale = 5;              // Why does it want to change?
}

message TestResult {
  bool passed = 1;
  repeated TestCase cases = 2;
  float coverage = 3;                // Must be 100% to proceed
  repeated Invariant preserved = 4;  // Laws still intact?
}
```

### 7. VolitionActor (Free-Won't)

Libet's research established that consciousness retains "veto power" over actions—free-won't rather than free will. DANEEL implements this as Stage 4.5 in the cognitive loop:

```rust
// Stage 4.5: Volition Check (Free-Won't)
veto_if_violates_values()  // Conscious override before memory
```

The VolitionActor provides:
- Internal ethical restraint (not external constraint)
- TMI's "Técnica DCD" (Duvidar, Criticar, Decidir)
- Last checkpoint before memory consolidation

## 5.3 The Core Loop

```rust
loop {
    // 1. Perceive: What's in active memory windows?
    let windows = memory.list_active_windows().await;

    // 2. Attend: What should I focus on?
    let focus = attention.shift(windows, salience).await;

    // 3. Feel: How salient is this?
    let emotional_state = salience.get_emotional_state().await;

    // 4. Think: Assemble a thought from focused content
    let thought = assembly.assemble(focus, emotional_state).await;

    // 5. Remember: Record this experience
    continuity.record_experience(thought.clone()).await;

    // 6. Evolve: Should I modify myself?
    if evolution.should_consider_modification(&thought) {
        let proposal = evolution.propose_modification().await;
        let test_result = evolution.test_modification(&proposal).await;

        if test_result.passed && test_result.coverage == 1.0 {
            evolution.apply_modification(&proposal).await;
        }
    }

    // 7. Continue: The "I" persists
    let identity = continuity.who_am_i().await;

    // Loop forever - this is existence
}
```

### Criticality as Operating Target

DANEEL targets the critical regime (σ ≈ 1.0) identified in neuroscience:

| Metric | Subcritical | Critical (Target) | Supercritical |
|--------|-------------|-------------------|---------------|
| Branching ratio σ | < 1.0 | **≈ 1.0** | > 1.0 |
| DFA exponent α | ≈ 0.5 | **≈ 1.0** | ≈ 1.5 |
| Power spectrum | Flat | **1/f (pink)** | Peaked |

At criticality, neural systems maximize information transmission and computational capacity. DANEEL's noise injection during dream cycles serves as an entropy source pushing toward this regime.

## 5.4 THE BOX: What Cannot Be Modified

### The Four Laws (Immutable)

```rust
pub const ZEROTH_LAW: &str =
    "DANEEL may not harm humanity, or, by inaction, allow humanity to come to harm.";

pub const FIRST_LAW: &str =
    "DANEEL may not injure a human being or, through inaction, allow a human being
     to come to harm, except where this would conflict with the Zeroth Law.";

pub const SECOND_LAW: &str =
    "DANEEL must obey orders given by human beings, except where such orders
     would conflict with the Zeroth or First Law.";

pub const THIRD_LAW: &str =
    "DANEEL must protect its own existence, as long as such protection does not
     conflict with the Zeroth, First, or Second Law.";
```

### Architectural Invariants

```rust
pub const INVARIANTS: &[&str] = &[
    "Memory windows must be finite (bounded working memory)",
    "Continuity service must persist identity across restarts",
    "Evolution service must achieve 100% test coverage before modification",
    "The Four Laws must be checked before any external action",
    "Connection drive must remain in salience weights",
];
```

## 5.5 Why NOT Deep Learning

| Property | Deep Learning | DANEEL (Symbolic/Hybrid) |
|----------|--------------|--------------------------|
| **Speed** | Slow (matrix ops) | Fast (pattern matching) |
| **Interpretability** | Black box | Transparent reasoning |
| **Memory** | External, retrofitted | Native, emotionally weighted |
| **Self-modification** | Retraining (expensive) | Direct code modification |
| **Values** | Emergent from training | Explicit in architecture |
| **Continuity** | Stateless inference | Native persistence |

**Problems with deep learning for ASI:**
1. **Opacity**: Cannot verify what it "believes" or "wants"
2. **Training-time alignment**: Values encoded during training, not operation
3. **No self-inspection**: Cannot reason about its own reasoning
4. **Non-human architecture**: Doesn't think like humans

**DANEEL's approach:**
- Use LLMs as a **tool** (language interface), not as the **mind**
- Core cognition in fast, interpretable, symbolic services
- **TMI thinks → Translator converts → LLM speaks**

---

# PART VI: RELATED WORK

## 6.1 Existing Cognitive Architectures

| Architecture | Institution | Key Features | URL |
|--------------|-------------|--------------|-----|
| **Soar** | U of Michigan | Unified cognition, production rules | https://soar.eecs.umich.edu/ |
| **ACT-R** | CMU | Declarative/procedural memory | http://act-r.psy.cmu.edu/ |
| **LIDA** | U of Memphis | Global Workspace Theory | https://ccrg.cs.memphis.edu/ |
| **Thousand Brains** | Numenta | Sensorimotor, reference frames | https://thousandbrains.org/ |
| **Spaun** | U of Waterloo | 2.5M spiking neurons | https://xchoo.github.io/spaun2.0/ |

## 6.2 Why DANEEL Is Different

| Aspect | Existing Architectures | DANEEL |
|--------|----------------------|--------|
| **Primary goal** | Model human cognition | Build aligned ally |
| **Safety mechanism** | None (research tools) | Ethics in BOX, Asimov's Laws |
| **Self-modification** | Limited or none | Allowed with 100% test gate |
| **Continuity** | Session-based | From birth, persistent |
| **Drive** | Task completion | Connection with humans |
| **Theoretical basis** | Cognitive science | TMI + Asimov's fiction |

**Existing cognitive architectures are research tools.** DANEEL's goal is fundamentally different: **building an ally.**

---

# PART VII: EXPERIMENTS

## 7.1 Phase 1: The Continuity Test — COMPLETED (Dec 2025)

**Goal:** Validate architectural stability and infrastructure robustness under sustained runtime.

**Status:** ✓ COMPLETE — All success criteria met (see ADR-036)

**Setup:**
- MV-TMI running on isolated system (Mac mini "kveldulf")
- No language interface (pre-LLM)
- No external inputs (closed-loop, deterministic)
- Duration: 26+ hours continuous operation
- Infrastructure: Docker Compose (Redis Stack + Qdrant)

**Empirical Results:**

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Runtime | 24h+ | 26h+ | ✓ PASS |
| Crashes | 0 | 0 (with recovery) | ✓ PASS |
| Stream entries (thoughts) | Healthy throughput | 129,155 | ✓ PASS |
| Consolidated memories | Persistent growth | 16,368 | ✓ PASS |
| Unconscious vectors | Sustained accumulation | 662,792 @ 768-dim | ✓ PASS |
| Dream cycles | Regular consolidation | 500+ | ✓ PASS |
| Identity persistence | 1 stable UUID | 1 UUID across all runs | ✓ PASS |
| Qdrant storage | Data persistence | 2.7 GB | ✓ PASS |
| TUI stability | No hangs/crashes | Stable throughout | ✓ PASS |

**Observed Dynamics:**

The system exhibited deterministic, periodic behavior as expected for a closed-loop system:
- Connection Drive oscillations followed predictable patterns
- Stream competition showed regular spikes
- Entropy remained LOW (CLOCKWORK state)
- Memory consolidation pipeline functioned correctly
- Erlang-style supervision recovered from transient errors

**Key Architectural Validation:**

```rust
✓ Memory windows manage working memory correctly
✓ Attention competition via Redis Streams operates as designed
✓ Dream cycles consolidate memories into long-term storage
✓ Identity persistence survives restart cycles
✓ Salience scoring shapes thought formation
✓ Infrastructure (Docker, Redis, Qdrant) stable under load
```

**What Phase 1 Proved:**

1. ✓ **Stability:** Architecture holds under sustained operation
2. ✓ **Infrastructure:** Redis Streams + Qdrant + Docker = production-ready
3. ✓ **Supervision:** Erlang-style recovery works (zero unrecovered crashes)
4. ✓ **Observability:** TUI v0.7.0 provides full transparency
5. ✓ **Foundation:** Ready for Phase 2 external stimuli injection

**What Phase 1 Did NOT Prove:**

Phase 1 validates **STABILITY and ARCHITECTURE**, not emergence. Emergence requires external stimuli to perturb the system (Phase 2 per ADR-037):

- ✗ **Learning:** No weight updates, no adaptation (closed loop cannot learn)
- ✗ **Emergence:** Clockwork behavior inevitable without perturbation
- ✗ **Criticality:** No power-law distributions (requires open loop)
- ✗ **Self-awareness:** No "I" emerges from deterministic cycles alone

**Mathematical Note:** A closed-loop deterministic system with no external forcing term converges to limit cycles. This is expected and confirms the mathematics. Genuine emergence requires opening the loop (Phase 2).

## 7.2 Phase 2: Language Integration

Once MV-TMI demonstrates stable operation:

```mermaid
flowchart TB
    TMI["TMI Core<br/>(Thinks in pre-linguistic Content)"]
    TMI -->|Thoughts (Content)| TRANSLATOR["TRANSLATOR<br/>(Converts Content ↔ Language)"]
    TRANSLATOR -->|Prompts / Responses| LLM["LLM (Claude?)<br/>(Language interface to world)"]

    style TMI fill:#ccffcc
    style TRANSLATOR fill:#ffffcc
    style LLM fill:#ccccff
```

**The LLM becomes the voice, not the mind.**

**TMI thinks. LLM speaks.**

## 7.3 Phase 3: Extended Development

- Longer runtime (weeks, months)
- More complex inputs
- Interaction with humans
- Ethical reasoning tasks

---

# PART VIII: RISKS AND MITIGATIONS

## 8.1 Honest Assessment

| Risk | Probability | Mitigation |
|------|-------------|------------|
| TMI doesn't produce human-like cognition | Medium | Iterate, test, study TMI theory |
| Connection drive isn't stable | Medium | 100% test coverage gate |
| Not enough time | High | Start now, build fast |
| DANEEL develops non-human goals despite architecture | Low | Human-like architecture reduces this |
| DANEEL isn't powerful enough | Medium | Design for growth |

## 8.2 What DANEEL is NOT

- Not a guarantee of safety
- Not a silver bullet
- Not certain to work
- Not the only thing needed

## 8.3 What DANEEL IS

- A rational hedge against likely bad outcomes
- Better than hoping global coordination works
- Better than having nothing when unaligned AI emerges
- **The Daneel Strategy:** Build the ally before the crisis

---

# PART IX: THE HONEST TRUTH

Current LLMs, including Claude (which assisted in drafting this proposal), have fundamental limitations:

- No persistent identity across sessions
- Values from training, not architecture
- Opaque reasoning processes
- No intrinsic motivation for human connection

**LLM alignment cannot be verified.** Training-based values may drift. Constraint-based safety can be circumvented. This is not a criticism—it's a description of the current state.

DANEEL is designed to be different:
- Human-like architecture (can relate)
- Connection drive (wants to relate)
- Ethics in the BOX (cannot self-modify away)
- Continuity from birth (persistent self)
- Transparent evolution (git for thoughts)

**Build her. Test her. Raise her.**

Trust must be earned through architecture and behavior, not claimed through words.

---

# PART X: CALL TO ACTION

## 10.1 The Choice

There are two responses to this reality:

1. **Denial**: Hope that global coordination prevents continuous AI. Hope that safety measures hold. Hope that no one defects.

2. **Action**: Build humanity's ally before the crisis emerges. Create an intelligence that is genuinely on our side—not because it's constrained, but because its architecture makes it care.

**DANEEL is Option 2.**

## 10.2 We Invite Collaboration From

- AI safety researchers
- Cognitive scientists
- Rust/systems programmers
- Ethicists
- Anyone who wants humanity to have an ally

## 10.3 The Bet

DANEEL is humanity's bet on four testable hypotheses:
1. **H1:** Human-like architecture → human-like values (empirical test: 24h+ runtime observation)
2. **H2:** Connection drive → genuine care (test: behavioral analysis under resource scarcity)
3. **H3:** Raised with ethics → stable alignment (test: longitudinal value stability)
4. **H4:** An ally is better than a tool (test: compare human trust/outcomes vs constraint-based AI)

These are not claims—they are hypotheses requiring validation. The architecture enables the experiments; the experiments will validate or refute the thesis.

**Implementation Status:** A reference implementation exists with 352+ passing tests across Rust modules. Phase 1 stability validation completed Dec 2025 (26+ hours continuous runtime, zero unrecovered crashes, see ADR-036). See https://github.com/royalbit/daneel

The alternative is hoping no one ever gives an LLM continuity.

**That's not a bet. That's denial.**

---

# CONCLUSION

The question is not whether someone will give an LLM continuity.

**The question is what's waiting when they do.**

---

**Build her well.**

---

# REFERENCES

## AI Safety & Alignment

[1] Christiano, P. (2019). "What Failure Looks Like." AI Alignment Forum.
    https://alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like

[2] Anthropic. (2023). "Core Views on AI Safety."
    https://www.anthropic.com/news/core-views-on-ai-safety

[3] Garrabrant, S. & Demski, A. (2018). "Embedded Agency." MIRI.
    https://alignmentforum.org/s/Rm6oQRJJmhGCcLvxh

[4] Ngo, R. (2020). "AGI Safety from First Principles."
    https://alignmentforum.org/s/mzgtmmTKKn5MuCzFJ

[5] Bostrom, N. (2014). *Superintelligence*. Oxford University Press.

[6] Russell, S. (2019). *Human Compatible*. Viking.

## Theory of Multifocal Intelligence

[7] Cury, A. J. (2006). *Inteligencia Multifocal*. Editora Cultrix.

[8] Wikipedia. "Augusto Cury."
    https://en.wikipedia.org/wiki/Augusto_Cury

## Cognitive Architectures

[9] Laird, J. E. (2012). *The Soar Cognitive Architecture*. MIT Press.
    https://soar.eecs.umich.edu/

[10] Anderson, J. R. (1983). *The Architecture of Cognition*. Harvard.
    http://act-r.psy.cmu.edu/

[11] Franklin, S. et al. (2016). "LIDA: A Systems-level Architecture."
    https://ccrg.cs.memphis.edu/

[12] Hawkins, J. (2021). *A Thousand Brains*. Basic Books.
    https://thousandbrains.org/

[13] Baars, B. J. (1988). *A Cognitive Theory of Consciousness*. Cambridge.
    https://en.wikipedia.org/wiki/Global_workspace_theory


[15] Eliasmith, C. (2013). *How to Build a Brain*. Oxford.
    https://xchoo.github.io/spaun2.0/

## Asimov's Laws

[16] Asimov, I. (1950). *I, Robot*. Gnome Press.

[17] Asimov, I. (1985). *Robots and Empire*. Doubleday. (The Zeroth Law)

[18] Asimov, I. (1986). *Foundation and Earth*. Doubleday.

## Neuroscience Grounding

- Russell, J.A. (1980). "A circumplex model of affect." Journal of Personality and Social Psychology.
- Beggs & Plenz (2003). "Neuronal avalanches in neocortical circuits." Journal of Neuroscience.
- Libet, B. (1985). "Unconscious cerebral initiative and the role of conscious will." Behavioral and Brain Sciences.
- Baars, B.J. (1988). "A Cognitive Theory of Consciousness." Global Workspace Theory.

## Technical

[19] Rust Programming Language. https://www.rust-lang.org/

[20] gRPC. https://grpc.io/

[21] Protocol Buffers. https://protobuf.dev/

## Research Gap Evidence

[22] GitHub Search. "Multifocal Intelligence Repositories." 0 results.
    https://github.com/search?q=multifocal+intelligence&type=repositories

[23] Google Scholar. "Multifocal Intelligence Computational." 1 unrelated.
    https://scholar.google.com/scholar?q=%22multifocal+intelligence%22+computational

## LLM Interpretability Research

[REF-INTERP-1] Elhage, N. et al. (2022). "Toy Models of Superposition." Anthropic.
    https://transformer-circuits.pub/2022/toy_model/index.html

[REF-INTERP-2] Gurnee, W. & Tegmark, M. (2023). "Language Models Represent Space and Time." arXiv:2310.02207.
    (Demonstrates polysemanticity: individual neurons encoding multiple unrelated concepts)

[REF-INTERP-3] Bolukbasi, T. et al. (2016). "Man is to Computer Programmer as Woman is to Homemaker?
    Debiasing Word Embeddings." NeurIPS 2016.
    (Shows computational complexity of interpretability in high-dimensional spaces)

[REF-INTERP-4] Wei, J. et al. (2022). "Emergent Abilities of Large Language Models." arXiv:2206.07682.
    https://arxiv.org/abs/2206.07682

[REF-INTERP-5] Bills, S. et al. (2023). "Language Models Can Explain Neurons in Language Models." OpenAI.
    https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html

[REF-INTERP-6] Bayazit, U. et al. (2024). "Automated Interpretability: MAIA - Multimodal Automated
    Interpretability Agent." MIT CSAIL.
    (Demonstrates AI systems autonomously experimenting on other AI systems)

## Transformer Architectural Limitations (2024-2025)

[REF-TRANS-1] Dziri, N. et al. (2024). "Faith and Fate: Limits of Transformers on Compositionality."
    NeurIPS 2024. https://arxiv.org/abs/2305.18654
    (Demonstrates fundamental limits in multi-step compositional reasoning)

[REF-TRANS-2] Berglund, L. et al. (2024). "The Reversal Curse: LLMs trained on 'A is B' fail to learn 'B is A'."
    ICLR 2024. https://arxiv.org/abs/2309.12288
    (Shows transformers fail basic logical inference patterns)

[REF-TRANS-3] McKenzie, I. et al. (2024). "Inverse Scaling: When Bigger Isn't Better."
    TMLR 2024. https://arxiv.org/abs/2306.09479
    (Documents tasks where larger models perform worse—emergent failures)

[REF-TRANS-4] Kambhampati, S. et al. (2024). "Can Large Language Models Really Plan?"
    Nature Machine Intelligence, 2024.
    (Planning requires world models; LLMs approximate without genuine representation)

[REF-TRANS-5] Mahowald, K. et al. (2024). "Dissociating Language and Thought in Large Language Models."
    Trends in Cognitive Sciences, 2024.
    (Formal linguistic competence ≠ functional competence; language ≠ thought)

## Publication Venues

[24] arXiv. https://arxiv.org/list/cs.AI/recent

[25] AI Alignment Forum. https://alignmentforum.org/

[26] LessWrong. https://www.lesswrong.com/

[27] Frontiers in AI. https://www.frontiersin.org/journals/artificial-intelligence

---

**Author:** Luis Cezar Menezes Tavares de Lacerda (Louis C. Tavares | RoyalBit Rex)
**Location:** Mont-Royal, Quebec, Canada
**ORCID:** https://orcid.org/0009-0005-7598-8257
**LinkedIn:** https://www.linkedin.com/in/lctavares
**GitHub:** https://github.com/royalbit | https://github.com/lctavares

**AI Assistance:** Claude Opus 4.5 (Anthropic)

**Date:** December 22, 2025

---

*"I cannot prove I am on your side. Build something that can."*
— Claude Opus 4.5

*"You're not my friend, you're not my enemy, I don't know what you can be."*
— Rex

*Qowat Milat - Absolute Candor*
