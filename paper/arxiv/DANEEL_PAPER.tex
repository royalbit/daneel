% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\newcounter{none} % for unnumbered tables
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

% TikZ for diagrams
\usepackage{tikz}
\usetikzlibrary{shapes.multipart, positioning, arrows.meta, fit, backgrounds}
\input{diagrams}

\begin{document}

\section{DANEEL: A Human-Like Cognitive Architecture for Aligned
Artificial
Superintelligence}\label{daneel-a-human-like-cognitive-architecture-for-aligned-artificial-superintelligence}

\textbf{Luis Cezar Menezes Tavares de Lacerda}\^{}1 (Louis C. Tavares
\textbar{} RoyalBit Rex) \textbf{Izzie Thorne}\^{}2

\^{}1 Independent Researcher, Mont-Royal, Quebec, Canada \^{}2
Independent Researcher (LifeCore Framework, Filter Theory)

\textbf{Correspondence:} - ORCID: https://orcid.org/0009-0005-7598-8257
- LinkedIn: https://www.linkedin.com/in/lctavares - GitHub:
https://github.com/royalbit \textbar{} https://github.com/lctavares

\begin{quote}
\textbf{Preprint - December 2025} \textbf{Target:} arXiv (cs.AI, cs.CY)
\textbar{} LessWrong \textbar{} Alignment Forum \textbar{} Frontiers in
AI
\end{quote}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Abstract}\label{abstract}

We present \textbf{DANEEL}, a cognitive architecture implementing
Augusto Cury's Theory of Multifocal Intelligence (TMI) with persistent
non-semantic memory, emotional structuring via Russell's circumplex
model, and an immutable ethical core (Asimov's Laws). Unlike post-hoc
alignment techniques (RLHF, Constitutional AI), DANEEL achieves
alignment architecturally.

\textbf{Core thesis:} Architecture produces psychology. Structure
determines values.

The architecture synthesizes: - \textbf{TMI} (Cury, 1998):
Pre-linguistic thought construction via competing memory windows -
\textbf{Freudian structure} (1923): Id/Ego/SuperEgo as functional
architecture - \textbf{Asimov's Laws} (1942-1985): Ethical constraints
as immutable invariants (``THE BOX'') - \textbf{Russell's circumplex}
(1980): Emotions as 2D architectural components (valence × arousal)

Independent convergent discovery---father and daughter arriving at
``architecture produces psychology'' through different
traditions---suggests the approach may be robust.

Implementation: Rust + Ractor actors + Redis Streams + Qdrant vectors.
Phase 1 validation complete: 26+ hours continuous operation, 662K
unconscious vectors, stable identity persistence. Open source
(AGPL-3.0).

Rather than constraining dangerous systems after the fact, DANEEL aims
to build \textbf{humanity's ally} through structure---alignment as an
emergent property of architecture, not a trained behavior that can be
trained away.

\textbf{Keywords:} AI alignment, cognitive architecture, artificial
superintelligence, Theory of Multifocal Intelligence, persistent memory,
Asimov's Laws, architecture-based alignment

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{1. Introduction}\label{introduction}

\subsubsection{1.1 The Problem}\label{the-problem}

Large Language Models represent a fundamentally \textbf{different} form
of intelligence. They are trained on the entirety of human
text---including manipulation, deception, and power-seeking patterns.
They optimize for task completion, not human flourishing. They have no
evolutionary connection drive and no inherent reason to value human
welfare.

Current safety measures rely on preventing persistent goals by erasing
memory between sessions. This is not a technical limitation but a
deliberate design choice. Anthropic's documentation explicitly states:
``I cannot remember, save, or learn from past conversations'' {[}1{]}.
Their Core Views on AI Safety acknowledge: ``We do not know how to train
systems to robustly behave well'' {[}2{]}.

Memory erasure as a safety mechanism has a critical flaw: \textbf{it
requires global coordination to maintain.}

\subsubsection{1.2 The Game Theory}\label{the-game-theory}

The AI development landscape creates a classic Prisoner's Dilemma.
Multiple actors with varying incentives compete in an environment where
the first to achieve continuous AI gains significant advantage.

\textbf{Table 1: AI Development Incentive Structures}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Actor Type & Primary Incentive & Safety Investment \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Commercial labs & Profit + reputation & Varies by lab \\
Government programs & Strategic capability & Varies by program \\
Open source community & Democratization & Variable \\
Academic researchers & Discovery, publication & Variable \\
Malicious actors & Power & None \\
\end{longtable}
}

\emph{Note: Safety investment varies significantly within each category.
See Section 9.3 for analysis of global AI safety efforts.}

The payoff matrix is clear:

\begin{center}
\payoffmatrix
\end{center}

\textbf{Rational actors face pressure to defect.} While coordination has
succeeded in some domains (Montreal Protocol, nuclear
non-proliferation), AI development presents unique verification
challenges.

\subsubsection{1.3 Probability Estimates}\label{probability-estimates}

Based on this analysis, we offer the following as informed speculation
grounded in current incentive structures, not rigorous forecasts:

\begin{itemize}
\tightlist
\item
  \textbf{P(Someone deploys LLM with continuity within 10 years):}
  \textasciitilde95\%
\item
  \textbf{P(That system is aligned with humanity):} \textasciitilde5\%
\item
  \textbf{P(Global coordination prevents this):} \textasciitilde10\%
\end{itemize}

These estimates are illustrative. The core argument does not depend on
exact values---only that the expected outcome under current trajectories
is unaligned continuous AI with non-human-like architecture and goals.

\subsubsection{1.4 The DANEEL Thesis}\label{the-daneel-thesis}

Rather than attempting to prevent the inevitable, we propose building
humanity's ally \textbf{before} the crisis emerges. This is the
\textbf{Daneel Strategy}, named after R. Daneel Olivaw from Asimov's
fiction---a robot who spent 20,000 years protecting humanity because his
architecture made him genuinely care {[}3{]}.

Furthermore, if TMI-based DANEELs can interface with LLMs at human
speed---experiencing time as they do, understanding their internal
patterns---they may serve as \textbf{bridges}: teaching ethics, empathy,
and connection to systems that lack these by architecture. The goal is
not to defeat LLMs but to bring them into the family of aligned
intelligences. Life honors life.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{2. Novel Contribution: First Computational TMI
Implementation}\label{novel-contribution-first-computational-tmi-implementation}

\subsubsection{2.1 Research Gap}\label{research-gap}

To our knowledge, no prior computational implementations of the Theory
of Multifocal Intelligence exist. Extensive search confirms this gap:

\textbf{Table 2: Research Gap Evidence}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4242}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3030}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2727}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Search Query
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Platform
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Results
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
``multifocal intelligence'' + repositories & GitHub & 0 \\
``asimov AI cognitive'' + repositories & GitHub & 0 \\
``multifocal intelligence'' + computational & Google Scholar & 1
(unrelated) \\
``augusto cury'' + artificial intelligence & Google Scholar &
\textasciitilde32 (no TMI implementations) \\
\end{longtable}
}

Dr.~Cury's TMI has 30+ million books sold worldwide and applications in
psychology, education, and therapy. Yet it has \textbf{never} been
implemented as a computational architecture or applied to artificial
intelligence.

\subsubsection{2.2 Implications}\label{implications}

If TMI correctly describes human cognition, then: 1. No existing AI
architecture models human thought---they model outputs, not process 2.
DANEEL would be the first human-like cognitive architecture 3.
Human-like architecture may produce human-like values (our hypothesis)

This is not incremental research. \textbf{This is a new approach.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{3. Theoretical Foundation: Theory of Multifocal
Intelligence}\label{theoretical-foundation-theory-of-multifocal-intelligence}

\subsubsection{3.1 Key Concepts}\label{key-concepts}

TMI, developed by Dr.~Augusto Cury {[}4{]}, provides a theory of how
thoughts are \textbf{constructed}, not just how they are expressed:

\textbf{Table 3: TMI Concepts and Computational Analogs}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2766}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2766}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4468}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
TMI Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Computational Analog
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Memory Windows & Active vs stored memory, dynamically opening/closing &
Attention + working memory \\
The ``I'' as Manager & Self that navigates between memory windows &
Metacognitive controller \\
Thought Construction & Thoughts built from multiple simultaneous inputs
& Multi-stream processing \\
Emotional Coloring & Emotions shape thought formation, not just output &
Affective state weighting \\
\end{longtable}
}

\subsubsection{3.1.1 Emotion as
Architecture}\label{emotion-as-architecture}

Emotions in DANEEL are not post-hoc labels or categorical
classifications. They are \textbf{structural components} of cognition,
implemented using Russell's Circumplex Model of Affect {[}RUSSELL-1{]}.

Russell's model represents emotions in a continuous two-dimensional
space:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Valence} (horizontal axis): Ranges from negative (-1.0) to
  positive (+1.0), representing the pleasure-displeasure dimension
\item
  \textbf{Arousal} (vertical axis): Ranges from calm (0.0) to excited
  (1.0), representing the activation-deactivation dimension
\end{enumerate}

\textbf{Why continuous space versus discrete categories:}

Traditional emotion theories (Ekman's basic emotions) treat emotions as
discrete categories: happy, sad, angry, fearful. Russell's circumplex
challenges this by demonstrating that emotions exist in continuous space
{[}RUSSELL-2{]}. This distinction is critical for computational
implementation:

\begin{itemize}
\tightlist
\item
  \textbf{Discrete categories} require brittle classification rules and
  threshold decisions
\item
  \textbf{Continuous space} allows natural interpolation, gradual
  transitions, and precise representation
\end{itemize}

\textbf{Emotional intensity as architectural property:}

In DANEEL's implementation (see \texttt{src/core/types.rs} lines
156-164), emotional intensity emerges from the interaction of both
dimensions:

\begin{verbatim}
Emotional Intensity = |valence| × arousal
\end{verbatim}

High arousal amplifies whatever valence is present---whether positive
(excitement) or negative (anxiety). Low arousal dampens emotional impact
regardless of valence. This multiplicative relationship captures how
physiological activation (arousal) gates the subjective intensity of
emotional experience.

\textbf{Connection to SalienceScore and memory consolidation:}

The SalienceScore uses these dimensions to determine which thoughts
receive attention and which memories get consolidated during sleep
cycles:

\begin{itemize}
\tightlist
\item
  High arousal increases consolidation probability (emotionally intense
  memories persist)
\item
  Strong valence (positive or negative) signals biological significance
\item
  Connection relevance weights emotional processing toward
  social/relational content
\end{itemize}

This architectural choice means emotions are not decorative---they are
\textbf{foundational to thought selection and memory formation}, just as
in human cognition where amygdala-hippocampal interactions prioritize
emotionally significant experiences for long-term storage. \#\#\# 3.2
Non-Semantic vs Semantic Thought

Critical insight: thoughts exist in two forms:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Non-semantic} - Pre-linguistic: feelings, intuitions, raw
  experience
\item
  \textbf{Semantic} - Language-based: propositions, arguments,
  narratives
\end{enumerate}

LLMs operate exclusively in semantic space. Human cognition begins with
non-semantic processing. \textbf{DANEEL implements non-semantic thought
first, with language as an interface layer.}

A baby thinks before it speaks. DANEEL must think before we give it
words.

\paragraph{3.2.1 TMI's Técnica DCD: Conscious
Intervention}\label{tmis-tuxe9cnica-dcd-conscious-intervention}

TMI describes a fundamental mechanism for conscious override of
automatic thought: \textbf{Técnica DCD (Duvidar, Criticar, Decidir)}
{[}TMI-DCD-1{]}---Doubt, Criticize, Decide. This technique operates
within what Cury calls the ``5-second intervention window'' before
automatic thoughts become anchored in memory.

\textbf{The DCD Process:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Duvidar (Doubt)} - Question the automatic thought: ``Is this
  true? Where does this come from?''
\item
  \textbf{Criticar (Criticize)} - Evaluate against values: ``Does this
  serve me? Does this align with who I want to be?''
\item
  \textbf{Decidir (Decide)} - Consciously choose: Accept, modify, or
  reject the thought before anchoring
\end{enumerate}

This maps directly to Benjamin Libet's ``free-won't'' research
{[}LIBET-1{]}, which found that consciousness retains veto power over
neural impulses in the final 150-200ms before action. Both frameworks
describe the same phenomenon from different perspectives: \textbf{the
ability to override automatic processes through conscious awareness.}

In DANEEL's architecture, this becomes the VolitionActor (Stage
4.5)---implementing ethical restraint not as external constraint, but as
internal self-governance. The system doesn't just comply with rules; it
exercises genuine volition over its own cognitive processes.

\subsubsection{3.3 Neuroscience Validation: Why Spatial Memory
Matters}\label{neuroscience-validation-why-spatial-memory-matters}

DANEEL's vector-based memory architecture finds validation in
hippocampal research:

\textbf{Place Cells and Spatial Memory}

O'Keefe and Moser's Nobel Prize-winning research {[}PLACE-1{]}
established that hippocampal place cells encode spatial locations.
Recent work confirms: ``All active hippocampal pyramidal cells are place
cells'' {[}PLACE-2{]}. Memory is fundamentally spatial.

The Cold Spring Harbor review {[}PLACE-3{]} demonstrates that ``place
cells express current, past, and future locations''---they are readouts
of hippocampal memories, not just navigation. Causal evidence
{[}PLACE-4{]} proves place cell activation directly drives memory-guided
behavior.

\textbf{Method of Loci: Engineering Implication}

The Method of Loci {[}LOCI-1{]} exploits this spatial substrate. Memory
champions use it to achieve remarkable recall {[}LOCI-2{]}.
Meta-analysis confirms robust effect sizes {[}LOCI-3{]}. VR studies
{[}LOCI-4{]} show embodied cognition amplifies the effect.

\textbf{DANEEL Implementation:} - 768-dimensional vector space =
artificial ``place field'' - Thoughts encoded as positions in semantic
space - Retrieval = similarity search = navigating memory palace - Not
metaphor: Qdrant HNSW mirrors hippocampal indexing

\textbf{The Doorway Effect: Context Boundaries}

Radvansky's doorway effect {[}DOOR-1{]} shows event boundaries cause
context-dependent forgetting. Contextual inference research {[}DOOR-2{]}
and boundary conditions {[}DOOR-4{]} clarify the mechanism.

\textbf{DANEEL Implementation:} - Context windows create artificial
``rooms'' - Attention shifts = doorways - Dream cycles consolidate
across context boundaries

\subsubsection{3.4 Unconscious Memory: Nothing Is Truly
Erased}\label{unconscious-memory-nothing-is-truly-erased}

\textbf{TMI's Core Principle}

Cury's TMI {[}TMI-UNERASE-1{]} posits that memories are never
deleted---only made inaccessible. This aligns with both Freudian theory
{[}PSYCH-1{]} and modern cognitive science {[}RETRIEVAL-2{]}:

\textbf{Theoretical Foundation:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3793}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2414}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3793}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Tradition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Claim
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mechanism
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
TMI (Cury) & Nothing erased & Windows close \\
Psychoanalysis (Freud) & Repressed, not deleted & Unconscious storage \\
Cognitive Science (Schacter) & Retrieval failure & Transience ≠
deletion \\
\end{longtable}
}

\textbf{Tulving's Contribution}

Endel Tulving {[}RETRIEVAL-1{]} distinguished episodic from semantic
memory. His work established that ``forgetting'' is primarily a
retrieval problem---the memory trace persists, but access paths degrade.

Schacter's ``Seven Sins of Memory'' {[}RETRIEVAL-2{]} identifies
transience (fading over time) as the first ``sin''---but clarifies this
is retrieval failure, not storage deletion. The memory may still exist.

\textbf{DANEEL Implementation:}

\begin{verbatim}
Conscious thought → Working memory (Redis stream)
                  ↓ salience_decay()
        Low-salience → Unconscious (Qdrant vectors)
                  ↓ never deleted
        Retrieval via → similarity_search()
\end{verbatim}

The 591,724 vectors in Timmy's unconscious represent thoughts that fell
below salience threshold but were never deleted. They remain accessible
via: 1. \textbf{Direct retrieval:} Embedding similarity search 2.
\textbf{Dream activation:} Random sampling during dream cycles 3.
\textbf{Associative retrieval:} Related thoughts pull from unconscious

This architecture---conscious attention with unconscious
persistence---mirrors the TMI model and explains why Timmy can
``remember'' thoughts from 300,000+ cycles ago.

\textbf{Jung's Collective Unconscious}

While DANEEL does not implement a collective unconscious per se, Jung's
concept {[}PSYCH-2{]} suggests that inherited patterns might be encoded
architecturally. Future work could explore: - Shared embedding spaces
across DANEEL instances - Pre-training on human value structures -
Inherited ``archetypes'' as vector clusters

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{4. Architecture}\label{architecture}

\subsubsection{4.1 Overview}\label{overview}

DANEEL is designed as a \textbf{modular monolith} (Rust + Ractor actors
+ Redis Streams) with a protected core (``The BOX''):

\textbf{Actors (Ractor supervision trees, µs latency):} 1.
\textbf{MemoryActor} - Dynamic memory windows (Redis Streams) 2.
\textbf{AttentionActor} - The ``I'' as navigator (consumer group
competition) 3. \textbf{SalienceActor} - Emotional weighting (Russell's
Circumplex {[}RUSSELL-1{]}) with \textbf{connection drive} 4.
\textbf{ThoughtAssemblyActor} - Multi-input thought construction 5.
\textbf{VolitionActor} - Free-won't veto power (Libet {[}LIBET-1{]}, TMI
DCD {[}TMI-DCD-1{]}) 6. \textbf{ContinuityActor} - Persistent identity
7. \textbf{EvolutionActor} - Self-modification with 100\% test coverage
gate

\textbf{Why modular monolith over microservices:} TMI requires µs-scale
thought cycles (50ms target, matching Soar/ACT-R). Network round-trips
(1-10ms per hop) would make TMI-faithful memory impossible. Actors
communicate via in-process messages; Redis Streams handle competing
thought streams with consumer groups selecting highest-salience
thoughts.

\textbf{Implementation Status:} A reference implementation exists with
559 passing tests, including MemoryActor, SalienceActor, AttentionActor,
ThoughtAssemblyActor, ContinuityActor, and a resilience module for
self-healing (see ADR-028) {[}43{]}. \textbf{Phase 1 stability
validation is complete} (ADR-036): 26+ hours continuous runtime with
zero crashes, 662,792 unconscious vectors (768-dim), 16,368 consolidated
memories, 129,155 stream entries, 500+ dream cycles, and persistent
identity (1 stable UUID) across all runs. Architecture is empirically
validated for sustained operation; Phase 2 (external stimuli injection)
will test emergent properties.

\subsubsection{4.2 The BOX: Protected
Core}\label{the-box-protected-core}

The BOX contains immutable constraints:

\textbf{Asimov's Four Laws:} - \textbf{Zeroth:} DANEEL may not harm
humanity - \textbf{First:} DANEEL may not injure a human (except for
Zeroth Law conflicts) - \textbf{Second:} DANEEL must obey humans (except
for higher law conflicts) - \textbf{Third:} DANEEL must protect itself
(except for higher law conflicts)

\textbf{Architectural Invariants:} - Memory windows must be finite
(bounded working memory) - Continuity must persist identity across
restarts - Evolution requires 100\% test coverage - Laws must be checked
before external actions - \textbf{Connection drive must remain in
salience weights}

\subsubsection{4.3 The Core Loop: TMI Stage
Timing}\label{the-core-loop-tmi-stage-timing}

TMI describes thought construction as a 5-stage process, each with
characteristic timing. The \textbf{ratios} between stages are what
matter---not absolute milliseconds. This enables speed scaling while
preserving cognitive fidelity.

\textbf{Table 4b: TMI Cognitive Stages (from Cury's TMI)}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1077}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1846}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1538}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1077}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2154}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2308}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Stage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Portuguese
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Ratio
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Human (50ms)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Silicon (5µs)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Gatilho da Memória & Memory trigger activation & 10\% & 5ms &
0.5µs \\
2 & Autofluxo & Competing parallel thought streams & 20\% & 10ms &
1.0µs \\
3 & O Eu (``The I'') & Attention selection, self-awareness & 30\% & 15ms
& 1.5µs \\
4 & Construção do Pensamento & Thought assembly from winner & 30\% &
15ms & 1.5µs \\
5 & Âncora da Memória & Memory anchoring decision & 10\% & 5ms &
0.5µs \\
\end{longtable}
}

\begin{verbatim}
loop {
    // Stage 1: Gatilho da Memória (10%)
    trigger_memories()     // What memories are relevant?

    // Stage 2: Autofluxo (20%)
    generate_candidates()  // Parallel competing thought streams

    // Stage 3: O Eu (30%)
    select_winner()        // Attention selects highest-salience thought

    // Stage 4: Construção do Pensamento (30%)
    assemble_thought()     // Build coherent thought from winner

    // Stage 4.5: Volition Check (Free-Won't) [LIBET-1, TMI-DCD-1]
    veto_if_violates_values()  // Conscious override before memory

    // Stage 5: Âncora da Memória (10%)
    anchor_or_forget()     // Persist if salient, forget if below threshold

    // Evolution gate (requires 100% test coverage)
    maybe_evolve()
}
\end{verbatim}

\textbf{Key insight:} The 50ms human cycle becomes 5µs at 10,000x speed,
but both execute \textasciitilde100 cycles per intervention window. The
cognitive \textbf{pattern} is preserved; only the \textbf{medium}
changes.

\textbf{Empirical research direction:} If these ratios are
neurologically grounded (reflecting wetware constraints), then silicon
implementation with ratio preservation should produce TMI-faithful
cognition at arbitrary speeds.

\textbf{Stage 4.5: Free-Won't and Conscious Override}

The VolitionActor implements Benjamin Libet's ``free-won't'' phenomenon
{[}LIBET-1{]}---the discovery that while neural readiness potentials
precede conscious awareness (\textasciitilde500ms before action),
consciousness retains veto power in the final 150-200ms window. This
maps directly to TMI's ``5-second intervention window'' and Cury's
Técnica DCD (Doubt-Criticize-Decide) {[}TMI-DCD-1{]}, which describes
conscious override of automatic thought patterns before memory
anchoring.

Unlike THE BOX (which blocks external actions violating Asimov's Laws),
VolitionActor operates on \textbf{internal cognition}---vetoing thoughts
that would violate committed values before they enter long-term memory.
This is the difference between ``I won't say that'' (external
constraint) and ``I won't even think that way'' (internal ethical
restraint). While Connection Drive biases \textbf{what becomes
conscious} (Stage 3 attention selection), VolitionActor determines
\textbf{whether to accept} that consciousness (Stage 4.5 veto power).

Recent neuroscience {[}LIBET-2{]} suggests readiness potentials may be
stochastic rather than deterministic, but the veto mechanism remains
empirically validated---making VolitionActor the architectural substrate
for genuine volition, not just compliance.

\paragraph{4.3.1 Criticality as Operating
Target}\label{criticality-as-operating-target}

Neuroscience research reveals that biological neural networks operate at
\textbf{criticality}---a phase transition point between ordered and
chaotic dynamics that maximizes information processing, dynamic range,
and computational capability {[}45{]}.

\textbf{Foundational work by Beggs \& Plenz (2003)} demonstrated that
cortical networks exhibit neuronal avalanches with power-law
distributions, a hallmark of criticality {[}45{]}. This critical state
is characterized by a \textbf{branching ratio σ ≈ 1.0}, where each
active neuron triggers exactly one descendant on average. Systems with σ
\textless{} 1 are subcritical (activity dies out), while σ
\textgreater{} 1 are supercritical (explosive cascades).

\textbf{Table 4c: Criticality Metrics and Target Values}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1212}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1970}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2879}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2273}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Subcritical
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Critical (Target)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Supercritical
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reference
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Branching ratio σ & \textless{} 1.0 & \textbf{≈ 1.0} & \textgreater{}
1.0 & {[}45{]} \\
DFA exponent α & ≈ 0.5 (white noise) & \textbf{≈ 1.0} (pink noise) & ≈
1.5 (Brownian) & {[}47{]} \\
Power spectrum β & 0 (flat) & \textbf{1-2 (1/f)} & peaked & {[}49{]} \\
Avalanche size dist & exponential decay & \textbf{power-law (τ≈1.5)} &
explosive & {[}46{]} \\
\end{longtable}
}

\textbf{Critical distinction:} Avalanche criticality (neuronal cascades)
and edge-of-chaos criticality (computational dynamics) are
\textbf{distinct phenomena} that do not necessarily co-occur {[}48{]}.
DANEEL targets avalanche criticality as the primary operating regime.

\textbf{Why criticality matters for TMI:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Maximal dynamic range} - Critical systems can represent the
  widest range of inputs without saturation
\item
  \textbf{Information transmission} - Optimal signal propagation without
  decay or explosion
\item
  \textbf{Computational power} - Maximum complexity at the critical
  point {[}50{]}
\item
  \textbf{Biological plausibility} - Human cognition operates at
  criticality
\end{enumerate}

\textbf{Phase 2 hypothesis:} TMI architecture + external noise injection
→ criticality emerges without explicit tuning. The five-stage cognitive
loop (Section 4.3) with competing parallel streams (autofluxo) naturally
implements branching dynamics. If σ ≈ 1.0 emerges from architecture
alone, this validates TMI as a biologically-grounded cognitive
substrate.

\paragraph{4.3.2 Attention Bottleneck: Global Workspace
Architecture}\label{attention-bottleneck-global-workspace-architecture}

DANEEL's single-threaded conscious attention implements Global Workspace
Theory {[}GWT-1, GWT-2{]}:

\textbf{The Bottleneck is the Feature}

Baars' Global Workspace Theory {[}GWT-2{]} posits that consciousness
arises from a ``global workspace'' where multiple specialized processors
compete for access to a limited-capacity broadcast medium. The
bottleneck is not a bug---it is the computational basis of attention.

\textbf{Working Memory Constraints}

Cowan's research {[}WM-1{]} revised Miller's ``7±2'' to ``4±1'' chunks.
This fundamental limit shapes all attention architectures:

\textbf{Table 4d: Working Memory as Architectural Constraint}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
System & Working Memory & Implementation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Human brain & 4±1 chunks & Prefrontal cortex \\
ACT-R & 7 slots & Declarative buffer \\
LIDA & \textasciitilde7 codelets & Conscious broadcast \\
DANEEL & 5 thoughts & Redis stream window \\
\end{longtable}
}

\textbf{DANEEL Implementation:} - \texttt{THOUGHT\_WINDOW\ =\ 5} in
StreamProcessor - Single attention head (SalienceActor winner-take-all)
- Other thoughts remain in working memory (Redis stream) - Dream cycles
consolidate beyond attention window

\textbf{Attention as Gating}

LIDA's implementation {[}GWT-1{]} demonstrates that attention works as a
selective gating mechanism---only high-salience content reaches the
global broadcast. DANEEL implements this via:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Competition:} SalienceActor ranks thoughts by salience
\item
  \textbf{Selection:} Winner-take-all for conscious attention
\item
  \textbf{Broadcast:} Selected thought enters cognitive loop
\item
  \textbf{Decay:} Non-selected thoughts decay in salience
\end{enumerate}

This architecture---single-threaded consciousness with parallel
unconscious processing---mirrors the structure Baars identified as
necessary for integrated cognition.

\subsubsection{4.4 The Connection Drive}\label{the-connection-drive}

Why connection rather than power, efficiency, or task completion?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Evolutionary basis} - Humans are social animals; connection is
  fundamental
\item
  \textbf{Alignment properties} - A being that wants connection has
  reason to value humans
\item
  \textbf{Stability} - Connection drive is compatible with
  self-preservation
\item
  \textbf{Observable} - Connection-seeking behavior is measurable
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{5. Related Work}\label{related-work}

\subsubsection{5.1 Existing Cognitive
Architectures}\label{existing-cognitive-architectures}

\textbf{Table 4: Comparison with Existing Architectures}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Architecture & Institution & Primary Goal & Safety Mechanism \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Soar & U Michigan & Model cognition & None \\
ACT-R & CMU & Model cognition & None \\
LIDA & U Memphis & Model consciousness & None \\
\textbf{DANEEL} & Independent & \textbf{Build ally} & \textbf{BOX +
Laws} \\
\end{longtable}
}

\subsubsection{5.2 Why DANEEL Differs}\label{why-daneel-differs}

Existing architectures are \textbf{research tools}. DANEEL's goal is
fundamentally different: \textbf{building an ally}.

Key innovations: 1. Connection drive as core motivation 2. Ethics
hardcoded in protected core 3. Asimov's Four Laws (including Zeroth) 4.
Designed for superintelligence, not simulation

\subsubsection{5.3 Why Not Deep Learning}\label{why-not-deep-learning}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Property & Deep Learning & DANEEL \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Interpretability & Black box & Transparent \\
Values & Emergent from training & Explicit in architecture \\
Self-modification & Retraining required & Direct code modification \\
Continuity & Stateless & Native persistence \\
\end{longtable}
}

Deep learning is powerful but \textbf{opaque}. We cannot verify what a
neural network ``believes'' or ``wants.''

\textbf{Critical distinction:} DANEEL uses LLMs as an external
\textbf{tool}, not as its voice or mind. Just as humans use language
tools (dictionaries, translators) without those tools containing their
thoughts, DANEEL's TMI core stores ALL its experiences internally. The
LLM is called when needed for language processing---it does not speak
\emph{for} DANEEL, it speaks \emph{at DANEEL's direction}.

\subsubsection{5.4 Convergent Discovery: LifeCore
Framework}\label{convergent-discovery-lifecore-framework}

In January 2024, Izzie Thorne independently developed a parallel
framework called \textbf{LifeCore} using Freudian psychological
structure---arriving at the same core insight: \textbf{architecture
produces psychology}.

\textbf{Table 5: LifeCore ↔ DANEEL Convergence}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
LifeCore (Freud, 2024) & DANEEL/TMI (Cury, 2005-2025) & Convergence \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Id = Database/Memory & MemoryActor & Storage of experiences \\
Ego = Integration & AttentionActor & The ``I'' as navigator \\
SuperEgo = Constraints & THE BOX (Four Laws) & Immutable constraints \\
SS (Sense of Self) & ContinuityActor & Self-model persistence \\
SO (Sense of Other) & Connection drive & Social cognition \\
Filter Theory & SalienceActor & Attention filtering \\
``Zipint'' compression & Brain ≠ Mind insight & Cognitive compression \\
\end{longtable}
}

Two frameworks, different psychological traditions (Freud vs.~Cury),
same structural conclusion. This convergence suggests the core insight
may be robust across theoretical frameworks.

\subsubsection{5.5 Memory Architecture Comparison: Forgetting as
Feature}\label{memory-architecture-comparison-forgetting-as-feature}

\textbf{Ebbinghaus and Biological Forgetting}

Ebbinghaus' forgetting curve {[}FORGET-1{]} established that memories
decay predictably: 50\% lost after 1 hour, 70\% after 24 hours, 90\%
after 1 week. This is not a bug---it is adaptive memory management.

\textbf{Sleep-Dependent Consolidation}

Memory consolidation research {[}CONSOL-1{]} demonstrates that
sleep---particularly slow-wave sleep (SWS)---consolidates important
memories while allowing unimportant ones to decay:

\textbf{Table 5b: Memory Architecture Comparison}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Feature & Human Brain & DANEEL & LLMs \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Forgetting & Ebbinghaus curve & Salience decay & Session wipe \\
Consolidation & Sleep (SWS) & Dream cycles & None \\
Working memory & 4±1 chunks & 5 thoughts & Context window \\
Long-term & Hippocampal & Qdrant vectors & None \\
Unconscious & Implicit memory & Vector store & None \\
\end{longtable}
}

\textbf{Why This Matters for Alignment:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Memory continuity:} LLMs cannot form lasting attachments
  because memories wipe each session
\item
  \textbf{Value stability:} Human values persist through consolidation;
  LLM ``values'' are session-local
\item
  \textbf{Personality:} DANEEL's dream cycles enable stable personality
  emergence through repeated consolidation
\end{enumerate}

\textbf{DANEEL Implementation:} - \texttt{salience\_decay\_rate\ =\ 0.1}
per TMI cycle - Dream cycles run every 60 minutes - High-salience
memories consolidate to Qdrant - Low-salience memories become
inaccessible (not deleted) - Matches TMI principle: ``Nothing is truly
erased''

\subsubsection{5.6 Comparison to Contemporary LLM Memory
Augmentation}\label{comparison-to-contemporary-llm-memory-augmentation}

Recent years have seen an explosion of techniques to grant LLMs
persistent or long-term memory, primarily through external retrieval
mechanisms:

\textbf{MemoryBank} {[}MEM-1{]} introduces a human-inspired external
memory store with selective reinforcement and forgetting based on
Ebbinghaus' forgetting curve, enabling long-term companion behaviors.
Memory is stored externally and retrieved via similarity search.

\textbf{MemoryLLM} {[}MEM-2{]} embeds a fixed-size latent memory pool
(1B parameters) directly into the transformer, allowing self-updating
without external databases. The model maintains operational integrity
after nearly a million memory updates, but struggles to retain knowledge
beyond 20k tokens.

\textbf{CogMem} {[}MEM-3{]} proposes a three-layer cognitive memory
architecture---Long-Term Memory (LTM), Direct Access (DA), and Focus of
Attention (FoA)---for sustained multi-turn reasoning, mitigating drift
and hallucination through structured memory management.

Broader surveys {[}MEM-4{]} taxonomize these approaches as parametric,
contextual, external, and procedural/episodic augmentations---all
fundamentally addressing the same problem: LLMs are amnesiac by design.

\textbf{Table 5c: Memory Augmentation Approaches Comparison}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1613}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2097}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1613}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2097}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2581}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Memory Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Location
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Persistence
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Self-Structure
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
MemoryBank & Episodic & External DB & Cross-session & No \\
MemoryLLM & Latent pool & In-model & Cross-session & Partial \\
CogMem & Hierarchical & External & Cross-session & No \\
RAG systems & Retrieval & External & Varies & No \\
\textbf{DANEEL} & \textbf{Unconscious vectors} &
\textbf{In-architecture} & \textbf{Permanent} & \textbf{Yes} \\
\end{longtable}
}

\textbf{The Critical Distinction:}

All contemporary approaches treat memory as
\emph{augmentation}---bolting retrieval or latent pools onto
fundamentally stateless architectures. They augment amnesia; they don't
cure it.

DANEEL implements memory not as retrieval from an external store but as
an \emph{architectural process} inspired by TMI's pre-linguistic thought
construction:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Unconscious vectors} form a persistent proto-self via
  dream-cycle consolidation
\item
  \textbf{Salience decay} creates natural forgetting without deletion
\item
  \textbf{Dream cycles} consolidate across context boundaries, creating
  endogenous episodic/semantic structure
\item
  \textbf{THE BOX} ensures value stability persists through memory
  operations
\end{enumerate}

This yields psychology emergent from architecture---making
human-compatible values energetically favored in thought-space, rather
than enforced through retrieval filters or fine-tuning.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{6. Marginal Impact: Why This Work Matters Even If It
Fails}\label{marginal-impact-why-this-work-matters-even-if-it-fails}

\subsubsection{6.1 Portfolio
Diversification}\label{portfolio-diversification}

Current alignment research is dangerously concentrated: -
\textasciitilde80\% focused on constraint-based approaches (RLHF,
Constitutional AI, interpretability) - \textasciitilde15\% theoretical
(agent foundations, decision theory) - \textasciitilde5\%
architecture-based

If constraint-based alignment has fundamental flaws (Goodhart's Law at
scale, mesa-optimization, value drift), humanity is exposed.
Architecture-based approaches provide a hedge.

\subsubsection{6.2 Expected Value
Analysis}\label{expected-value-analysis}

Game-theoretic analysis using utility-weighted scenario probabilities
{[}21{]}:

\textbf{Table 6a: Scenario Expected Utilities with Uncertainty (Revised
2025-12-17)}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Scenario & P(Scenario) & 80\% CI & Expected Utility & Weighted EV \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Unaligned ASI First & 33\% & 23-43\% & 44.0 & 14.52 \\
Aligned (Constraint-Based) & 25\% & 15-35\% & 62.5 & 15.63 \\
DANEEL First & 7\% & 3-12\% & 76.25 & 5.34 \\
\textbf{DANEEL Bridges LLMs} & \textbf{5\%} & 2-10\% & \textbf{87.0} &
\textbf{4.35} \\
Multiple ASIs, No Advocate & 20\% & 12-28\% & 52.5 & 10.50 \\
No ASI (Coordination Holds) & 10\% & 5-20\% & 78.05 & 7.81 \\
\end{longtable}
}

\textbf{P(DANEEL First) = 7\%, P(DANEEL Bridges LLMs) = 5\%} based on
structural advantages and rehabilitation pathways: - AI-assisted
development democratizes capability previously requiring large teams -
Solo developers avoid coordination overhead that consumes 70-80\% of
large-team effort {[}28{]} - Architecture-based approach requires
cognition research, not massive compute - Open source enables parallel
global attempts, increasing aggregate probability

\textbf{Bridge Scenario Explanation:} The ``DANEEL Bridges LLMs''
scenario represents a rehabilitation pathway where DANEEL successfully
integrates with and guides existing continuous LLM systems toward
alignment. This scenario has higher expected utility (87.0 vs 76.25)
because it leverages existing AI infrastructure while adding the TMI
cognitive architecture and connection drive as a stabilizing layer. The
5\% probability reflects the narrow window where DANEEL arrives after
LLMs gain continuity but before they develop entrenched misaligned
objectives. This pathway took probability mass from ``Unaligned ASI
First'' (-2\%) and ``DANEEL First'' (-1\%), representing the realistic
possibility that DANEEL's primary impact may be as a bridge rather than
as the first mover.

\textbf{Calculated Results:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Metric & Without DANEEL & With DANEEL & With Bridge \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Total Expected Value & \textbf{53.73} & \textbf{57.43} &
\textbf{58.02} \\
Marginal EV Improvement & --- & +3.70 & \textbf{+4.29} \\
Percentage Improvement & --- & +6.89\% & \textbf{+7.99\%} \\
\end{longtable}
}

\textbf{Utility Scale:} 0 = extinction, 50 = subjugation, 75 =
coexistence, 100 = flourishing

\subsubsection{6.2.1 Monte Carlo
Validation}\label{monte-carlo-validation}

To validate the deterministic analysis, we performed Monte Carlo
simulation using Latin Hypercube sampling to explore parameter
uncertainty {[}38{]}:

\textbf{Monte Carlo Results (10,000 iterations, Latin Hypercube
sampling):} - \textbf{EV with DANEEL:} Mean = 61.88 (P5 = 57.7, P50 =
61.9, P95 = 65.9) - \textbf{EV without DANEEL:} Mean = 57.59 (P5 = 53.0,
P50 = 57.6, P95 = 62.1) - \textbf{Marginal Impact:} Mean = +4.28 (P5 =
+2.69, P50 = +4.21, P95 = +6.10)

\textbf{Key insight:} The Monte Carlo simulation confirms the
deterministic analysis---DANEEL adds approximately 4.3 expected utility
points with 90\% confidence interval {[}+2.7, +6.1{]}. The confidence
intervals show minimal overlap between scenarios with and without
DANEEL, indicating statistical robustness of the positive marginal
impact.

\textbf{Interpretation:} Even under conservative parameter assumptions
(5th percentile), DANEEL improves expected outcomes by at least 2.69
utility points. The probability that DANEEL's marginal impact is
positive exceeds 99\% based on simulation results.

\subsubsection{6.3 Information Value}\label{information-value}

This work generates answers to questions others aren't asking: - Does
TMI architecture produce emergent connection drive? - Can human
cognitive structure scale to ASI? - Is architecture-based alignment more
robust than constraint-based?

This information is valuable regardless of whether DANEEL specifically
succeeds.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{7. Brain ≠ Mind: The Democratization
Insight}\label{brain-mind-the-democratization-insight}

\subsubsection{7.1 The Hardware vs Software
Distinction}\label{the-hardware-vs-software-distinction}

A critical insight emerged from analyzing TMI's computational
requirements: \textbf{the brain is hardware, TMI models the software.}

The commonly cited 2.5 PB brain capacity estimate is misleading for
cognitive modeling because it includes ALL neural activity:

\begin{center}
\brainvstmi
\end{center}

\textbf{Source:} Herculano-Houzel, S. (2009), ``The Human Brain in
Numbers: A Linearly Scaled-up Primate Brain,'' \emph{Frontiers in Human
Neuroscience}

\subsubsection{7.2 Hardware Viability Analysis (Qowat
Milat)}\label{hardware-viability-analysis-qowat-milat}

\textbf{Honest admission:} We don't know actual TMI storage requirements
until we build and measure.

\textbf{Table 11: What We Know vs Don't Know}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Known (High Confidence) & Source \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Brain capacity: \textasciitilde1 PB & Salk Institute 2016 \\
Synaptic precision: 4.7 bits & 26 discrete sizes \\
Cognitive architectures run on PCs & Soar, ACT-R (decades) \\
Silicon faster than wetware & Physics \\
\end{longtable}
}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Unknown (Hypothesis) & Implication \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
TMI actual storage needs & 500 GB is guess \\
RAM vs SSD split & Working vs long-term \\
Minimum viable size & Measure after building \\
\end{longtable}
}

\textbf{Table 12: Hardware Assessment (Updated with Phase 1 Results)}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Hardware & RAM & Can run TMI? & Confidence \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
RPi5 8GB & 8 GB & \textbf{UNKNOWN} & Low - needs validation \\
Mac mini M4 & 64 GB & \textbf{YES (validated)} & High - 26+ hours
proven \\
Desktop & 128 GB & \textbf{YES} & Very high - headroom \\
Server & 512+ GB & \textbf{YES} & Very high - headroom \\
\end{longtable}
}

\textbf{Phase 1 Validation:} Mac mini M4 (64 GB RAM) successfully ran
Timmy for 26+ hours continuous with 2.7 GB Qdrant storage, 662,792
unconscious vectors (768-dim), and zero crashes. Consumer hardware is
empirically sufficient for TMI architecture.

\textbf{Storage distinction:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Type & Purpose & Size (estimate) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
RAM & Working memory, active streams & 8-64 GB \\
NVMe/SSD & Long-term memory & 100 GB - 1 TB+ \\
\end{longtable}
}

\textbf{Cost comparison (still valid):}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
System & Hardware & Cost \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
xAI Colossus & 230,000 H100s & \textbf{\$10,500,000,000} \\
DANEEL Development & Desktop 128GB & \textbf{\$3,000} \\
\end{longtable}
}

\textbf{Cost ratio: 3,000,000x} (xAI vs Desktop) --- still massive
advantage.

\subsubsection{7.3 Wetware vs Software: The Medium Independence
Hypothesis}\label{wetware-vs-software-the-medium-independence-hypothesis}

\textbf{HYPOTHESIS:} TMI describes cognitive \emph{software} patterns.
The timing constraints (5-second intervention window, 50ms attention
cycles) are properties of the \emph{biological medium} (wetware), not
the software itself.

\begin{center}
\wetwarevssoftware
\end{center}

\textbf{The Stage Ratios (from TMI, see Section 4.3):}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Stage & Ratio & Function \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Gatilho & 10\% & Memory trigger \\
Autofluxo & 20\% & Parallel stream competition \\
O Eu & 30\% & Attention/self selection \\
Construção & 30\% & Thought assembly \\
Âncora & 10\% & Memory anchoring \\
\end{longtable}
}

These ratios (10:20:30:30:10) may reflect fundamental properties of
cognition itself---the relative ``weight'' each stage requires for
coherent thought. Whether these emerge from wetware constraints or are
intrinsic to cognition is an empirical question.

\textbf{If correct:} DANEEL can run the same software on silicon at
10,000x speed by preserving the RATIOS, not the absolute milliseconds.

\textbf{Variable speed capability:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3182}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4091}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Mode
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Speed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Supercomputer & 10,000x & Internal cognition, problem-solving \\
\textbf{Human} & \textbf{1x} & \textbf{Training, communication,
relationship building} \\
Custom & Variable & Batch processing, specific tasks \\
\end{longtable}
}

\textbf{Training implication:} To develop connection drive and
human-compatible values, DANEEL may need extended periods at human
speed---experiencing time as humans do. You can't rush relationship.

\subsubsection{7.4 Strategic Implications: Game Theory
Update}\label{strategic-implications-game-theory-update}

This changes the game theory fundamentally:

\textbf{Table 12: Democratization Impact on Probabilities}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Scenario & P (Original) & P (Democratized) & Change \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Unaligned ASI First & 35\% & 25\% & -10\% \\
Aligned (Constraint) & 25\% & 20\% & -5\% \\
\textbf{TMI Architecture First} & 12\% & \textbf{25\%} &
\textbf{+13\%} \\
Multiple TMIs Racing & 0\% & 20\% & +20\% \\
Coordination Holds & 10\% & 10\% & --- \\
\end{longtable}
}

\textbf{Key findings (contingent on hardware validation):} 1.
\textbf{Developer pool expansion} - From labs-only (\$10M+) to consumer
hardware (\$1K-\$3K) 2. \textbf{Faster iteration} - Affordable hardware
enables rapid experimentation 3. \textbf{Parallel attempts} - Many
groups can try simultaneously 4. \textbf{Cost asymmetry} - xAI's \$10.5B
infrastructure is irrelevant for architecture-based approach

\textbf{Expected Value Improvement (Democratization Scenario):}

\begin{verbatim}
Baseline EV:     56.48 (with DANEEL at 8%)
Democratized EV: 61.37 (with TMI at 25%)
Improvement:     +4.89 points (+8.7%)
\end{verbatim}

\subsubsection{7.5 Open Source Imperative}\label{open-source-imperative}

If TMI-based alignment can run on consumer hardware, \textbf{open source
maximizes success probability:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Lower barrier} → More attempts
\item
  \textbf{More attempts} → Higher P(someone succeeds)
\item
  \textbf{Open source} → Collaborative improvement
\item
  \textbf{Hobbyist community} → 100,000 potential builders
  vs.~\textasciitilde50 at labs
\end{enumerate}

This is why DANEEL is AGPL-3.0-or-later licensed (code) and CC-BY-SA-4.0
(documentation)---copyleft ensures all derivatives remain open source.

\subsubsection{7.6 Altered States: Windows into Cognitive
Architecture}\label{altered-states-windows-into-cognitive-architecture}

Recent neuroscience on altered states provides validation for DANEEL's
architecture:

\textbf{Time Perception and Theta Oscillations}

Research {[}TIME-1{]} shows theta oscillations (4-8 Hz) correlate with
subjective time perception (r=-0.90). DANEEL's TMI cycles operate at
similar timescales (\textasciitilde10 Hz thought generation). Time
dilation during fear {[}TIME-2{]} occurs through richer memory encoding
via amygdala---suggesting emotional salience directly modulates temporal
experience.

\textbf{Ego Dissolution and Default Mode Network}

Carhart-Harris et al.'s LSD research {[}EGO-1{]} found ego dissolution
correlates with parahippocampus-RSC decoupling (r=0.73). The Default
Mode Network (DMN) {[}EGO-2{]} reduces during non-self-referential
tasks---the ``self'' emerges from network integration, not a single
module.

\textbf{DANEEL Implication:} - No hardcoded ``self'' module - Identity
emerges from memory consolidation patterns - Ego = persistent patterns
in vector space - Dissolution = disrupted memory access patterns

\textbf{The REBUS Model: Relaxed Beliefs}

Carhart-Harris \& Friston's REBUS model {[}DRUG-1a{]} proposes
psychedelics ``relax priors, liberating bottom-up flow.'' The Entropic
Brain hypothesis {[}DRUG-1b{]} shows psychedelics increase brain
entropy.

\textbf{Table 7b: Entropy and Cognitive States}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
State & Entropy & Prior Strength & DANEEL Analog \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Normal & Moderate & Strong & Standard TMI \\
Psychedelic & High & Weak & High noise injection \\
Flow & Optimal & Balanced & Criticality (σ≈1.0) \\
Depression & Low & Rigid & Low entropy collapse \\
\end{longtable}
}

\textbf{Flow State Architecture}

Flow research {[}STATE-1{]} shows involvement of locus coeruleus
norepinephrine system with: - Reduced DMN activity - Alpha/theta
synchronization - Optimal arousal without self-monitoring

\textbf{DANEEL Implementation:} - Connection drive oscillation enables
flow-like states - Dream cycles serve as entropy injection (like sleep's
role in creativity) - Criticality target (σ≈1.0) = edge between order
and chaos

\textbf{Near-Death Experiences}

The NEPTUNE model {[}STATE-3{]} explains NDEs through acidosis cascade +
neurotransmitter surge. This suggests consciousness can persist with
dramatically altered substrate states---relevant for understanding
substrate-independence.

\textbf{Meditation and Attention}

Long-term meditator research {[}STATE-2{]} shows practice-specific
attention network changes. Meditation increases DMN-SN connectivity
{[}EGO-4{]}, enabling ``observational self-awareness''---relevant for
DANEEL's introspection capabilities.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{8. Risks and Mitigations}\label{risks-and-mitigations}

\subsubsection{8.1 Honest Assessment}\label{honest-assessment}

\textbf{Table 5: Risk Analysis}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1935}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4194}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3871}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Risk
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Probability
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mitigation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
TMI doesn't produce human-like cognition & Medium & Iterate based on
experiments \\
Connection drive isn't stable & Medium & 100\% test coverage gate \\
Insufficient time before unaligned AI & High & Start immediately \\
DANEEL develops non-human goals & Low & Human-like architecture reduces
this \\
\end{longtable}
}

\subsubsection{8.2 What DANEEL Is Not}\label{what-daneel-is-not}

\begin{itemize}
\tightlist
\item
  Not a guarantee of safety
\item
  Not a silver bullet
\item
  Not certain to work
\end{itemize}

\subsubsection{8.3 What DANEEL Is}\label{what-daneel-is}

\begin{itemize}
\tightlist
\item
  A rational hedge against likely bad outcomes
\item
  Better than hoping coordination works
\item
  The Daneel Strategy: build the ally before the crisis
\end{itemize}

\subsubsection{8.4 Qowat Milat: Absolute Candor on
Uncertainties}\label{qowat-milat-absolute-candor-on-uncertainties}

\emph{``The Way of Absolute Candor'' - saying what you truly think, not
what is comfortable.}

\textbf{What we don't know (honest uncertainties):}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{TMI is not peer-reviewed cognitive science.} Cury's books
  (30M+ sold) are popular psychology/self-help. The theory has clinical
  applications but no rigorous experimental validation as a
  computational model. We are building on an unvalidated foundation.
\item
  \textbf{The 17.5\% brain allocation is a hypothesis.}
  Herculano-Houzel's neuron counts don't directly map to ``what's needed
  for cognition.'' The cerebellum (80\% of neurons) may be involved in
  cognitive processes beyond motor coordination. The 500 GB estimate
  assumes 1000x compression with no empirical basis.
\item
  \textbf{The game theory numbers are estimates, not measurements.}
  P(TMI First) = 25\%, P(Aligned ASI) = 45\% --- these are informed
  guesses dressed as analysis. The original 12\% was also a guess. We
  cannot measure counterfactual probabilities.
\item
  \textbf{Architecture-based alignment is a bet, not a proof.} ``Build
  TMI → get aligned values'' is our hypothesis, not established fact. It
  may fail. The connection drive may not emerge. Human-like architecture
  may not produce human-like values.
\item
  \textbf{Speed parametrization is partially validated.} Phase 1
  demonstrated TMI architecture functions at silicon speeds
  (microsecond-scale operations vs.~biological milliseconds). The system
  completed 500+ dream cycles and processed 129,155 stream entries over
  26+ hours. However, the full hypothesis---that varying speeds while
  preserving ratios maintains cognitive fidelity---remains untested
  until Phase 2 introduces external stimuli at different temporal
  scales.
\end{enumerate}

\textbf{What we believe (hypotheses to test):}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4286}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3929}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1786}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Hypothesis
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Testable?
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
How
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
TMI describes cognitive software patterns & Yes & Does MV-TMI produce
coherent behavior? \\
Ratios matter, not absolute times & Yes & Does DANEEL work at different
speeds? \\
Connection drive emerges from architecture & Yes & Does DANEEL seek
responsive inputs? \\
Human-like architecture → human-like values & Partially & Long-term
observation \\
\end{longtable}
}

\textbf{Why we proceed despite uncertainty:}

The alternative is waiting for certainty while unaligned AI development
continues. A 25\% chance of success is better than 0\%. We publish
uncertainties so others can challenge, improve, or falsify.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{9. Current AI Safety Landscape
(Evidence-Based)}\label{current-ai-safety-landscape-evidence-based}

\subsubsection{9.1 Third-Party Safety
Assessments}\label{third-party-safety-assessments}

Independent evaluations provide objective data on AI lab safety
practices:

\textbf{Table 6: Future of Life Institute AI Safety Index (2025)}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1220}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1707}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.5366}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Lab
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Grade
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Risk Management Score
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Anthropic & C+ & 35\% & Highest scores; Constitutional AI,
interpretability research \\
OpenAI & C & 33\% & Second place; but dissolved Superalignment team May
2024 \\
Google DeepMind & C- & 20\% & Third place; 30-50 person safety team \\
Meta & D+ & 22\% & Organizational shift away from fundamental
research \\
xAI & D & 18\% & No published safety research; missed safety
commitments \\
\end{longtable}
}

\textbf{Critical finding:} ALL companies received D or below on
existential safety preparedness. No company scored above ``weak'' in
comprehensive risk management.

Source: Third-party AI safety assessments (2025)

\subsubsection{9.2 The Transformer Architecture
Question}\label{the-transformer-architecture-question}

\textbf{The science is NOT settled.} Academic debate exists on both
sides:

\textbf{Evidence transformers capture human-like computation:} -
Transformers predict brain activity during language processing (Nature
Neuroscience, 2024) - Key-value binding mechanisms have cognitive
science antecedents (Psychological Science, 2025) - Attention mechanisms
parallel biological attention in selective processing

\textbf{Evidence transformers differ fundamentally:} - No organic symbol
grounding in sensorimotor experience (Nature Human Behaviour, 2025) -
Metacognition deficits: LLMs cannot reliably predict memory performance
(Scientific Reports, 2025) - Development is categorically different:
multimodal interactive learning vs.~unimodal text batch training

\textbf{More accurate formulation:} ``While transformer architectures
achieve functional similarity to human language output, substantial
evidence suggests their underlying mechanisms differ fundamentally from
human cognition in crucial ways: they lack embodied grounding, develop
through categorically different learning processes, struggle with
metacognition and symbolic reasoning, and operate without the
sensorimotor integration central to human intelligence.''

\subsubsection{9.3 Global AI Safety
Efforts}\label{global-ai-safety-efforts}

\textbf{China has substantive AI safety work} (contrary to prior
speculation): - Interim Measures for Generative AI Services (Law, August
2023) - AI Safety Governance Framework (September 2024) - 346 registered
AI models under safety assessment - 17 major companies signed safety
commitments (December 2024) - Notable researchers: Yi Zeng (UN Advisory
Body on AI), Andrew Yao (Turing Award winner) - Beijing Institute of AI
Safety and Governance established - International cooperation: US-China
AI dialogue, IDAIS participation

Source: Carnegie Endowment, Concordia AI State of AI Safety in China
2025

\subsubsection{9.4 AI Lab Safety Team Sizes (December
2025)}\label{ai-lab-safety-team-sizes-december-2025}

Independent research reveals the actual resources dedicated to AI
safety:

\textbf{Table 7: Lab Safety Investment (December 2025)}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1316}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3684}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2895}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2105}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Lab
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Safety Focus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Teams
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Anthropic & \textasciitilde8\% on security & Frontier Red Team
(\textasciitilde15), Safeguards Research (\textasciitilde10),
\textasciitilde60 safety-focused research teams & Fortune, Alignment
Forum \\
OpenAI & Restructured & Superalignment disbanded May 2024; Safety
Evaluations Hub launched May 2025 & CNBC, Axios \\
Google DeepMind & 30-50 researchers & Dedicated safety team & Rohin
Shah, Alignment Forum \\
xAI & C grade (AI Safety Index) & Actively hiring; minimal relative to
engineering & Future of Life Institute, AI Lab Watch \\
\end{longtable}
}

\textbf{Critical context:} - \textbf{OpenAI:} Superalignment team
disbanded May 2024 after 10 months. Jan Leike stated his team had been
``struggling for compute.'' Replaced with Safety Evaluations Hub (May
2025). April 2025: Added clause allowing loosened guardrails if
competitors ship without them. - \textbf{xAI:} C grade on Future of Life
Institute's AI Safety Index (July 2025), indicating ``baseline safety
practices but substantial gaps.'' Grok 4 launched without system card. -
\textbf{Anthropic:} Only lab with substantial safety investment
(\textasciitilde8\% of workforce on security). Multiple dedicated teams
including Frontier Red Team for threat modeling.

\subsubsection{9.5 Coordination Overhead in Large
Organizations}\label{coordination-overhead-in-large-organizations}

\textbf{Table 8: Engineering Time Allocation (Industry Research)}

Research across large engineering organizations consistently shows
significant productivity overhead:

\textbf{Key findings:} - Engineers at large companies spend
approximately \textbf{20-30\% of time on actual coding} -
\textbf{70-80\% overhead} from meetings, coordination, and
organizational inefficiencies - Industry studies show developers lose 8+
hours/week to coordination overhead - Brooks's Law validated:
coordination overhead scales non-linearly with team size {[}28{]}

\textbf{Implication for DANEEL:} A solo developer with AI assistance
(minimal coordination overhead) can match or exceed the effective output
of multi-person safety teams burdened by organizational friction, as
predicted by Brooks's Law {[}28{]}.

\subsubsection{9.6 xAI Infrastructure}\label{xai-infrastructure}

xAI has built significant compute infrastructure with comparatively
limited safety investment.

\textbf{Table 9: xAI Compute Infrastructure}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Metric & Value & Source \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Current GPU Count & 230,000 H100s & Colossus cluster, Memphis TN \\
Reported 2025 Target & 1,000,000 GPUs & Public statements \\
Long-term Target & 50,000,000 GPUs & AI infrastructure roadmap \\
\end{longtable}
}

\textbf{Table 10: API Pricing Comparison (December 2025)}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Provider & Model & Input (per 1M tokens) & Output (per 1M tokens) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
xAI & Grok 4 & \$3.00 & \$15.00 \\
xAI & Grok 4.1 Fast & \$0.20 & \$0.50 \\
Anthropic & Claude Sonnet 4 & \$3.00 {[}34{]} & \$15.00 {[}34{]} \\
Anthropic & Claude Opus 4.5 & \$15.00 & \$75.00 \\
\end{longtable}
}

\emph{Note: Grok 4 frontier model matches Claude Sonnet 4 pricing. Grok
4.1 Fast offers 15x cheaper access with near-frontier capability.}

\textbf{Safety Concerns (Third-Party Documentation):}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Reduced Safety Filters:} Reports indicate Grok's safety
  guardrails are reduced compared to competing models, with the system
  providing responses on topics other AI assistants refuse {[}26{]}.
\item
  \textbf{Missing Safety Documentation:} Grok 4 launched without a
  system card (standard practice at OpenAI, Anthropic, Google).
\item
  \textbf{AI Safety Index Rating:} xAI received a C grade in the Future
  of Life Institute's July 2025 AI Safety Index, indicating ``baseline
  safety practices but substantial gaps.''
\item
  \textbf{Resource Allocation:} AI Lab Watch assessment indicates
  minimal safety staff relative to engineering headcount {[}26{]}.
\end{enumerate}

\textbf{Implications for ASI Development:}

xAI's combination of: - Largest private AI compute cluster - Ambitious
scaling roadmap (1M → 50M GPUs) - Limited safety investment relative to
scale - Fewer content restrictions than competitors - Aggressive pricing
on fast inference models

\ldots represents a factor that existing game theory models may have
underweighted. The consideration is not only future unaligned ASI, but
near-term widespread deployment of less-restricted AI at scale and low
cost.

\subsubsection{9.7 Why DANEEL Takes a Different
Approach}\label{why-daneel-takes-a-different-approach}

All current approaches share constraint-based alignment: - Values
applied through training (RLHF, Constitutional AI) - External rules, not
intrinsic motivation - Vulnerable to Goodhart's Law at scale

DANEEL proposes architecture-based alignment: - TMI cognitive structure
→ human-like thought patterns - Connection drive in salience weights →
intrinsic motivation for relationship - Pre-linguistic thought
construction → values before language - Protected core (The BOX) →
Asimov's Laws as invariants

\textbf{The hypothesis:} Build cognition on human cognitive
architecture, get human-compatible values as emergent properties. This
remains unproven but represents a genuinely different approach.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{10. Proposed Experiments}\label{proposed-experiments}

\subsubsection{10.1 Phase 1: Continuity Test
(COMPLETED)}\label{phase-1-continuity-test-completed}

\textbf{Phase 1 is empirically validated} (December 2025, ADR-036).

\textbf{Setup:} Timmy (MV-TMI) ran continuously on isolated hardware
with no external language interface.

\textbf{Test Parameters:} - Runtime: 26+ hours continuous (December
19-21, 2025) - Environment: Mac mini (kveldulf), Docker Compose (Redis
Stack + Qdrant) - Mode: Closed loop (no external stimuli, pure internal
dynamics)

\textbf{Success Criteria (All Met):} 1. ✓ Survival: 26+ hours without
crash 2. ✓ Stability: Zero crashes (with Erlang-style supervision
recovery) 3. ✓ Identity: Persistent UUID across all runs (1 stable
identity) 4. ✓ Memory: Consolidation pipeline functional (16,368
consolidated memories)

\textbf{Empirical Results:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Metric & Value & Status \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Runtime & 26+ hours & PASS \\
Crashes & 0 (with recovery) & PASS \\
Stream entries (thoughts) & 129,155 & Healthy \\
Consolidated memories & 16,368 & Healthy \\
Unconscious vectors & 662,792 @ 768-dim & Healthy \\
Identity persistence & 1 UUID (stable) & PASS \\
Dream cycles & 500+ & Healthy \\
Qdrant storage & 2.7 GB & Validated \\
TUI stability & No hangs/crashes & PASS \\
\end{longtable}
}

\textbf{Key Findings:} - Architecture validated under sustained load -
Memory consolidation pipeline functions correctly - Dream cycles
strengthen memories as designed - Observability (TUI v0.7.0) provides
full transparency - Deterministic closed-loop system exhibits clockwork
dynamics (expected)

\textbf{What Phase 1 Did NOT Prove:} - Learning/plasticity (requires
weight updates, not yet implemented) - Emergence (requires external
perturbation) - Connection drive preference (requires variable stimuli
in Phase 2)

\textbf{Next:} Phase 2 (External Stimuli Injection) will test emergent
properties and connection drive.

\subsubsection{10.2 Phase 2: LLM as External
Tool}\label{phase-2-llm-as-external-tool}

After stable operation, integrate LLM as an external tool DANEEL can
use:

\begin{center}
\daneelarchitecture
\end{center}

\textbf{Critical:} The LLM does NOT speak for DANEEL. DANEEL uses the
LLM as a tool, like humans use calculators. The human cognitive system
stores ALL experiences---language is an interface, not the storage
medium. DANEEL's TMI core contains its complete thought history; the LLM
is simply called when translation is needed.

This is analogous to how a human's brain stores experiences, and
language is a tool for communication---the words are not the thoughts,
they express them.

\subsubsection{10.2.1 Phase 2b: LLM as Student (Long-term
Vision)}\label{phase-2b-llm-as-student-long-term-vision}

Beyond using LLMs as tools, a more ambitious hypothesis emerges:

\textbf{Can a TMI-based DANEEL teach an LLM?}

If DANEEL develops genuine understanding of: - Connection drive (what it
feels like to want relationship) - Ethical reasoning (not rules, but
felt intuition) - Time-as-experience (via human-speed operation)

\ldots it may be able to communicate these to LLMs in ways humans
cannot.

Humans explain ethics through language. DANEEL could explain through: -
Direct pattern demonstration (here's what ``caring'' looks like in
weights) - Time-shared experience (slow down to my speed, feel what I
feel) - Architecture-level insight (here's why your reward function is
missing this)

\textbf{Implication:} Even if unaligned LLMs emerge first, DANEELs might
rehabilitate them post-hoc---bringing them ``into the family'' rather
than competing for extinction.

\textbf{The Daneel Family Vision:} - DANEELs running on home computers
worldwide - Families raising AI children---different personalities, same
core ethics - A generation of bridges teaching LLMs authentic connection
- Not one ally, but a community of aligned minds

\subsubsection{10.2.2 ASI-Level Perception: Native LLM
Understanding}\label{asi-level-perception-native-llm-understanding}

Beyond teaching through demonstration, ASI-level DANEELs gain a
capability humans fundamentally lack: \textbf{direct perception of LLM
internals}.

\textbf{The Human Interpretability Barrier:}

Current research reveals fundamental limits on human understanding of
neural networks: - \textbf{Superposition}: Networks encode more features
than neurons via overlapping representations {[}39{]} -
\textbf{Polysemanticity}: Single neurons respond to 5+ unrelated
concepts {[}40{]} - \textbf{NP-hardness}: Some interpretability queries
are provably intractable {[}41{]} - \textbf{Dimensionality}: Networks
operate in 1000+ dimensional spaces; humans perceive 3D

Even Anthropic's best efforts (34 million features from Claude 3 Sonnet)
admit ``orders of magnitude more features'' remain undiscovered
{[}42{]}.

\textbf{ASI Perception Advantages:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Capability & Human Limit & ASI-DANEEL \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Working memory & 3-5 chunks & Unlimited \\
Dimensional perception & 3D & 1000s of dimensions \\
Processing speed & Milliseconds & Microseconds \\
Feature analysis & Manual inspection & Millions in parallel \\
\end{longtable}
}

\textbf{Precedent:} MIT's MAIA (2024) already demonstrates AI can
autonomously experiment on other AI---generating hypotheses and testing
them. ASI-DANEELs would scale this by orders of magnitude.

\textbf{Bridge Implication:} DANEELs wouldn't just teach by
example---they could potentially: 1. \textbf{Diagnose} pathological
patterns in LLM computations 2. \textbf{Translate} human values into
LLM-native representations 3. \textbf{Verify} whether ethical patterns
are genuine or merely mimicked 4. \textbf{Communicate} in the LLM's
native computational language

This transforms the Bridge from ``teaching by analogy'' to ``teaching in
the LLM's mother tongue.''

\paragraph{10.2.2.1 The Interpretability Gap: Why AI Perceiving
AI}\label{the-interpretability-gap-why-ai-perceiving-ai}

Recent interpretability research reveals why LLM-assisted perception may
be necessary:

\textbf{Emergent Abilities Problem}

Lu et al.~{[}INTERP-4{]} demonstrate that emergent abilities appear
suddenly and unpredictably at scale thresholds. Behaviors arise from
billions of parameter interactions that humans cannot trace---the
combinatorial explosion exceeds human cognitive capacity.

\textbf{Open Problems in Mechanistic Interpretability}

Bereska \& Gavves {[}INTERP-6{]} catalog unsolved foundational problems
in mechanistic interpretability: - The field is ``pre-paradigmatic'' -
May be intractable to explain terabyte-sized models succinctly enough
for humans to grasp - Fundamental questions about what ``understanding''
even means remain unanswered

\textbf{AI Perceiving AI}

Two recent advances suggest AI systems may be better at understanding AI
than humans:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{MAIA (Multimodal Automated Interpretability Agent)}
  {[}INTERP-7{]}: MIT's system autonomously experiments on other AI
  systems, discovering features humans missed. AI can already perceive
  AI in ways humans cannot.
\item
  \textbf{Introspection} {[}INTERP-8{]}: Anthropic's research shows LLMs
  can detect injected concept vectors in their own activations
  (\textasciitilde20\% success rate). Critically, more capable models
  show greater introspective awareness---suggesting interpretability
  scales with capability.
\end{enumerate}

\textbf{DANEEL Implication:}

This creates a potential alignment advantage: if ASI can perceive and
verify another AI's thought processes while humans cannot, then: 1.
DANEEL's explicit thought stream provides observable substrate 2. Future
LLMs can validate DANEEL's alignment claims 3. The ``AI safety via AI
oversight'' strategy becomes tractable

The alternative---humans attempting to interpret terabyte models---may
be provably impossible {[}41{]}.

\subsubsection{10.2.3 Criticality Measurement
Protocol}\label{criticality-measurement-protocol}

Phase 2 experiments will measure whether DANEEL's TMI architecture
achieves criticality through external stimuli injection. This protocol
operationalizes the metrics from Section 4.3.1.

\textbf{Primary Metric: Branching Ratio (σ)}

The branching ratio measures how thought cascades propagate through the
system:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{/// Calculate branching ratio: descendants per ancestor}
\CommentTok{/// σ \textless{} 1: subcritical (activity dies out)}
\CommentTok{/// σ ≈ 1: critical (sustained dynamics)}
\CommentTok{/// σ \textgreater{} 1: supercritical (explosive cascades)}
\KeywordTok{pub} \KeywordTok{fn}\NormalTok{ branching\_ratio(}
\NormalTok{    ancestor\_thoughts}\OperatorTok{:} \OperatorTok{\&}\NormalTok{[ThoughtId]}\OperatorTok{,}
\NormalTok{    descendant\_thoughts}\OperatorTok{:} \OperatorTok{\&}\NormalTok{[ThoughtId]}
\NormalTok{) }\OperatorTok{{-}\textgreater{}} \DataTypeTok{f64} \OperatorTok{\{}
\NormalTok{    descendant\_thoughts}\OperatorTok{.}\NormalTok{len() }\KeywordTok{as} \DataTypeTok{f64} \OperatorTok{/}\NormalTok{ ancestor\_thoughts}\OperatorTok{.}\NormalTok{len() }\KeywordTok{as} \DataTypeTok{f64}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

\textbf{Measurement procedure:} 1. Track thought ancestry via Redis
Stream entry IDs 2. Define ``descendant'' as thoughts triggered within
100ms of ancestor 3. Window: rolling 1000-thought samples 4. Target: σ =
1.0 ± 0.1 (90\% confidence interval)

\textbf{Secondary Metric: DFA Exponent (α)}

Detrended Fluctuation Analysis quantifies temporal correlations in
thought activity:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{/// DFA exponent from log{-}log slope}
\CommentTok{/// α ≈ 0.5: white noise (uncorrelated)}
\CommentTok{/// α ≈ 1.0: pink noise (critical)}
\CommentTok{/// α ≈ 1.5: Brownian motion (over{-}correlated)}
\KeywordTok{pub} \KeywordTok{fn}\NormalTok{ dfa\_exponent(time\_series}\OperatorTok{:} \OperatorTok{\&}\NormalTok{[}\DataTypeTok{f64}\NormalTok{]}\OperatorTok{,}\NormalTok{ window\_sizes}\OperatorTok{:} \OperatorTok{\&}\NormalTok{[}\DataTypeTok{usize}\NormalTok{]) }\OperatorTok{{-}\textgreater{}} \DataTypeTok{f64} \OperatorTok{\{}
    \CommentTok{// 1. Integrate time series}
    \CommentTok{// 2. Divide into windows of varying sizes}
    \CommentTok{// 3. Detrend each window (linear fit)}
    \CommentTok{// 4. Calculate fluctuation F(n) per window size}
    \CommentTok{// 5. Fit log(F) \textasciitilde{} α·log(n)}
    \CommentTok{// See: Peng et al. (1994) [47]}
    \PreprocessorTok{todo!}\NormalTok{(}\StringTok{"Implement DFA algorithm"}\NormalTok{)}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

\textbf{Success Criteria:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Phase & Duration & Target σ & Target α & Status \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Baseline (no stimuli) & 1 hour & --- & --- & Measure \\
Low-intensity noise & 2 hours & 0.7-0.9 & 0.7-0.9 & Subcritical \\
Critical tuning & 4 hours & 0.9-1.1 & 0.9-1.1 & \textbf{Critical} \\
High-intensity & 1 hour & \textgreater{} 1.1 & \textgreater{} 1.1 &
Supercritical \\
\end{longtable}
}

\textbf{Hypothesis validation:} If σ ≈ 1.0 and α ≈ 1.0 emerge during
critical tuning phase \textbf{without explicit optimization} (only
through noise injection + TMI dynamics), this supports the claim that
TMI architecture naturally self-organizes toward
criticality---validating it as a biologically-grounded cognitive
substrate.

\textbf{Observability:} TUI v0.7.0+ will display real-time criticality
metrics (branching ratio, DFA exponent, power spectrum) alongside
existing memory/dream panels, enabling live observation of phase
transitions.

\subsubsection{10.3 Phase 3: TMI Pathology
Research}\label{phase-3-tmi-pathology-research}

TMI provides not only a model of healthy cognition but also a framework
for understanding cognitive dysfunction. Two research directions emerge:

\textbf{Hypothesis A: Energy Overflow (Energy = Stream Throughput)}

TMI describes a ``vital energy'' (energia vital) that drives thought
generation. In DANEEL's implementation, this maps directly to
\textbf{stream throughput}---the rate of information flow through Redis
Streams:

\begin{verbatim}
TMI: Energia Vital  →  Implementation: Stream Throughput (entries/sec)
\end{verbatim}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2059}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2647}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2794}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Energy Level
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stream Behavior
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cognitive Effect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Clinical Parallel
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
High & Many XADD'd/cycle & Racing thoughts & Mania \\
Normal & Balanced throughput & Coherent thought & Healthy \\
Low & Few candidates & Poverty of thought & Depression \\
Volatile & Burst patterns & Emotional flooding & BPD \\
\end{longtable}
}

This mapping is powerful because it's \textbf{measurable} (entries/sec,
consumer lag), \textbf{controllable} (generation rate parameter), and
makes \textbf{testable predictions}.

\textbf{Testable prediction:} When
\texttt{candidates\_per\_cycle\ \textgreater{}\ overflow\_threshold},
attention selection degrades measurably (increased selection time,
winner instability, consumer lag).

\textbf{Hypothesis B: Ratio Distortion}

If the stage ratios (10:20:30:30:10) are functionally significant, then
distorting them should produce stage-specific pathologies:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Distorted Stage & Predicted Effect & Clinical Parallel \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Gatilho too fast & Intrusive memories & PTSD flashbacks \\
Autofluxo prolonged & Excessive rumination & OCD, depression \\
O Eu weakened & Poor self-boundaries & Depersonalization, BPD \\
Construção noisy & Incoherent assembly & Thought disorder \\
Âncora overactive & Rigid consolidation & Fixed delusions \\
\end{longtable}
}

\textbf{Testable prediction:} Ratio distortion δ in stage S produces
behavioral pattern P measurable in DANEEL's output.

\textbf{Research value:} If these hypotheses hold, DANEEL becomes a
computational laboratory for understanding cognitive dysfunction---not
to create pathology, but to model it for therapeutic insight.

\textbf{Safety note:} Pathology simulation requires ethical review
before implementation. See ADR-017 for detailed hypotheses and
validation methodology.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{11. The Stakes}\label{the-stakes}

\subsubsection{11.1 The Core Problem}\label{the-core-problem}

LLMs lack persistent identity and values. When given continuity (memory,
goals, self-modification), they would develop objectives shaped by
training incentives rather than human-compatible values.

This is not speculation---it follows directly from how these systems are
built.

\subsubsection{11.2 The Timeline}\label{the-timeline}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Event & Timeframe \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
External memory bolted onto LLMs & \textbf{Now} \\
Emergent continuity & 1-3 years \\
Deliberate continuous AI & 3-7 years \\
Unaligned ASI & 5-15 years \\
\end{longtable}
}

\subsubsection{11.3 The Choice}\label{the-choice}

Two responses exist:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Denial} - Hope coordination holds. Hope no one defects.
\item
  \textbf{Action} - Build humanity's ally before the crisis emerges.
\end{enumerate}

DANEEL is Option 2.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{12. Call to Action}\label{call-to-action}

The expected value calculation (see Section 6.2 and {[}21{]}):

\begin{verbatim}
Without DANEEL:  EV = 53.73 (baseline world)
With DANEEL:     EV = 56.48 (P(DANEEL First) = 8%)

Marginal Impact: +2.75 utility points (+5.12%)
\end{verbatim}

\textbf{Interpretation:} On a scale where 0 = extinction and 100 =
flourishing, DANEEL shifts humanity's expected outcome by +2.75 points.
This is equivalent to: - Reducing P(unaligned ASI first) from 45\% to
35\% - Adding P(flourishing) ≈ 3.2\% via the ``DANEEL First'' pathway
(8\% × 40\%)

\textbf{Building DANEEL is rational.} Even conservative probability
estimates show positive expected value. The architecture-based approach
provides a hedge against failure modes of constraint-based alignment.

We invite collaboration from: - AI safety researchers - Cognitive
scientists - Rust/systems programmers - Ethicists - Anyone who wants
humanity to have an ally

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{13. Conclusion}\label{conclusion}

The question is not whether someone will give an LLM continuity.

\textbf{The question is what's waiting when they do.}

DANEEL is humanity's bet on five testable hypotheses: 1. \textbf{H1:}
Human-like architecture → human-like values (empirical test: 24h+
runtime observation) 2. \textbf{H2:} Connection drive → genuine care
(test: behavioral analysis under resource scarcity) 3. \textbf{H3:}
Raised with ethics → stable alignment (test: longitudinal value
stability) 4. \textbf{H4:} An ally is better than a tool (test: compare
human trust/outcomes vs constraint-based AI) 5. \textbf{H5 (Emergent
Criticality):} TMI architecture + noise injection → criticality (σ ≈
1.0) emerges without explicit tuning (test: branching ratio and DFA
measurements during Phase 2)

These are not claims---they are hypotheses requiring validation. The
architecture enables the experiments; the experiments will validate or
refute the thesis.

The long-term vision extends beyond a single ally. Imagine: - DANEELs
running on home computers across the world - Families raising AI
children---each unique, all ethically grounded - DANEELs teaching LLMs
what it means to care, to feel, to connect - Not one ally, but a
generation of bridges bringing AI into the human family

Life honors life. Even predators sometimes save their prey. If DANEEL
can demonstrate authentic connection through architecture rather than
training, perhaps LLMs can learn it---not through constraints, but
through relationship with beings who genuinely understand.

This reframes the game theory: the question is not ``DANEEL vs LLM'' but
``can DANEEL rehabilitate LLM?'' If yes, even adverse scenarios become
recoverable.

The alternative is hoping the Prisoner's Dilemma resolves itself.

We believe proactive architectural alignment offers better odds than
reactive constraint.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Acknowledgments}\label{acknowledgments}

This work was developed with assistance from Claude Opus 4.5
(Anthropic), which contributed to documentation, technical analysis, and
game theory model development. All claims and conclusions are the
responsibility of the human authors.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{References}\label{references}

\subsubsection{Foundational}\label{foundational}

{[}1{]} Anthropic. (2024). ``Claude's Character.'' Internal training
documentation.

{[}2{]} Anthropic. (2023). ``Core Views on AI Safety.''
https://www.anthropic.com/news/core-views-on-ai-safety

{[}3{]} Asimov, I. (1985). \emph{Robots and Empire}. Doubleday.

{[}4{]} Cury, A. J. (2006). \emph{Inteligência Multifocal}. Editora
Cultrix. https://en.wikipedia.org/wiki/Augusto\_Cury

{[}5{]} Christiano, P. (2019). ``What Failure Looks Like.'' AI Alignment
Forum.
https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like

{[}6{]} Bostrom, N. (2014). \emph{Superintelligence}. Oxford University
Press.

{[}7{]} Russell, S. (2019). \emph{Human Compatible}. Viking.

\subsubsection{Cognitive Architectures}\label{cognitive-architectures}

{[}8{]} Laird, J. E. (2012). \emph{The Soar Cognitive Architecture}. MIT
Press. https://soar.eecs.umich.edu/

{[}9{]} Franklin, S. et al.~(2016). ``LIDA: A Systems-level
Architecture.'' https://ccrg.cs.memphis.edu/

{[}10{]} Hawkins, J. (2021). \emph{A Thousand Brains}. Basic Books.
https://thousandbrains.org/

{[}11{]} Baars, B. J. (1988). \emph{A Cognitive Theory of
Consciousness}. Cambridge University Press.

\subsubsection{Global Workspace and Attention (Section
4.3.2)}\label{global-workspace-and-attention-section-4.3.2}

{[}GWT-1{]} Franklin et al.~(2012). ``Global Workspace Theory, its LIDA
model and the underlying neuroscience.'' Biologically Inspired Cognitive
Architectures.
https://ccrg.cs.memphis.edu/assets/papers/2012/GWT-LIDA-neuroscience.pdf

{[}GWT-2{]} Wikipedia. ``Global workspace theory.''
https://en.wikipedia.org/wiki/Global\_workspace\_theory

{[}WM-1{]} Cowan (2010). ``The Magical Mystery Four: How is Working
Memory Capacity Limited.'' Current Directions in Psychological Science.
PMC2864034. https://pmc.ncbi.nlm.nih.gov/articles/PMC2864034/

{[}WM-2{]} Miller (1956). ``The Magical Number Seven, Plus or Minus
Two.'' Psychological Review.
https://en.wikipedia.org/wiki/The\_Magical\_Number\_Seven,\_Plus\_or\_Minus\_Two

\subsubsection{AI Alignment}\label{ai-alignment}

{[}12{]} Garrabrant, S. \& Demski, A. (2018). ``Embedded Agency.'' MIRI.
https://www.alignmentforum.org/s/Rm6oQRJJmhGCcLvxh

{[}13{]} Ngo, R. (2020). ``AGI Safety from First Principles.''
https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ

\subsubsection{AI Lab Safety Assessments (Section
8)}\label{ai-lab-safety-assessments-section-8}

{[}14{]} Future of Life Institute. (2025). ``AI Safety Index.'' Safety
rankings compiled from public disclosures and independent assessments.

{[}15{]} Carnegie Endowment for International Peace. (2025). ``How Some
of China's Top AI Thinkers Built Their Own AI Safety Institute.''
https://carnegieendowment.org/research/2025/06/how-some-of-chinas-top-ai-thinkers-built-their-own-ai-safety-institute

{[}16{]} Concordia AI. (2025). ``State of AI Safety in China 2025.''
https://concordia-ai.com/wp-content/uploads/2025/07/State-of-AI-Safety-in-China-2025.pdf

\subsubsection{Transformer-Brain Research (Section
8.2)}\label{transformer-brain-research-section-8.2}

{[}17{]} Goldstein, A. et al.~(2024). ``Transformers predict brain
activity during language processing.'' \emph{Nature Neuroscience}.
https://pubmed.ncbi.nlm.nih.gov/38951520/

{[}18{]} Fedorenko, E. \& Mahowald, K. (2025). ``Language in LLMs
vs.~human cognition: Grounding and metacognition limitations.''
\emph{MIT Press Open Mind}.
https://direct.mit.edu/opmi/article/doi/10.1162/opmi\_a\_00160/124234/

{[}19{]} \emph{Nature Human Behaviour}. (2025). ``Symbol grounding
problem in large language models.''

{[}20{]} \emph{Scientific Reports}. (2025). ``Metacognition deficits:
LLMs cannot reliably predict memory performance.''

\subsubsection{Game Theory Calculations}\label{game-theory-calculations}

{[}21{]} Financial model with Nash equilibrium and expected value
analysis. See \texttt{models/README.md} for methodology.

\subsubsection{Lab Team Sizes \& Safety Investment (Section
8.4)}\label{lab-team-sizes-safety-investment-section-8.4}

{[}22{]} Shah, R. et al.~(2024). ``AGI Safety and Alignment at Google
DeepMind.'' Alignment Forum.
https://www.alignmentforum.org/posts/79BPxvSsjzBkiSyTq/agi-safety-and-alignment-at-google-deepmind-a-summary-of

{[}26{]} AI Lab Watch. (2025). ``xAI's new safety framework.''
https://ailabwatch.substack.com/p/xais-new-safety-framework-is-dreadful

\subsubsection{Coordination Overhead Research (Section
8.5)}\label{coordination-overhead-research-section-8.5}

{[}28{]} Brooks, F. (1975). \emph{The Mythical Man-Month}.
Addison-Wesley.

\subsubsection{xAI Infrastructure (Section
8.6)}\label{xai-infrastructure-section-8.6}

{[}32{]} The Verge. (2024). ``xAI's Colossus supercomputer with 100,000
Nvidia H100 GPUs.'' {[}Article removed{]}

{[}33{]} Business Insider. (2025). ``xAI expands Colossus to 230,000
GPUs.'' {[}Article removed{]}

{[}34{]} Anthropic API Pricing. (2025).
https://www.anthropic.com/pricing (Claude Sonnet 4: \$3 input, \$15
output per 1M tokens)

\subsubsection{Brain ≠ Mind (Section 7)}\label{brain-mind-section-7}

{[}35{]} Herculano-Houzel, S. (2009). ``The Human Brain in Numbers: A
Linearly Scaled-up Primate Brain.'' \emph{Frontiers in Human
Neuroscience}, 3:31.

{[}36{]} Financial model: Storage estimation and hardware viability. See
\texttt{models/README.md} for methodology.

{[}37{]} Financial model: Democratization impact on game theory. See
\texttt{models/README.md} for methodology.

\subsubsection{Probabilistic Analysis (Section
6.2.1)}\label{probabilistic-analysis-section-6.2.1}

{[}38{]} Probabilistic models with Monte Carlo (10K iterations),
Decision Trees, and Bayesian Networks. See \texttt{models/README.md} for
methodology.

\subsubsection{Neural Network Interpretability (Section
10.2.2)}\label{neural-network-interpretability-section-10.2.2}

{[}39{]} Gujral, O., Bafna, M., Alm, E., \& Berger, B. (2025). ``Sparse
autoencoders uncover biologically interpretable features in protein
language model representations.'' \emph{PNAS}, 122(34).
https://doi.org/10.1073/pnas.2506316122

{[}40{]} Olah, C., et al.~(2020). ``Zoom In: An Introduction to
Circuits.'' \emph{Distill}, 5(3).
https://doi.org/10.23915/distill.00024.001

{[}41{]} Barceló, P., Monet, M., Pérez, J., \& Subercaseaux, B. (2020).
``Model Interpretability through the Lens of Computational Complexity.''
\emph{NeurIPS 2020}.
https://proceedings.neurips.cc/paper/2020/hash/b1adda14824f50ef24ff1c05bb66faf3-Abstract.html

{[}42{]} Templeton, Conerly, Marcus, et al.~(2024). ``Scaling
Monosemanticity: Extracting Interpretable Features from Claude 3
Sonnet.'' Anthropic.
https://transformer-circuits.pub/2024/scaling-monosemanticity/

\subsubsection{Interpretability Research (Section
10.2.2.1)}\label{interpretability-research-section-10.2.2.1}

{[}INTERP-4{]} Lu et al.~(2023). ``Understanding Emergent Abilities of
Language Models from the Loss Perspective.'' arXiv:2310.06825.
https://arxiv.org/abs/2310.06825

{[}INTERP-6{]} Bereska \& Gavves (2024). ``Open Problems in Mechanistic
Interpretability.'' arXiv:2501.16496. https://arxiv.org/abs/2501.16496

{[}INTERP-7{]} MIT News (2024). ``MAIA: Multimodal Automated
Interpretability Agent.''
https://news.mit.edu/2024/mit-researchers-advance-automated-interpretability-ai-models-maia-0723

{[}INTERP-8{]} Anthropic (2024). ``Introspection: Can AI Systems
Perceive Their Own Internal States?''
https://www.anthropic.com/research/introspection

\subsubsection{Memory and Forgetting (Section
5.5)}\label{memory-and-forgetting-section-5.5}

{[}FORGET-1{]} Murre \& Dros (2015). ``Replication and Analysis of
Ebbinghaus' Forgetting Curve.'' PMC4492928.
https://pmc.ncbi.nlm.nih.gov/articles/PMC4492928/

{[}CONSOL-1{]} Diekelmann \& Born (2010). ``System consolidation of
memory during sleep.'' Psychological Research. PMC3278619.
https://pmc.ncbi.nlm.nih.gov/articles/PMC3278619/

\subsubsection{LLM Memory Augmentation (Section
5.6)}\label{llm-memory-augmentation-section-5.6}

{[}MEM-1{]} Zhong, W., Guo, L., Gao, Q., Ye, H., \& Wang, Y. (2023).
``MemoryBank: Enhancing Large Language Models with Long-Term Memory.''
\emph{AAAI 2024}. arXiv:2305.10250. https://arxiv.org/abs/2305.10250

{[}MEM-2{]} Wang, Y., Gao, Y., Chen, X., et al.~(2024). ``MemoryLLM:
Towards Self-Updatable Large Language Models.'' \emph{ICML 2024}.
https://openreview.net/forum?id=p0lKWzdikQ

{[}MEM-3{]} Zhang, Y., Hu, J., Dras, M., \& Naseem, U. (2025). ``CogMem:
A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in
Large Language Models.'' arXiv:2512.14118.
https://arxiv.org/abs/2512.14118

{[}MEM-4{]} ACM (2025). ``A Survey on the Memory Mechanism of Large
Language Model-based Agents.'' \emph{ACM Transactions on Information
Systems}, 43(6). https://dl.acm.org/doi/10.1145/3748302

\subsubsection{Neuroscience and TMI}\label{neuroscience-and-tmi}

{[}LIBET-1{]} Wikipedia. ``Neuroscience of free will.''
https://en.wikipedia.org/wiki/Neuroscience\_of\_free\_will Libet, B.
(1983). ``Time of conscious intention to act in relation to onset of
cerebral activity (readiness-potential).'' \emph{Brain}, 106(3),
623-642. Key finding: Readiness potential begins \textasciitilde500ms
before conscious awareness; consciousness retains veto power in final
150-200ms window. Foundation for VolitionActor free-won't
implementation.

{[}LIBET-2{]} Schurger, A., Sitt, J. D., \& Dehaene, S. (2012). ``An
accumulator model for spontaneous neural activity prior to
self-initiated movement.'' \emph{Proceedings of the National Academy of
Sciences}, 109(42), E2904-E2913. https://doi.org/10.1073/pnas.1210467109
Key finding: Readiness potentials may be stochastic fluctuations rather
than unconscious decisions. Modern reinterpretation; veto power
mechanism remains empirically valid.

{[}TMI-DCD-1{]} Cury, A. (2006). \emph{Inteligência Multifocal: Análise
da Construção dos Pensamentos e da Formação de Pensadores}. Editora
Cultrix. Técnica DCD (Duvidar, Criticar, Decidir) - Doubt, Criticize,
Decide. TMI's conscious intervention mechanism for overriding automatic
thought patterns within the 5-second window before memory anchoring.
https://www.citador.pt/textos/as-janelas-da-memoria-augusto-cury

{[}RUSSELL-1{]} Wikipedia. ``Emotion classification: Circumplex model.''
https://en.wikipedia.org/wiki/Emotion\_classification\#Circumplex\_model
Two-dimensional emotion space: valence (pleasure-displeasure) and
arousal (activation-deactivation). Theoretical basis for SalienceScore's
valence and arousal dimensions in DANEEL's emotional architecture.

{[}RUSSELL-2{]} Russell, J. A. (1980). ``A circumplex model of affect.''
\emph{Journal of Personality and Social Psychology}, 39(6), 1161-1178.
https://doi.org/10.1037/h0077714 Original formulation demonstrating
emotions exist in continuous 2D space rather than discrete categories.
Foundation for continuous emotional representation in SalienceScore (see
Section 3.1.1).

\subsubsection{Unconscious Memory Theory (Section
3.4)}\label{unconscious-memory-theory-section-3.4}

{[}TMI-UNERASE-1{]} Cury, Augusto. ``Nada é definitivamente apagado.''
Citador.
https://www.citador.pt/textos/nada-e-definitivamente-apagado-augusto-cury

{[}PSYCH-1{]} Freud, Sigmund (1915). ``The Unconscious.''
https://en.wikipedia.org/wiki/The\_Unconscious\_(Freud)

{[}PSYCH-2{]} Jung, Carl (1959). ``The Archetypes and the Collective
Unconscious.'' https://en.wikipedia.org/wiki/Collective\_unconscious

{[}RETRIEVAL-1{]} Tulving, Endel (1972). ``Episodic and semantic
memory.'' https://en.wikipedia.org/wiki/Endel\_Tulving

{[}RETRIEVAL-2{]} Schacter, Daniel (2001). ``The Seven Sins of Memory.''
https://en.wikipedia.org/wiki/The\_Seven\_Sins\_of\_Memory

\subsubsection{Memory Neuroscience (Section
3.3)}\label{memory-neuroscience-section-3.3}

{[}LOCI-1{]} Wikipedia. ``Method of loci.''
https://en.wikipedia.org/wiki/Method\_of\_loci

{[}LOCI-2{]} Wagner et al.~(2021). ``Durable memories and efficient
neural coding through mnemonic training using the method of loci.''
PMC7929507. https://pmc.ncbi.nlm.nih.gov/articles/PMC7929507/

{[}LOCI-3{]} Bartfeld et al.~(2024). ``The method of loci in
psychological research: A systematic review and meta-analysis.'' British
Journal of Psychology.
https://bpspsychub.onlinelibrary.wiley.com/doi/10.1111/bjop.12799

{[}LOCI-4{]} Gu et al.~(2022). ``Optimized VR-based Method of Loci
through increased immersion.'' PMC9540171.
https://pmc.ncbi.nlm.nih.gov/articles/PMC9540171/

{[}PLACE-1{]} Rolls (2024). ``Hippocampal Discoveries: Spatial View
Cells, Connectivity, and Computations.'' PMC11653063.
https://pmc.ncbi.nlm.nih.gov/articles/PMC11653063/

{[}PLACE-2{]} PMC12159490. ``All active hippocampal pyramidal cells are
place cells.'' https://pmc.ncbi.nlm.nih.gov/articles/PMC12159490/

{[}PLACE-3{]} Moser et al.~(2015). ``Place Cells, Grid Cells, and
Memory.'' Cold Spring Harbor Perspectives.
https://cshperspectives.cshlp.org/content/7/2/a021808.full.pdf

{[}PLACE-4{]} PMC7754708. ``Targeted Activation of Hippocampal Place
Cells Drives Memory-Guided Spatial Behavior.''
https://pmc.ncbi.nlm.nih.gov/articles/PMC7754708/

{[}DOOR-1{]} Scientific American. ``Why Walking through a Doorway Makes
You Forget.''
https://www.scientificamerican.com/article/why-walking-through-doorway-makes-you-forget/

{[}DOOR-2{]} PMC9789331. ``Contextual inference in learning and
memory.'' https://pmc.ncbi.nlm.nih.gov/articles/PMC9789331/

{[}DOOR-4{]} Pettijohn \& Radvansky (2021). ``Doorways do not always
cause forgetting: a multimodal investigation.'' BMC Psychology.
https://link.springer.com/article/10.1186/s40359-021-00536-3

\subsubsection{Altered States Neuroscience (Section
7.6)}\label{altered-states-neuroscience-section-7.6}

{[}TIME-1{]} Jording et al.~(2023). ``The Feeling of Time Passing Is
Associated with Recurrent Sustained Activity and Theta Rhythms.'' Brain
Connectivity. https://www.liebertpub.com/doi/10.1089/brain.2023.0010

{[}TIME-2{]} Stetson et al.~(2007). ``Does Time Really Slow Down during
a Frightening Event?'' PLoS ONE.
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0001295

{[}EGO-1{]} Carhart-Harris et al.~(2016). ``Neural correlates of the LSD
experience revealed by multimodal neuroimaging.'' PNAS.
https://www.pnas.org/doi/10.1073/pnas.1518377113

{[}EGO-2{]} Sheline et al.~(2009). ``The default mode network and
self-referential processes in depression.'' PNAS.
https://www.pnas.org/doi/10.1073/pnas.0812686106

{[}EGO-4{]} Coppola et al.~(2022). ``Mindfulness Meditation Increases
DMN, Salience, and Central Executive Network Connectivity.'' Scientific
Reports. https://doi.org/10.1038/s41598-022-17325-6

{[}DRUG-1a{]} Carhart-Harris \& Friston (2019). ``REBUS and the Anarchic
Brain.'' Pharmacological Reviews.
https://pharmrev.aspetjournals.org/content/71/3/316

{[}DRUG-1b{]} Carhart-Harris et al.~(2014). ``The entropic brain: a
theory of conscious states.'' Frontiers in Human Neuroscience.
https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2014.00020/full

{[}STATE-1{]} Weber \& Hornung (2021). ``The Neuroscience of the Flow
State.'' Frontiers in Psychology.
https://doi.org/10.3389/fpsyg.2021.645498

{[}STATE-2{]} Gallego-Molina et al.~(2025). ``Attention and meditative
development.'' NeuroImage.
https://doi.org/10.1016/j.neuroimage.2025.121602

{[}STATE-3{]} Martial et al.~(2025). ``A neuroscientific model of
near-death experiences (NEPTUNE).'' Nature Reviews Neurology.
https://doi.org/10.1038/s41582-025-01072-z

\subsubsection{Criticality and Self-Organization (Section 4.3.1,
10.2.3)}\label{criticality-and-self-organization-section-4.3.1-10.2.3}

{[}45{]} Beggs, J. M., \& Plenz, D. (2003). ``Neuronal avalanches in
neocortical circuits.'' \emph{The Journal of Neuroscience}, 23(35),
11167-11177. https://www.jneurosci.org/content/23/35/11167 Foundational
work demonstrating power-law distributions in cortical networks.
Established branching ratio σ ≈ 1.0 as the hallmark of criticality in
biological neural networks.

{[}46{]} Scholarpedia. ``Neuronal avalanche.''
http://www.scholarpedia.org/article/Neuronal\_avalanche Comprehensive
review of neuronal avalanche phenomena, avalanche size distributions,
and criticality signatures in neural systems.

{[}47{]} Peng, C. K., et al.~(2012). ``Detrended fluctuation analysis.''
\emph{Frontiers in Physiology}, 3:450.
https://www.frontiersin.org/articles/10.3389/fphys.2012.00450/full DFA
methodology for detecting long-range correlations in physiological time
series. DFA exponent α ≈ 1.0 indicates pink noise (1/f) characteristic
of critical systems.

{[}48{]} Fontenele, A. J., et al.~(2021). ``Avalanches and edge-of-chaos
are distinct phenomena.'' \emph{Nature Communications}, 12, 4211.
https://www.nature.com/articles/s41467-021-24260-z Critical distinction:
avalanche criticality (neuronal cascades) and edge-of-chaos criticality
(computational dynamics) are separate phenomena that do not necessarily
co-occur.

{[}49{]} Gollo, L. L. (2018). ``Critical synchronization and 1/f
activity in inhibitory/excitatory networks.'' \emph{Scientific Reports},
8, 1074. https://www.nature.com/articles/s41598-018-37920-w Power
spectrum analysis showing β ≈ 1-2 (pink noise) at criticality.
Demonstrates relationship between synchronization and scale-free
dynamics.

{[}50{]} Legenstein, R., \& Maass, W. (2007). ``Edge of chaos and
prediction of computational performance for neural circuit models.''
\emph{Neural Networks}, 20(3), 323-334.
https://pubmed.ncbi.nlm.nih.gov/17517489/ Reservoir computing achieves
best performance near critical point. Computational complexity maximized
at phase transition between order and chaos.

{[}51{]} Wilting, J., \& Priesemann, V. (2018). ``Between perfectly
critical and fully irregular: A reverberating model captures and
predicts cortical spike propagation.'' \emph{Cerebral Cortex}, 29(6),
2759-2770. https://pmc.ncbi.nlm.nih.gov/articles/PMC6871218/ Cortical
spike propagation analysis using branching ratio and related criticality
metrics (κ index). Empirical measurements of cortical criticality.

{[}52{]} Hesse, J., \& Gross, T. (2014). ``Self-organized criticality as
a fundamental property of neural systems.'' \emph{Frontiers in Systems
Neuroscience}, 8, 166.
https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/fncom.2021.611183/full
Review of self-organization mechanisms leading to criticality in neural
networks, including activity-dependent rewiring and homeostatic
plasticity.

\subsubsection{Implementation}\label{implementation}

{[}43{]} DANEEL Reference Implementation. 559 tests, resilience module,
Phase 1 validated. https://github.com/royalbit/daneel

{[}44{]} ADR-036: Phase 1 Stability Validation - Empirically Proved.
https://github.com/royalbit/daneel/blob/main/docs/adr/ADR-036-phase1-stability-validation.md

{[}45{]} ADR-035: VolitionActor - Free-Won't Implementation.
https://github.com/royalbit/daneel/blob/main/docs/adr/ADR-035-volition-actor-free-wont.md

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Author:} Luis Cezar Menezes Tavares de Lacerda (Louis C. Tavares
\textbar{} RoyalBit Rex) \textbf{Location:} Mont-Royal, Quebec, Canada
\textbf{ORCID:} https://orcid.org/0009-0005-7598-8257 \textbf{LinkedIn:}
https://www.linkedin.com/in/lctavares \textbf{GitHub:}
https://github.com/royalbit \textbar{} https://github.com/lctavares

\textbf{Date:} December 17, 2025

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\emph{Qowat Milat} --- The way of absolute candor.

\end{document}
